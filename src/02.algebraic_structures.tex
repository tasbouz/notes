%! suppress = Quote
%! suppress = EscapeUnderscore
%! suppress = EscapeAmpersand
\bd [Algebraic Structures]
A set $A$ (called the underlying set, carrier set or domain), together with a collection of maps (called operations)
on $A$ of finite arity (typically binary operations), and a finite set of identities, known as axioms, that these
operations must satisfy, is called an \textbf{algebraic structure}. Some algebraic structures also involve another
set (called the scalar set). \ed

Examples of algebraic structures with a single underlying set include groups, fields and rings. Examples of algebraic
structures with two underlying sets include vector spaces, modules, and algebras. In this section we will review the
most important algebraic structures for our purposes. \v

One has to be careful with the terminology since it changes depending on the area of mathematics. For example, in the
context of universal algebra, the set $A$ with this structure is called an algebra, while, in other contexts, it is
(somewhat ambiguously) called an algebraic structure, the term algebra being reserved for specific algebraic
structures that are vector spaces over a field or modules over a commutative ring. \v

The properties of specific algebraic structures are studied in abstract algebra. The general theory of algebraic
structures has been formalized in universal algebra. The language of category theory is used to express and study
relationships between different classes of algebraic and non-algebraic objects. This is because it is sometimes
possible to find strong connections between some classes of objects, sometimes of different kinds. For example,
Galois theory establishes a connection between certain fields and groups: two algebraic structures of different kinds. \v

In this chapter we will introduce the basic algebraic structures by giving their definitions and some of their key
properties. In later chapter we get into depth in various topics of algebraic structures.

\section{Groups}

A group is a set equipped with an operation that combines any two elements to form a third element while being
associative as well as having an identity element and inverse elements. These three conditions, called group axioms,
hold for number systems and many other mathematical structures.

\bd [Group]
A \textbf{group} is a tuple $(G,\cdot)$, where $G$ is a set (called the underlying set of the group) and $\cdot$ is a
map (called operation) $G\times G \to G$ satisfying the following four group axioms:
\bit
\item Closure: $\forall \, a,b \in G : a \cdot b \in G$.
\item Associativity: $\forall \, a,b,c \in G : (a \cdot b) \cdot c=a \cdot (b \cdot c)$.
\item Neutral Element: $\exists \, e \in G : \forall \, a \in G : a \cdot e = e \cdot a=a$.
\item Inverse Element: $\forall \, a \in G : \exists \, a^{-1} \in G : a \cdot a^{-1} =a^{-1} \cdot a = e$.
\eit
\ed

The identity element $e$ of a group $G$ is often written as $1$ a notation inherited from the multiplicative identity.
If a group is abelian, then one may choose to denote the group operation by $+$ and the identity element by $0$. \v

The result of the group operation may depend on the order of the operands. In other words, the result of combining
element $a$ with element $b$ need not yield the same result as combining element $b$ with element $a$, so the
equation $a \cdot b = b \cdot a$ may not be true for every two elements $a$ and $b$.

\bd [Abelian Group]
A group $G$ is called \textbf{Abelian} if on top of the four group axioms it also satisfies the axiom of commutativity:
\bit
\item Commutativity: $\forall \, a,b \in G : a \cdot b = b \cdot a$.
\eit
\ed

Commutativity always holds in the group of integers under addition, because $a + b = b + a$ for any two integers
(commutativity of addition). The symmetry group is an example of a group that is not abelian.

\section{Fields}

A field is a set on which addition, subtraction, multiplication, and division are defined and behave as the
corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure which is
widely used in algebra, number theory, and many other areas of mathematics. \v

The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers.
Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and
p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry.

\bd [Field]
An \textbf{(algebraic) field} is a triple $(K,+, \cdot)$, where $K$ is a set and $+,\cdot$ are maps $K\times K \to K$
satisfying the following axioms:
\bit
\item $(K,+)$ is an abelian group, i.e.\ :
\ben
\item[i)] Closure: $\forall \, a,b \in K : a + b \in K$.
\item[ii)] Associativity: $\forall \, a,b,c \in K : (a+b)+c=a+(b+c)$.
\item[iii)] Neutral Element: $\exists \, 0 \in K : \forall \, a \in K : a+0=0+a=a$.
\item[iv)] Inverse Element: $\forall \, a \in K : \exists \, {-a} \in K : a+(-a)=(-a)+a=0$.
\item[v)] Commutativity: $\forall \, a,b \in K : a+b=b+a$.
\een

\item $(K^*,\cdot)$, where $K^* \coloneqq K\sm\{0\}$, is an abelian group, i.e.\ :
\ben
\item[vi)] Closure: $\forall \, a,b \in K^* : a \cdot b \in K^*$.
\item[vii)] Associativity: $\forall \, a,b,c \in K^* : (a\cdot b)\cdot c=a\cdot (b\cdot c)$.
\item[viii)] Neutral Element: $\exists \, 1 \in K^* : \forall \, a \in K^* : a\cdot 1=1\cdot a=a$.
\item[ix)] Inverse Element: $\forall \, a \in K^* : \exists \, a^{-1} \in K^* : a\cdot a^{-1}=a^{-1} \cdot a=1$.
\item[x)] Commutativity: $\forall \, a,b \in K^* : a\cdot b=b\cdot a$.
\een

\item The maps $+$ and $\cdot$ satisfy the distributive property:
\ben
\item[xi)] $\forall \, a,b,c \in K : (a+ b)\cdot c=a\cdot c + b\cdot c$.
\een
\eit
\ed

In the above definition, we included axiom v for the sake of clarity, but in fact it can be proven starting from the
other axioms.

\section{Vector Spaces}

A vector space (also called a linear space) is a set of objects called vectors, which may be added together and
multiplied (``scaled'') by numbers called scalars. Scalars are often real numbers, but some vector spaces have scalar
multiplication by complex numbers or, generally, by a scalar from any mathematic field. The operations of vector
addition and scalar multiplication must satisfy certain requirements, called vector axioms.

\bd [K-Vector Space]
Let $(K,+,\cdot)$ be a field. A $K$\textbf{-vector space}, or \textbf{vector space over $K$} or \textbf{linear space
over $K$} is a triple $(V,\oplus,\odot)$, where $V$ is a set and:
\bi{rl}
\oplus &\cl V\times V \to V \\ \odot &\cl K\times V \to V
\ei

are maps satisfying the following axioms:
\bit
\item $(V,\oplus)$ is an abelian group i.e.
\ben
\item[i)] Closure: $\forall \, v,w \in V : v \oplus w \in K$.
\item[ii)] Associativity: $\forall \, v,w,z \in V : (v \oplus w) \oplus z = v \oplus (w \oplus z)$.
\item[iii)] Neutral Element: $\exists \, 0 \in V : \forall \, v \in V : v \oplus 0 = 0 \oplus v = v$.
\item[iv)] Inverse Element: $\forall \, v \in V : \exists \, -v \in V : v \oplus (-v) = (-v) \oplus v = 0$.
\item[v)] Commutativity: $\forall \, v,w \in V : v \oplus w = w \oplus v$.
\een

\item The map $\odot$ is an \emph{action} of $K$ on $(V,\oplus)$:
\ben
\item[vi)] Distributivity Of Scalar Multiplication - Vector Addition: $\forall \, \lambda \in K : \forall \, v,w \in
V : \lambda\odot(v\oplus w)=(\lambda\odot v)\oplus (\lambda\odot w)$.
\item[vii)] Distributivity Of Scalar Multiplication - Field Addition: $\forall \, \lambda,\mu \in K : \forall \, v
\in V : (\lambda+\mu)\odot v= (\lambda \odot v) \oplus (\mu \odot v)$.
\item[viii)] Compatibility Of Scalar Multiplication - Field Multiplication: $\forall \, \lambda,\mu \in K : \forall
\, v \in V : (\lambda\cdot\mu)\odot v= \lambda \odot (\mu \odot v)$.
\item[ix)] Neutral Element Of Scalar Multiplication: $\forall \, v \in V : 1\odot v = v$.
\een
\eit
\ed

The elements of a vector space are called \emph{vectors}, while the elements of $K$ are often called \emph{scalars},
and the map $\odot$ is called \emph{scalar multiplication}.

\subsection{Linear Maps}

As usual by now, we will look at the structure-preserving maps between vector spaces.

\bd [Linear Maps]
Let $(V,\oplus,\odot)$, $(W,\boxplus,\boxdot)$ be vector spaces over the same field $K$ and let $f\cl V\to W$ be a
map. We say that $f$ is a \textbf{linear map}, or a \textbf{homomorphism}, and we denote it as $f\cl
V\xrightarrow{\sim}W$, if for all $v_1,v_2\in V$ and all $\lambda \in K$:
\bse
f((\lambda\odot v_1)\oplus v_2) = (\lambda\boxdot f( v_1))\boxplus f(v_2)
\ese
\ed

From now on, we will drop the special notation for the vector space operations and suppress the dot for scalar
multiplication. For instance, we will write the equation above as $f(\lambda v_1+v_2)=\lambda f(v_1)+f(v_2)$, hoping
that this will not cause any confusion.

\bd [Linear Isomorphism]
A bijective linear map (or a bijective homomorphism) is called a \textbf{linear isomorphism} of vector spaces.
\ed

\bd [Isomorphic Vector Spaces]
Two vector spaces are said to be \textbf{isomorphic} if there exists a linear isomorphism between them. We write
$V\cong_\mathrm{vec}W$.
\ed

Based on the linear maps (a.k.a.\ homomorphisms) and bijective linear maps (a.k.a.\ an isomorphisms) we can define three
important notions of vector spaces: $\mathrm{Hom}(V,W)$, $\mathrm{End}(V)$ and $\mathrm{Aut}(V)$.

\bd [$\mathrm{Hom}(V,W)$]
Let $V$ and $W$ be vector spaces over the same field $K$. We define the set $\mathrm{Hom}(V,W)$ as the set of all
linear maps (a.k.a.\ all homomorphisms) between $V$ and $W$:
\bse
\mathrm{Hom}(V,W) \coloneqq \{f \mid f\cl V\xrightarrow{\sim}W \}
\ese
\ed

$\mathrm{Hom}(V,W)$ can itself be made into a vector space over $K$ by defining:
\bi{rrCl}
\diamondplus \cl &\mathrm{Hom}(V,W) \times \mathrm{Hom}(V,W) &\to &\mathrm{Hom}(V,W) \\
& (f,g) & \mapsto & f \diamondplus g
\ei

where:
\bi{rcCl}
f \diamondplus g \cl &V &\xrightarrow{\sim} &W \\ & v & \mapsto & (f \diamondplus g)(v) \coloneqq f(v)+g(v)
\ei

and:
\bi{rrCl}
\diamonddot \cl &K \times \mathrm{Hom}(V,W) &\to &\mathrm{Hom}(V,W) \\ & (\lambda,f) & \mapsto & \lambda \diamonddot f
\ei

where:
\bi{rcCl}
\lambda \diamonddot f \cl &V &\xrightarrow{\sim} &W\\ & v & \mapsto & (\lambda \diamonddot f)(v) \coloneqq \lambda f(v)
\ei

\v

It is easy to check that both $f \diamondplus g$ and $\lambda \diamonddot f$ are indeed linear maps from $V$ to $W$.
For instance, we have:

\bi{rCl"s}
(\lambda \diamonddot f)(\mu v_1+v_2) & = & \lambda f(\mu v_1+v_2) & (by definition)\\
& = & \lambda (\mu f( v_1)+f(v_2)) & (since $f$ is linear)\\
& = & \lambda \mu f( v_1)+\lambda f(v_2) & (by axioms i and iii)\\
& = & \mu \lambda f( v_1)+\lambda f(v_2) & (since $K$ is a field)\\
& = & \mu (\lambda \diamonddot f)( v_1)+(\lambda \diamonddot f)(v_2) &
\ei

so that $\lambda \diamonddot f\in \mathrm{Hom}(V,W)$. One should also check that $\diamondplus$ and $\diamonddot$
satisfy the vector space axioms.

\bd [Endomorphisms]
Let $V$ be a vector space. An \textbf{endomorphism} of $V$ is a linear map $V\to V$. In other words an endomorphism is a
homomorphism whose domain equals the target.
\ed

\bd [$\mathrm{End}(V)$]
Let $V$ be a vector space. We define the set $\mathrm{End}(V)$ as the set of all endomorphisms of $V$:
\bse
\mathrm{End}(V) \coloneqq \mathrm{Hom}(V,V)
\ese
\ed

\v

It is easy to show that $\mathrm{End}(V)$ can again itself be made into a vector space over $K$.

\bd [Automorphism]
Let $V$ be a vector space. An \textbf{automorphism} of $V$ is a linear isomorphism $V\to V$. In other words an
automorphism is an endomorphism that is also an isomorphism.
\ed

\bd [$\mathrm{Aut}(V)$]
Let $V$ be a vector space. We define the set $\mathrm{Aut}(V)$ as the set of all automorphisms of $V$:
\bse
\mathrm{Aut}(V) \coloneqq \{f \in \mathrm{End}(V) \mid f \text{ is an isomorphism}\}
\ese
\ed

Note that $\mathrm{Aut}(V)$ \textbf{cannot} be made into a vector space. It is however a group under the operation of
composition of linear maps.

\bd [Dual Vector Space]
Let $V$ be a vector space over $K$. The \textbf{dual vector space} to $V$ is:
\bse
V^* \coloneqq \Hom(V,K)
\ese

where $K$ is considered as a vector space over itself.
\ed

The dual vector space to $V$ is the vector space of linear maps from $V$ to the underlying field $K$, which are
variously called \emph{linear functionals}, \emph{covectors}, or \emph{one-forms} on $V$. The dual plays a very
important role, in that from a vector space and its dual, we will construct the tensor space.

\bd [Bilinear Maps]
Let $V$, $W$, $Z$ be vector spaces over $K$. A map $f\cl V\times W \to Z$ is said to be \textbf{bilinear} if:
\bit
\item $\forall \, w\in W:\forall \, v_1,v_2\in V: \forall \,\lambda \in K : f(\lambda v_1+v_2,w)=\lambda f(v_1,w) +
f(v_2,w).$
\item $\forall \, v\in V:\forall \, w_1,w_2\in W: \forall \,\lambda \in K : f(v,\lambda w_1+w_2)=\lambda f(v,w_1) +
f(v,w_2).$
\eit

i.e.\ if the maps $v\mapsto f(v,w)$, for any fixed $w$, and $w\mapsto f(v,w)$, for any fixed $v$, are both linear as
maps $V\to Z$ and $W\to Z$, respectively.
\ed

Compare this with the definition of a linear map $f\cl V\times W \xrightarrow{\sim} Z$:
\bse
\forall \, x,y\in V \times W : \forall \, \lambda \in K : f(\lambda x+y)=\lambda f(x)+f(y)
\ese

More explicitly, if $x=(v_1,w_1)$ and $y = (v_2,w_2)$, then:
\bse
f(\lambda (v_1,w_1)+(v_2,w_2))=\lambda f((v_1,w_1))+f((v_2,w_2))
\ese

A bilinear map out of $V\times W$ is \emph{not} the same as a linear map out of $V\times W$. In fact, bilinearity is
just a special kind of non-linearity.

\be
The map $f\cl \R^2\to \R$ given by $(x,y)\mapsto x+y$ is linear but not bilinear, while the map $(x,y)\mapsto xy$ is
bilinear but not linear.
\ee

We can immediately generalise the above to define \emph{multilinear} maps out of a Cartesian product of vector spaces.

\bd [Tensors]
Let $V$ be a vector space over $K$. A \textbf{$(p,q)$-tensor} $T$ on $V$ is a multilinear map:
\bse
T\cl \underbrace{V^*\times\cdots \times V^*}_{p \text{ copies}} \times
\underbrace{V \times \cdots \times V}_{q\text{ copies}} \to K
\ese
\ed

By convention, a $(0,0)$ on $V$ is just an element of $K$, and hence, $T^0_0 V=K$.

\bd [Covariant / Contravariant Tensor]
A type $(p,0)$ tensor is called a \textbf{covariant $p$-tensor}, while a tensor of type $(0,q)$ is called a
\textbf{contravariant $q$-tensor}.
\ed

Now we are ready to define the set of all $(p,q)$-tensors as follows.

\bd [$T^p_q V$]
We define the set of all $(p,q)$-tensors $T$ as:
\bse
T^p_q V \coloneqq \underbrace{V\otimes\cdots \otimes V}_{p \text{ copies}} \otimes \underbrace{V^*
\otimes \cdots \otimes V^*}_{q \text{ copies}} \coloneqq \{T\mid T \text{ is a $(p,q)$-tensor on }V\}
\ese
\ed

Note that to define $T^p_q V$ as a set, we should be careful and invoke the principle of restricted comprehension, i.e.\
we should say where the $T$s are coming from. \v

In general, say we want to build a set of maps:
\bse
f\cl A\to B
\ese

satisfying some property $p$. Recall that this notation is hiding the fact that is a relation (indeed, a functional
relation), and a relation between $A$ and $B$ is a subset of $A\times B$. Therefore, we ought to write:
\bse
\{f\in \cP(A\times B)\mid f\cl A\to B \text{ and } p(f)\}
\ese

In the case of $T^p_q V$ we have:
\bse
T^p_q V \coloneqq \big\{T \in \cP\bigl(\underbrace{V^*\times\cdots \times V^*}_{p \text{ copies}} \times \underbrace{V
\times \cdots \times V}_{q \text{ copies}} \times K\bigr) \mid T \text{ is a $(p,q)$-tensor on }V\big\}
\ese

although we will not write this down every time. \v

The set $T^p_q V$ can be equipped with a $K$-vector space structure by defining:
\bi{rrCl}
\oplus\cl &T^p_q V \times T^p_q V &\to &T^p_q V \\ & (T,S) & \mapsto & T \oplus S
\ei

and:
\bi{rrCl}
\odot \cl &K \times T^p_q V &\to &T^p_q V \\ & (\lambda,T) & \mapsto & \lambda \odot T
\ei

where $T \oplus S$ and $\lambda \odot T$ are defined pointwise, as we did with $\mathrm{Hom}(V,W)$. \v

We now define an important way of obtaining a new tensor from two given ones.

\bd [Tensor Product]
Let $T\in T^p_q V$ and $S\in T^r_s V$. The \textbf{tensor product} of $T$ and $S$ is the tensor
$T\otimes S\in T^{p+r}_{q+s}V$ defined by:
\bi{rl}
(T\otimes S)(\omega_1,\ldots,\omega_p,\omega_{p+1},\ldots,\omega_{p+r},v_1, &\ldots,v_q,v_{q+1},\ldots,v_{q+s})\\
\coloneqq T(\omega_1,\ldots,\omega_p,v_1,&\ldots,v_q)\,S(\omega_{p+1}, \ldots,\omega_{p+r},v_{q+1},\ldots,v_{q+s})
\ei

with $\omega_i\in V^*$ and $v_i\in V$.
\ed

Some examples are in order.

\be
\ben[label=\alph*)]
\item $T^0_1 V \coloneqq \{T\mid T\cl V \xrightarrow{\sim} K\} = \mathrm{Hom}(V,K) \eqqcolon V^*$. Note that here
multilinear is the same as linear since the maps only have one argument.
\item $T^1_1 V\equiv V\otimes V^* \coloneqq \{T\mid T\text{ is a bilinear map }V^*\times V \to K\}$. We claim that
this is the same as $\mathrm{End}(V^*)$. Indeed, given $T\in V\otimes V^*$, we can construct $\widehat T \in
\mathrm{End}(V^*)$ as follows:
\bi{rrCl}
\widehat T \cl &V^* &\xrightarrow{\sim}& V^* \\ & \omega & \mapsto & T(-,\omega)
\ei

where, for any fixed $\omega$, we have:
\bi{rrCl}
T (-,\omega) \cl &V &\xrightarrow{\sim}& K \\ & v & \mapsto & T(v,\omega)
\ei

The linearity of both $\widehat T$ and $T(-,\omega)$ follows immediately from the bilinearity of $T$. Hence, $T(-,
\omega)\in V^*$ for all $\omega$, and $\widehat T \in \mathrm{End}(V^*)$. This correspondence is invertible, since
can reconstruct $T$ from $\widehat T$ by defining:
\bi{rrCl}
T \cl &V \times V^* &\to & K \\ & (v,\omega) & \mapsto & T(v,\omega) \coloneqq (\widehat T(\omega))(v)
\ei

The correspondence is in fact linear, hence, an isomorphism, and thus:

\bse
T^1_1V\cong_\mathrm{vec}\mathrm{End}(V^*)
\ese

\item $T^0_1 V \stackrel{?}{\cong}_\mathrm{vec} V$: while you will find this stated as true in some physics
textbooks, it is in fact \emph{not true} in general.
\item $T^1_1 V \stackrel{?}{\cong}_\mathrm{vec} \mathrm{End}(V)$: This is also not true in general.
\item $(V^*)^* \stackrel{?}{\cong}_\mathrm{vec} V$: This only holds if $V$ is finite-dimensional (we will define the
dimensions of a vector space in the next section).
\een
\ee

\subsection{Basis Of Vector Spaces}

In general a set $B$ of vectors in a vector space $V$ is called a basis if every element of $V$ may be written in a
unique way as a finite linear combination of elements of $B$. The coefficients of this linear combination are
referred to as components or coordinates of the vector with respect to $B$ and the elements of a basis are called
basis vectors. Equivalently, a set $B$ is a basis if its elements are linearly independent and every element of $V$
is a linear combination of elements of $B$. In other words, a basis is a linearly independent spanning set. We will
see all of those in detail in a while. \v

Given a vector space without any additional structure, the only notion of basis that we can define is a so-called
Hamel basis. In order to do so, we first need to define the notion of ``span''.

\bd [Span]
Given a vector space $V$ over a field $K$, the \textbf{span} of a set $S$ of vectors $\{s_1,\ldots,s_N\}$ of $V$ is
defined to be the set of all finite linear combinations of elements (vectors) of $S$:
\bse
\lspan_K(\mathcal{S}) \coloneqq \bigg\{\sum_{i=1}^n\lambda^is_i \ \Big| \ \lambda^i\in K, s_i\in S, n \geq 1\bigg\}
\ese
\ed

Now we are ready to define the so called ``Hamel basis''.

\bd [Hamel Basis]
Let $(V,+,\cdot)$ be a vector space over $K$. A subset $\mathcal{B}\se V$ is called a \textbf{Hamel basis} for $V$ if:
\bit
\item Every finite subset $\{b_1,\ldots,b_N\}$ of $\mathcal{B}$ is linearly independent, i.e.\ :
\bse
\sum_{i=1}^N \lambda^ib_i = 0 \ \imp \ \lambda^1 = \cdots = \lambda^N = 0
\ese

\item The span of $\mathcal{B}$ can recreate the whole $V$, i.e.\ :
\bse
V = \lspan_K(\mathcal{B}) \implies \forall \, v \in V : \exists \, v^1, \ldots,v^M\in K :
\exists \, b_1,\ldots,b_M \in \mathcal{B}:v=\ds \sum_{i=1}^Mv^ib_i
\ese
\eit
\ed

Once we have a basis $\mathcal{B}$, the expansion of $v\in V$ in terms of elements of $\mathcal{B}$ is, in fact,
unique. Hence, we can meaningfully speak of the \emph{components} of $v$ in the basis $\mathcal{B}$. \v

Note that we have been using superscripts for the elements of $K$, and these should not be confused with exponents. \v

The following characterisation of a Hamel basis is often useful.

\bt[]
Let $V$ be a vector space and $\mathcal{B}$ a Hamel basis of $V$. Then $\mathcal{B}$ is a minimal spanning and
maximal independent subset of $V$, i.e.\ if $S\se V$, then:
\bit \item $\lspan(S) = V\ \Rightarrow \ |S| \geq |\mathcal{B}|.$
\item $S$ is linearly independent $\ \Rightarrow\ |S| \leq |\mathcal{B}|.$
\eit
\et

Now by making use of the basis, let's provide the definition of the dimension of a vector space.

\bd [Dimension Of Vector Space]
Let $V$ be a vector space. The \textbf{dimension} of $V$ is $\dim V \coloneqq |\mathcal{B}|$, where $\mathcal{B}$ is
a Hamel basis for $V$.
\ed

Even though we will not prove it, it is the case that every Hamel basis for a given vector space has the same
cardinality, and hence, the notion of dimension is well-defined.

\bt[]
If $\dim V < \infty$ and $S\se V$, then we have the following:
\bit \item If $\lspan_K(S) = V$ and $|S| = \dim V$, then $S$ is a Hamel basis of $V.$
\item If $S$ is linearly independent and $|S| = \dim V$, then $S$ is a Hamel basis of $V.$
\eit
\et

\bt[]
If $\dim V < \infty$, then $(V^*)^*\cong_\mathrm{vec}V$.
\et

Note that while we need the concept of basis to state this result (since we require $\dim V < \infty$), the
isomorphism that we have constructed is independent of any choice of basis. \v

While a choice of basis often simplifies things, when defining new objects it is important to do so without making
reference to a basis. If we do define something in terms of a basis (e.g.\ the dimension of a vector space), then we
have to check that the thing is well-defined, i.e.\ it does not depend on which basis we choose. \v

If $V$ is finite-dimensional, then $V^*$ is also finite-dimensional and $V\cong_\mathrm{vec}V^*$. Moreover, given a
basis $\mathcal{B}$ of $V$, there is a spacial basis of $V^*$ associated to $\mathcal{B}$.

\bd [Dual Basis]
Let $V$ be a finite-dimensional vector space with basis $\mathcal{B}=\{e_1, \ldots,e_{\dim V}\}$. The \textbf{dual
basis} to $\mathcal{B}$ is the unique basis $\mathcal{B'}=\{\epsilon^1, \ldots,\epsilon^{\dim V}\}$ of $V^*$ such that:
\bse
\forall \, 1\leq i,j \leq \dim V :\quad \epsilon^i(e_j) = \delta^i_j \coloneqq
\begin{cases} 1 \quad \text{if }i=j\\0 \quad \text{if} i\neq j \end{cases}
\ese
\ed

If $V$ is finite-dimensional, then $V$ is isomorphic to both $V^*$ and $ (V^*)^*$. In the case of $V^*$, an
isomorphism is given by sending each element of a basis $\mathcal{B}$ of $V$ to a different element of the dual basis
$\mathcal{B}'$, and then extending linearly to $V$. \v

You will (and probably already have) read that a vector space is \emph{canonically} isomorphic to its double dual,
but \emph{not} canonically isomorphic to its dual, because an arbitrary choice of basis on $V$ is necessary in order
to provide an isomorphism. \v

Finally, by using a basis (and its dual) we can define the components of a tensor as follows.

\bd [Components Of A Tensor]
Let $V$ be a finite-dimensional vector space over $K$ with basis $\mathcal{B}=\{e_1,\ldots,e_{\dim V}\}$ and dual
basis $\{\epsilon^1,\ldots,\epsilon^{\dim V}\}$ and let $T\in T^p_q V$. We define the \textbf{components} of $T$ in the
basis $\mathcal{B}$ to be the numbers:
\bse
T^{a_1\ldots a_p}_{\phantom{a_1\ldots a_p}b_1\ldots b_q} \coloneqq T(\epsilon^{a_1},
\ldots, \epsilon^{a_p},e_{b_1}, \ldots, e_{b_q})\in K
\ese

where $1\leq a_i,b_j\leq \dim V$.
\ed

Just as with vectors, the components completely determine the tensor. Indeed, we can reconstruct the tensor from its
components by using the basis:
\bse
T = \underbrace{\sum_{a_1=1}^{\dim V}\!\cdots\!\sum_{b_q=1}^{\dim V}}_{p+q\text{ sums}}
T^{a_1 \ldots a_p}_{\phantom{a_1\ldots a_p}b_1\ldots b_q} e_{a_1}\otimes\cdots\otimes e_{a_p} \otimes
\epsilon^{b_1}\otimes \cdots\otimes \epsilon^{b_q}
\ese

where the $e_{a_i}$s are understood as elements of $T^1_0 V\cong_\mathrm{vec}V$ and the $\epsilon^{b_i}s$ as elements
of $T^0_1 V\cong_\mathrm{vec}V^*$. Note that each summand is a $(p,q) $-tensor and the (implicit) multiplication
between the components and the tensor product is the scalar multiplication in $T^p_q V$.

\subsubsection*{Notational Conventions}

From now on, we will employ the Einstein's summation convention, which consists in suppressing the summation sign
when the indices to be summed over each appear once as a subscript and once as a superscript in the same term. \v

For example, we write:
\bi{rcl}
v=v^ae_a, \qquad \omega=\omega_a \epsilon^a \qquad &\text{and}&
\qquad T=T^{ab}_{\phantom{ab}c}e_a\otimes e_b \otimes \epsilon^c
\ei

instead of:
\bi{rcl}
v=\sum_{a=1}^dv^ae_a, \qquad \omega= \sum_{a=1}^d \omega_a \epsilon^a \qquad &\text{and}&
\qquad T=\sum_{a=1}^d\sum_{b=1}^d\sum_{c=1}^d T^{ab}_{\phantom{ab}c}e_a\otimes e_b \otimes \epsilon^c
\ei

Indices that are summed over are called \emph{dummy indices}. They always appear in pairs, and clearly it doesn't
matter which particular letter we choose to denote them, provided it doesn't already appear in the expression. \v

Indices that are not summed over are called \emph{free indices}. Expressions containing free indices represent
multiple expressions, one for each value of the free indices. Free indices must match on both sides of an equation.
The ranges over which the indices run are usually understood and not written out. \v

The convention on which indices go upstairs and which downstairs (which we have already been using) is that:
\bit
\item The basis vectors of $V$ carry downstairs indices.
\item The basis vectors of $V^*$ carry upstairs indices.
\item All other placements are enforced by the Einstein's summation convention.
\eit

For example, since the components of a vector must multiply the basis vectors and be summed over, the Einstein's
summation convention requires that they carry upstair indices.

\be
Using the summation convention, we have:
\ben[label=\alph*)]
\item $\epsilon^a(v) = \epsilon^a(v^b e_b)=v^b\epsilon^a(e_b)=v^b\delta^a_b=v^a.$
\item $\omega(e_b)=(\omega_a\epsilon^a)(e_b)=\omega_a\epsilon^a(e_b)=\omega_b.$
\item $\omega(v)=\omega_a\epsilon^a(v^b e_b)=\omega_a v^a.$
\een

where $v\in V$, $\omega \in V^*$, $\{e_i\}$ is a basis of $V$ and $\{\epsilon^j\}$ is the dual basis to $\{e_i\}$.
\ee

The Einstein's summation convention should only be used when dealing with linear spaces and multilinear maps. The
reason for this is the following. Consider a map $\phi\cl V\times W \to Z$, and let $v=v^i e_i\in V$ and
$w=w^i\widetilde e_i\in W$. Then we have:
\bse
\phi(v,w) =
\phi\,\bigg({\color{lightgray}\sum_{i=1}^d}v^ie_i,{\color{lightgray}\sum_{j=1}^{\widetilde{d}}} w^j\widetilde e_j\bigg)
= {\color{lightgray}\sum_{i=1}^d \sum_{j=1}^{\widetilde{d}}}\phi(v^ie_i,w^j\widetilde e_j)
= {\color{lightgray}\sum_{i=1}^d \sum_{j=1}^{\widetilde{d}}}v^iw^j\phi(e_i,\widetilde e_j)
\ese

Note that by suppressing the greyed out summation signs, the second and third term above are indistinguishable. But
this is only true if $\phi$ is bilinear! hence, the summation convention should not be used (at least, not without
extra care) in other areas of mathematics.

\subsubsection*{Matrix Representation}

Having chosen a basis for $V$ and the dual basis for $V^*$, it is very tempting to think of $v=v^i e_i\in V$ and
$\omega=\omega_i\epsilon^i\in V^*$ as $d$-tuples of numbers. In order to distinguish them, one may choose to write
vectors as \emph{columns} of numbers and covectors as \emph{rows} of numbers:
\bse
v =v^ie_i \quad \leftrightsquigarrow\quad v\ \hat{=} \left(\ba{c}v^1\ \\ vdots \\ v^d \ea \right)
\ese

and:
\bse
\omega =\omega_i\epsilon^i \quad \leftrightsquigarrow\quad \omega \ \hat{=} \ (\omega_1,\ldots,\omega_d)
\ese

\v

Given $\phi\in\mathrm{End}(V)\cong_\mathrm{vec}T^1_1 V$, recall that we can write $\phi = \phi^i_{\phantom{i}j}\,
e_i\otimes \epsilon^j$, where $\phi^i_{\phantom{i}j} \coloneqq \phi (\epsilon^i,e_j)$ are the components of $\phi$
with respect to the chosen basis. It is then also very tempting to think of $\phi$ as a square array of numbers:
\bse
\phi = \phi^i_{\phantom{i}j}\, e_i\otimes \epsilon^j \quad \leftrightsquigarrow\quad \phi \ \hat{=} \left(\ba{cccc}
\phi^1_{\phantom{1}1} & \phi^1_{\phantom{1}2} & \cdots & \phi^1_{\phantom{1}d}\\
\phi^2_{\phantom{2}1} & \phi^2_{\phantom{2}2} & \cdots & \phi^2_{\phantom{2}d}\\ \vdots & \vdots & \ddots & \vdots\\
\phi^d_{\phantom{d}1} & \phi^d_{\phantom{d}2} & \cdots & \phi^d_{\phantom{d}d} \ea \right)
\ese

\v

The convention here is to think of the $i$ index on $\phi^i_{\phantom{i}j}$ as a \emph{row index}, and of $j$ as a
\emph{column index} (we cannot stress enough that this is pure convention). Hence, once we start using the ``matrix
representation`` (although technically it shouldn't be called representation since as we will see the word
``representation'' means something else), we can then express all the linear maps $\phi$ of $V$ (a.k.a.\ all the
elements of $\mathrm{End}(V)$) as $n \times n$ matrices. This coincides with the usual picture we have in physics,
where all the vectors are represented by a column vector of size $n$ and all the linear transformations are
represented by $n \times n$ matrices that act on $v$ and produce another vector $w$ (Hence, the $\mathrm{End}(V)$). \v

Going one step further, notice that not all matrices have an inverse. This coincides with the fact that not all
linear maps have an inverse. Since $\mathrm{End}(V)$ contains all linear maps, it also contains maps that are not
linear isomorphisms (a.k.a.\ maps that are not bijections, a.k.a.\ maps that do not have an inverse). However, if we
restrict ourselves more, from linear maps to linear isomorphisms then we move from $\mathrm{End}(V)$ to $
\mathrm{Aut}(V) \coloneqq \{\phi \in \mathrm{End}(V) \mid \phi \text{ is an isomorphism}\}$. And if we switch again
to the ``matrix representation'', now we are dealing with matrices that do have an inverse. We call the set of all
these matrices ``General Linear Group'' and we denote by $GL(V)$ (we can indeed equip this set with matrix
multiplication and show that it is closed under the operation, hence, the ``group'' in the name). From there we can
then restrict our transformations even more, and then we can get for example the ``Special Linear Group'' denoted by
$SL(V)$ etc \ldots \v

For the sake of completeness, let us make a final note that uses the concept of the determinant that we introduce in
the next section. Another way to say that a map is an isomorphism is to say that the determinant of the map $\det
\phi\neq 0$. This condition is a so-called \emph{open condition}, meaning that $\mathrm{GL}(V)$ can be identified
with an open subset of $V$, from which it then inherits a smooth structure and hence, the inverse. By using this we
can write $\mathrm{GL}(V)= \mathrm{Aut}(V) = \{\phi \in \mathrm{End}(V) \mid \det \phi \neq 0 \}$. Or in other words
since automorphisms are linear isomorphisms between a space and itself, $ GL(V) = \{\phi: V \xrightarrow{\sim} V \mid
\det \phi \neq 0\}$, which coincides with the ``matrix representation'' of $\mathrm{GL}(V)$ where a matrix has an
inverse only when its determinant is non-vanishing.

\be
If $\dim V<\infty$, then we have $\mathrm{End}(V)\cong_\mathrm{vec}T^1_1 V$. Explicitly, if $\phi \in \mathrm{End}(V)
$, we can think of $\phi \in T^1_1 V$, using the same symbol, as:
\bse
\phi(\omega,v) \coloneqq \omega(\phi(v))
\ese

\v

Hence, the components of $\phi\in\mathrm{End}(V)$ are $\phi^a_{\phantom{a}b} \coloneqq \epsilon^a(\phi(e_b))$. \v

Similarly, $\omega(v)=\omega_m v^m$ can be thought of as the \emph{dot product} $\omega \cdot v\equiv\omega^T v$, and:
\bse
\phi(v,w)=w_a\,\phi^a_{\phantom{a}b}\,v^b \quad \leftrightsquigarrow \quad \omega^T\phi v
\ese

The last expression could mislead you into thinking that the transpose is a ``good'' notion, but in fact it is not.
It is very bad notation. It almost pretends to be basis independent, but it is not at all. \v

Now consider $\phi,\psi\in\mathrm{End}(V)$. Let us determine the components of $\phi\circ \psi$. We have:
\bi{rCl}
(\phi\circ \psi)^a_{\phantom{a}b} & \coloneqq & (\phi\circ \psi)(\epsilon^a, e_b)\\
& \coloneqq &\epsilon^a( (\phi\circ \psi)(e_b))\\
&=& \epsilon^a( (\phi(\psi(e_b)))\\
&=& \epsilon^a(\phi(\psi^m_{\phantom{m}b}\,e_m))\\
&=& \psi^m_{\phantom{m}b} \epsilon^a( \phi(e_m))\\
& \coloneqq & \psi^m_{\phantom{m}b}\, \phi^a_{\phantom{a}m}
\ei

The multiplication in the last line is the multiplication in the field $K$, and since that's commutative, we have
$\psi^m_{\phantom{m}b}\, \phi^a_{\phantom{a}m} = \phi^a_{\phantom{a}m} \, \psi^m_{\phantom{m}b}$. \v

However, in light of the convention introduced in the previous remark, the latter is preferable. Indeed, if we think
of the superscripts as row indices and of the subscripts as column indices, then $\phi^a_{\phantom{a}m} \,
\psi^m_{\phantom{m}b}$ is the entry in row $a$, column $b$, of the matrix product $\phi\psi$.
\ee

The moral of the story is that you should try your best \emph{not} to think of vectors, covectors and tensors as
arrays of numbers. Instead, always try to understand them from the abstract, intrinsic, component-free point of view.

\subsection{Change Of Basis}

Let $V$ be a vector space over $K$ with $d=\dim V < \infty$ and let $\{e_1, \ldots,e_d\}$ be a basis of $V$. Consider
a new basis $\{\widetilde e_1,\ldots,\widetilde e_d\}$. Since the elements of the new basis are also elements of $V$,
we can expand them in terms of the old basis. We have:
\bse
\widetilde e_a = \sum_{b=1}^d A^b_{\phantom{b} a} e_b = A^b_{\phantom{b}a} e_b
\ese

for some $A^b_{\phantom{b}a} \in K$. Similarly, we have:
\bse
e_a= \sum_{m=1}^d B^m_{\phantom{m} a} \widetilde e_m = B^m_{\phantom{m}a}\widetilde e_m
\ese

for some $B^m_{\phantom{m}a} \in K$. It is a standard linear algebra result that the matrices $A$ and $B$, with
entries $A^b_{\phantom{b}a}$ and $B^b_{\phantom{b}a} $ respectively, are invertible and, in fact, $A^{-1}=B$. Note
that in index notation, the equation $AB=I$ reads $A^a_{\phantom{a}m}B^m_{\phantom{m}b}=\delta^a_b$. \v

We now investigate how the components of vectors and covectors change under a change of basis.
\ben[label=\alph*)]
\item Let $v=v^a e_a=\widetilde v^a\widetilde e_a\in V$. Then:
\bse
v^a = \epsilon^a(v) = \epsilon^a(\widetilde v^b\widetilde e_b) = \widetilde v^b \epsilon^a(\widetilde e_b)
= \widetilde v^b \epsilon^a (A^m_{\phantom{m}b}e_m)
= A^m_{\phantom{m}b} \widetilde v^b\epsilon^a(e_m)
= A^a_{\phantom{a}b} \widetilde v^b
\ese

\item Let $\omega = \omega_a\epsilon^a = \widetilde \omega_a\widetilde \epsilon^a \in V^*$. Then:
\bse
\omega_a \coloneqq \omega(e_a) = \omega(B^m_{\phantom{m}a}\widetilde e_m) = B^m_{\phantom{m}a}\omega(\widetilde e_m)
= B^m_{\phantom{m}a}\widetilde \omega_m
\ese
\een

Summarising, for $v\in V$, $\omega \in V^*$ and $\widetilde e_a=A^b_{\phantom{b}a}e_b$, we have:
\bi{rClcrCl}
v^a & = & A^a_{\phantom{a}b} \widetilde v^b &\qquad & \omega_a &= & B^b_{\phantom{b}a}\widetilde \omega_b \\
\widetilde v^a & = & B^a_{\phantom{a}b} v^b & & \widetilde \omega_a &= & A^b_{\phantom{b}a}\omega_b
\ei

The result for tensors is a combination of the above, depending on the type of tensor.

\ben
\item[c)] Let $T\in T^p_q V$. Then:
\bse
T^{a_1\ldots a_p}_{\phantom{a_1\ldots a_p}b_1\ldots b_q} = A^{a_1}_{\phantom{a_1}m_1}\cdots
A^{a_p}_{\phantom{a_p}m_p} B^{n_1}_{\phantom{n_1}b_1} \cdots B^{n_q}_{\phantom{n_q}b_q} \widetilde T^{m_1\ldots
m_p}_{\phantom{m_1\ldots m_p}n_1\ldots n_q}
\ese

i.e.\ the upstair indices transform like vector indices, and the downstair indices transform like covector indices.
\een

Coming back (once again) to the ``matrix representation'', let's see now one of the biggest misunderstandings that
might come up when we want to perform a change of basis for tensors. \v

Recall that, if $\phi \in T^1_1 V$, then we can arrange the components $\phi^a_{\phantom{a}b}$ in matrix form:
\bse
\phi = \phi^a_{\phantom{a}b}\, e_a\otimes \epsilon^b \quad \leftrightsquigarrow\quad \phi \ \hat{=} \left(\ba{cccc}
\phi^1_{\phantom{1}1} & \phi^1_{\phantom{1}2} & \cdots & \phi^1_{\phantom{1}d}\\
\phi^2_{\phantom{2}1} & \phi^2_{\phantom{2}2} & \cdots & \phi^2_{\phantom{2}d}\\
\vdots & \vdots & \ddots & \vdots\\
\phi^d_{\phantom{d}1} & \phi^d_{\phantom{d}2} & \cdots & \phi^d_{\phantom{d}d} \ea \right)
\ese

\v

Similarly, if we have $g\in T^0_2 V$, its components are $g_{ab} \coloneqq g(e_a,e_b)$ and we can write:
\bse
g = g_{ab}\, \epsilon^a\otimes \epsilon^b \quad \leftrightsquigarrow\quad g\ \hat{=} \left(\ba{cccc}
g_{11} & g_{12} & \cdots & g_{1d}\\
g_{21} & g_{22} & \cdots & g_{2d}\\
\vdots & \vdots & \ddots & \vdots\\
g_{d1} & g_{d2} & \cdots & g_{dd} \ea \right)
\ese

\v

Needless to say that these two objects could not be more different if they tried. Indeed:

\bit \item $\phi$ is an endomorphism of $V$. The first index in $\phi^a_{\phantom{a}b}$ transforms like a vector
\item index, while the second index transforms like a covector index. \item $g$ is a \emph{bilinear form} on $V$.
both indices in $g_{ab}$ transform like covector indices.
\eit

In linear algebra, you may have seen the two different transformation laws for these objects:
\bse
\phi \to A^{-1}\phi A \qquad \text{and} \qquad g \to A^TgA
\ese

where $A$ is the change of basis matrix. However, once we fix a basis, the matrix representations of these two
objects are indistinguishable. It is then very tempting to think that what we can do with a matrix, we can just as
easily do with another matrix. \v

For instance, the eigenvector equation reads:
\bse
A^{a}_{m} v^m = \lambda v^a
\ese

It is clear from the indices that $A^{a}_{m}$ is the components of an endomorphism. If we attempt to write a similar
equation for a bilinear from we will simply fail:
\bse
g_{m n} v^m = \lambda \underbrace{v_n}_{\text{(?)}}
\ese

Indeed, the eigenvectors equation holds only for endomorphisms, while bilinear forms carry the so called
``signature''. \v

Another example: if we have a rule to calculate the determinant of a square matrix, we should be able to apply it to
both of the above matrices. However, the notion of determinant is \emph{only} defined for endomorphisms. The only way
to see this is to give a basis-independent definition, i.e.\ a definition that does not involve the ``components of a
matrix''. Let's do that!

\subsection{Determinants}

In your previous course on linear algebra, you may have met the determinant of a square matrix as a number calculated
by applying a mysterious rule. Using the mysterious rule, you may have shown, with a lot of work, that for example,
if we exchange two rows or two columns, the determinant changes sign. But, as we have seen, matrices are the result
of pure convention. Hence, one more polemic remark is in order. \v

We will need some preliminary definitions (we will define $n$-forms in a more proper way in next chapters, so for now
do not spend a lot of time on them).

\bd [$n$-\emph{form}]
Let $V$ be a $d$-dimensional vector space. An $n$-\textbf{form} on $V$ is a $(0,n)$-tensor $\omega$ that is
\emph{totally antisymmetric}, i.e.\ :
\bse
\forall \, \pi \in S_n : \ \omega(v_1,v_2,\ldots,v_n)
= \mathrm{sgn}(\pi)\, \omega(v_{\pi(1)},v_{\pi(2)},\ldots, v_{\pi(n)})
\ese
\ed

Note that a $0$-form is a scalar, and a $1$-form is a covector. A $d$-form is also called a \emph{top form}, and one
can show that for two top forms $\omega$ and $\omega'$ the following holds:
\bse
\forall \, \omega,\omega' \in \Lambda^dV : \exists \, c \in K : \ \omega = c\, \omega'
\ese

i.e.\ there is essentially only one top form on $V$, up to a scalar factor.

\bd [Choice Of Volume]
A choice of top form on $V$ is called a \textbf{choice of volume} form on $V$. A vector space with a chosen volume
form is then called a \emph{vector space with volume}.
\ed

This terminology is due to the next definition.

\bd [Volume]
Let $\dim V = d$ be the dimension of vector space $V$ and let $v_1,\ldots, v_d\in V$, be $d$ vectors in $V$. Then the
\textbf{volume} spanned by $v_1,\ldots,v_d$ is:
\bse
\vol(v_1,\ldots,v_d) \coloneqq \omega(v_1,\ldots,v_d)
\ese

where $\omega$ is the (chosen) top form.
\ed

Intuitively, the antisymmetry condition on $\omega$ makes sure that $\vol (v_1,\ldots,v_d)$ is zero whenever the set
$\{v_1,\ldots,v_d\}$ is not linearly independent. Indeed, in that case $v_1, \ldots,v_d$ could only span a $(d-1)
$-dimensional hypersurface in $V$ at most, which should have $0$ volume. \v

You may have rightfully thought that the notion of volume would require some extra structure on $V$, such as a notion
of length or angles, and hence, an inner product. But instead, we only need a top form. \v

We are finally ready to define the determinant.

\bd [Determinant]
Let $V$ be a $d$-dimensional vector space and let $\phi \in \End (V) \cong_\mathrm{vec}T^1_1 V$. The
\textbf{determinant} of $\phi$ is:
\bse
\det \phi \coloneqq \frac{\omega(\phi(e_1),\ldots,\phi(e_d))}{\omega(e_1,\ldots,e_d)}
\ese

\v

for some top form $\omega$ and some basis $\{e_1,\ldots,e_d\}$ of $V$.
\ed

The first thing we need to do is to check that this is well-defined. That $\det \phi$ is independent of the choice of
$\omega$ is clear, since if $\omega,\omega'$ are top forms, then there is a $c \in K$ such that $\omega = c\,
\omega'$, and hence:
\bse
\frac{\omega(\phi(e_1),\ldots,\phi(e_d))}{\omega(e_1,\ldots,e_d)} =
\frac{\cancel{c}\,\omega'(\phi(e_1),\ldots,\phi (e_d))}{\cancel{c}\,\omega'(e_1,\ldots,e_d)}
\ese

The independence from the choice of basis is more cumbersome to show, but it does hold, and thus $\det \phi$ is
well-defined. \v

It is very important to notice that $\phi$ needs to be an endomorphism because we need to apply $\omega$ to $\phi
(e_1),\ldots,\phi(e_d)$, and thus $\phi$ needs to output a vector. Which means that the determinant can only be
defined for endomorphisms. \v

Of course, under the identification of $\phi$ as a matrix, this definition coincides with the usual definition of
determinant, and all your favourite results about determinants can be derived from it. However, once we switch to
``matrix representation'' as we said one is not able to distinguish between an endomorphism $\phi \in T^1_1 V$ and
the so called ``bilinear form'' $g\in T^0_2 V$, hence, one might think that they can calculate the determinant of the
second guy. Let's see why such a determinant is not well-defined. \v

In your linear algebra course, you may have shown the determinant is basis-independent as follows: if $A$ denotes the
change of basis matrix, then:
\bse
\det(A^{-1}\phi A)=\det(A^{-1})\det(\phi)\det(A)=\det(A^{-1}A)\det(\phi) = \det(\phi)
\ese

since scalars commute, and $\det(A^{-1}A)=\det(I)=1$. \v

Recall that the transformation rule for a bilinear form $g$ under a change of basis is $g \to A^T gA$. The determinant
of $g$ then transforms as:
\bse
\det(A^TgA)=\det(A^T)\det(g)\det(A) = (\det A)^2\det(g)
\ese

i.e.\ it is not invariant under a change of basis. It is not a well-defined object, and thus we should not use it. \v

We will later meet quantities $X$ that transform as:
\bse
X \to \frac{1}{(\det A)^2} \, X
\ese

under a change of basis, and hence, they are also not well-defined. However, we obviously have:
\bse
\det(g) X \to \frac{\cancel{(\det A)^2}}{\cancel{(\det A)^2}} \, \det(g)X = \det(g)X
\ese

so that the product $\det(g)X$ is a well-defined object. It seems that two wrongs make a right! \v

In order to make this mathematically precise, we will have to introduce \emph{principal fibre bundles}. Using them,
we will be able to give a bundle definition of tensor and of \emph{tensor densities} which are quantities that
transform with powers of $\det A$ under a change of basis. We will see all of that in later chapters.

\section{Rings}

\bd [Ring]
A \textbf{ring} is a triple $(R,+,\cdot)$, where $R$ is a set and $+,\cdot\cl R\times R\to R$ are maps satisfying the
following axioms:
\bit
\item $(R,+)$ is an abelian group:
\ben
\item[i)] Closure: $\forall \, a,b \in R : a + b \in R$.
\item[ii)] Associativity: $\forall \, a,b,c \in R : (a+b)+c=a+(b+c)$.
\item[iii)] Neutral Element: $\exists \, 0 \in R : \forall \, a \in R : a+0=0+a=a$.
\item[iv)] Inverse Element: $\forall \, a \in R : \exists \, {-a} \in R : a+(-a)=(-a)+a=0$.
\item[v)] Commutativity: $\forall \, a,b \in R : a+b=b+a$.
\een

\item The operation $\cdot$ is closed and associative in $R^* \coloneqq R\sm\{0\}$:
\ben
\item[vi)] Closure: $\forall \, a,b \in R^* : a \cdot b \in R^*$.
\item[vii)] Associativity: $\forall \, a,b,c \in R^* : (a\cdot b)\cdot c=a\cdot (b\cdot c)$.
\een

\item The maps $+$ and $\cdot$ satisfy the distributive properties:
\ben
\item[viii)] $\forall \, a,b,c \in R : (a+ b)\cdot c=a\cdot c + b\cdot c$.
\item[ix)] $\forall \, a,b,c \in R : a \cdot (b+c)=a\cdot b + a\cdot c$.
\een
\eit

Note that since $\cdot$ is not required to be commutative, axioms viii and ix are both necessary. In the case of
fields where $\cdot$ was commutative, ix followed from viii and commutativity of $\cdot$.
\ed

\bd [Commutative / Unital / Division Rings]

A ring $(R,+,\cdot)$ is said to be:
\bit \item \textbf{Commutative} if $\ \forall \, a,b\in R : a\cdot b = b \cdot a$.
\item \textbf{Unital} if $\ \exists\, 1\in R : \forall \, a\in R : 1\cdot a = a \cdot 1 = a$.
\item A \textbf{division} (or \textbf{skew}) \emph{ring} if it is unital and:
\bse
\forall\, a \in R\sm\{0\} : \exists \, a^{-1}\in R\sm\{0\}: \ a\cdot a^{-1}=a^{-1}\cdot a = 1
\ese
\eit
\ed

In a unital ring, an element for which there exists a multiplicative inverse is said to be a \emph{unit}. The set of
units of a ring $R$ is denoted by $R^*$ (not to be confused with the vector space dual) and forms a group under
multiplication. Then, $R$ is a division ring iff $R^*=R\sm\{0\}$.

\be
The sets $\Z$, $\Q$, $\R$, and $\C$ are all rings under the usual operations. They are also all fields, except $\Z$.
\ee

In general, if $(A,+,\cdot,\bullet)$ is an algebra, then $(A,+,\bullet)$ is a ring.

\section{Modules}

\bd [$R$-Module]
Let $(R,+,\cdot)$ be an unital ring. An \textbf{$R$-module} is a triple $(M,\oplus,\odot)$ where $M$ is a set and:
\bi{rrCl}
\oplus \cl & M \times M & \to & M \\ \odot \cl & R \times M & \to & M
\ei

are maps satisfying the following axioms:
\bit
\item $(M,\oplus)$ is an abelian group i.e.\ :
\ben
\item[i)] Closure: $\forall \, m,n \in M : m \oplus n \in M$.
\item[ii)] Associativity: $\forall \, m,n,s \in M : (m \oplus n) \oplus s = m \oplus (n \oplus s)$.
\item[iii)] Neutral Element: $\exists \, 0 \in M : \forall \, m \in M : m \oplus 0 = 0 \oplus m = m$.
\item[iv)] Inverse Element: $\forall \, m \in M : \exists \, -m \in M : m \oplus (-m) = (-m) \oplus m = 0$.
\item[v)] Commutativity: $\forall \, m,n \in M : m \oplus n = n \oplus m$.
\een

\item The map $\odot$ is an \emph{action} of $R$ on $(M,\oplus)$:
\ben
\item[vi)] Distributivity Of Scalar Multiplication - Vector Addition: $\forall \, r \in R : \forall \, m,n \in M : r
\odot (m\oplus n) = (r \odot m) \oplus (r \odot n)$.
\item[vii)] Distributivity Of Scalar Multiplication - Field Addition: $\forall \, r,s \in K : \forall \, m \in V :
(r+s)\odot m = (r\odot m)\oplus (s\odot m)$.
\item[viii)] Compatibility Of Scalar Multiplication - Field Multiplication: $\forall \, r,s \in R : \forall \, m \in
M : (r\cdot s)\odot m = r \odot (s\odot m)$.
\item[ix)] Neutral Element Of Scalar Multiplication: $\forall \, m \in M : 1\odot m = m$.
\een
\eit
\ed

Modules are vector spaces over rings instead of fields. For this reason, definitions carry over unaltered to modules.

\be
Any ring $R$ is a module over itself, in the sense that every field $K$ is a vector space over itself.
\ee

In the following, we will usually denote $\oplus$ by $+$ and suppress the $\odot$ as we did with vector spaces.

\bd [Direct Sum Of Modules]
The \textbf{direct sum of two $R$-modules} $M$ and $N$ is the $R$-module $M\oplus N$, which has $M\times N$ as its
underlying set and operations (inherited from $M$ and $N$) defined component-wise.
\ed

Note that while we have been using $\oplus$ to temporarily distinguish two ``plus-like'' operations in different
spaces, the symbol $\oplus$ is the standard notation for the direct sum.

\bd [Finitely Generated / Free / Projective Modules]
An $R$-module $M$ is said to be:
\bit
\item \textbf{Finitely generated} if it has a finite generating set.
\item \textbf{Free} is it has a basis.
\item \textbf{Projective} if it is a direct summand of a free $R$-module $F$, i.e.\ :
\bse
M \oplus Q = F
\ese

for some $R$-module $Q$.
\eit
\ed

\be
Clearly, every free module is also projective.
\ee

\bd [R-Linear Maps]
Let $M$ and $N$ be two $R$-modules. A map $f\cl M \to N$ is said to be an \textbf{$R$-linear map} if:
\bse
\forall \, r\in R : \forall \, m_1,m_2\in M : \ f(rm_1 + m_2)=rf(m_1)+f(m_2)
\ese

where it should be clear which operations are in $M$ and which in $N$.
\ed

\bd [Module Isomorphisms]
A bijective $R$-linear map is said to be a \textbf{module isomorphism}.
\ed

\bd [Isomorphic Modules]
Two modules are said to be \textbf{isomorphic} if there exists a module isomorphism between them. We write
$M\cong_{\mathrm{mod}}N$.
\ed

\bt[]
If a finitely generated module $R$-module $F$ is free, and $d\in \N$ is the cardinality of a finite basis, then:
\bse
F\cong_\mathrm{mod} = \underbrace{R\oplus\cdots\oplus R}_{d \text{ copies}} \eqqcolon R^d
\ese
\et

One can show that if $R^d\cong_\mathrm{mod} R^{d'}$, then $d=d'$ and hence, the concept of dimension is well-defined
for finitely generated, free modules.

\bt[]
Let $P,Q$ be finitely generated (projective) modules over a commutative ring $R$. Then:
\bse
\Hom_R(P,Q) \coloneqq \{\phi\cl P \xrightarrow{\sim} Q \mid \phi \text{\normalfont is $R$-linear}\}
\ese

is again a finitely generated (projective) $R$-module, with operations defined pointwise.
\et

The proof is exactly the same as with vector spaces. As an example, we can use this to define the dual of a module.

\subsection{Basis Of Modules}

The key fact that sets modules apart from vector spaces is that, unlike a vector space, an $R$-module need not have a
basis, unless $R$ is a division ring. This is actually a well-known theorem that we will state but not prove.

\bt[]
If $D$ is a division ring, then any $D$-module $V$ admits a basis.
\et

\bt[]
Every vector space has a basis, since any field is also a division ring.
\et

\section{Algebras}

\bd[Algebra]
Let $K$ be a field, and let $A$ be a vector space over $K$ equipped with an additional bilinear map (called binary
operation or product) $\bullet \cl A\times A \to A$. The quadruple $(A,+, \cdot,\bullet)$ is called an
\textbf{algebra} over a field $K$.
\ed

\bd [Associative / Unital / Commutative Algebra]
An algebra $(A,+,\cdot,\bullet)$ is said to be:
\ben[label=\roman*)]
\item \textbf{Associative} if $\ \forall \, v,w,z\in A : v\bullet (w\bullet z) = (v\bullet w)\bullet z$.
\item \textbf{Unital} if $\ \exists \, \mathbf{1} \in A : \forall \, v \in V : \mathbf{1}\bullet v = v \bullet \mathbf{1} = v$.
\item \textbf{Commutative} or \textbf{abelian} if $\ \forall \, v,w\in A : v\bullet w = w\bullet v$.
\een
\ed

\bd [Derivation]
Let $A$ and $B$ be algebras. A \textbf{derivation} on $A$ is a linear map $D\cl A \xrightarrow{\sim}B$ satisfying the
Leibniz rule:
\bse
D(v\bullet_Aw)= D(v)\bullet_Bw +_B v \bullet_B D(w)
\ese

for all $v,w \in A$.
\ed