%! suppress = EscapeUnderscore
%! suppress = Ellipsis
%! suppress = Quote
%! suppress = EscapeHashOutsideCommand
\section{Cloud Computing}

There are two main types of computing paradigms: ``on-premise'' and ``cloud'' computing.

\bd[On-Premise Computing]
\textbf{On-premise computing}, or \textbf{on-prem}, is the traditional method of running applications by hosting and
maintaning all computing resources on the premises of the organization that is using the applications.
\ed

\bd[Cloud Computing]
\textbf{Cloud computing} is a model for enabling the delivery of a shared pool of on-demand computing services over
the public internet, that can be rapidly provisioned and released with minimal management effort or service provider
interaction.
\ed

In what follows we will be focusing exclusively on cloud computing, since it is the most modern and widely used
computing paradigm. \v

Cloud computing is composed of five essential characteristics, as defined by the National Institute of Standards and
Technology (NIST):
\bit
\item \textbf{On-Demand Self-Service}: A consumer can unilaterally provision computing capabilities as needed,
automatically without requiring human interaction with each service provider.
\item \textbf{Broad Network Access}: Capabilities are available over the network and accessed through standard
mechanisms that promote use by heterogeneous clients.
\item \textbf{Resource Pooling}: The provider's computing resources are pooled to serve multiple consumers using a
multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to
consumer demand. There is a sense of location independence in that the customer generally has no control or knowledge
over the exact location of the provided resources but may be able to specify location at a higher level of abstraction.
\item \textbf{Rapid Elasticity}: Capabilities can be elastically provisioned and released, in some cases automatically,
to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for
provisioning often appear to be unlimited and can be appropriated in any quantity at any time.
\item \textbf{Measured Service}: Cloud systems automatically control and optimize resource usage by leveraging a
metering capability at some level of abstraction appropriate to the type of service. Resource usage can be monitored,
controlled,and reported, providing transparency for both the provider and consumer of the utilized service.
\eit

The three most popular cloud computing providers are Amazon Web Services (AWS) by Amazon, Google Cloud Platform (GCP)
by Google, and Microsoft Azure (Azure) by Microsoft. \v

\fig{cloud}{0.22}

\subsection{Deployment Models}

\bd[Deployment Model]
A \textbf{deployment model} is a type of cloud environment based on ownership, scale, and access, as well as the cloud's
nature and purpose.
\ed

Cloud computing comes in four different deployment models: ``public cloud'', ``multi-cloud'', ``private cloud'', and
``hybrid cloud'', each of them with its own unique characteristics and use cases.

\bd[Public Cloud]
\textbf{Public cloud} is the deployment model of cloud computing services which is offered by cloud computing provider
over the public internet, provisioned for open use by the general public. It exists on the premises of the cloud
provider.
\ed

\fig{gcp1}{0.21}

Public cloud is the most common deployment model, and it is used by many organizations. It is the most cost-effective
model as it is based on a ``pay-as-you-go'' model, where you only pay for the resources you use. It is also the most
scalable model as it allows you to scale up or down based on your needs.

\bd[Multi-Cloud]
\textbf{Multi-cloud} is the deployment model of cloud computing services which is a composition of two or more cloud
computing providers, giving organizations more flexibility to optimize performance, control costs, and leverage the
best cloud technologies available.
\ed

\fig{gcp2}{0.42}

What drives many cases of a multi cloud deployment is to prevent the so called ``vendor lock'', where you are locked
into a particular cloud provider's infrastructure and unable to move due to the vendor's specific feature set. The
main downfall to this type of architecture is that the infrastructure of the public cloud that you're using cannot
be fully utilized as each cloud vendor has their own proprietary resources that will only work in their specific
infrastructure. In other words, in order to replicate the environment it needs to be the same within each cloud,
which removes each cloud's unique features which is what makes them so special and the resources so compelling.

\bd[Private Cloud]
\textbf{Private cloud} is the deployment model of cloud computing services which is provisioned for exclusive use by a
single organization. It is owned, managed, and operated by the organization, and it exists on or off premises.
\ed

\fig{gcp3}{0.34}

Although private cloud exists on premise and is restricted to the business itself with no public access, it still
carries the same five essential characteristics. Each of the major cloud providers have their own flavor of private
cloud that can be implemented on site. GCP has ``Anthos'', AWS has ``AWS Outposts'' and Azure has ``Azure Stack''. All
of them show the same characteristic and leverage similar technologies that can be found in the provider's public cloud,
yet can be installed on one's own on-premise infrastructure.

\bd[Hybrid Cloud]
\textbf{Hybrid cloud} is the deployment model of cloud computing services which is a composition of two or more
deployment models that remain unique entities, but are bound together enabling them to share data and application
portability.
\ed

\fig{gcp4}{0.17}

In simple words, hybrid cloud is the situation where one combines the use of private cloud with public or multi-cloud
as a single system. This model is used when an organization wants to keep some data on-premise and some data in the
cloud, or to use a public cloud for some services and a private cloud for others. \v

Given the complexity of the 4 models, sometimes finding the right strategy can be tricky depending on the scenario.

\subsection{Service Models}

\bd[Service Model]
A \textbf{service model} is a category of cloud computing services based on the level of abstraction of the computing
infrastructure that is provided.
\ed

Application are deployed in an infrastructure stack which is a collection of needed infrastructure that the application
needs to run on. It is layered and each layer builds on top of the one previous to it, with each layer depending on the
one below it. The infrastructure stack is composed of the following layers: ``data center'', ``network \& storage'',
``physical servers'', ``virtualization'', ``operating system'', ``container'', ``runtime'',``data'', and ``application''.
\v

There are a few different service models that are used to describe the level of abstraction of the computing
infrastructure that is provided, and we will see them now.

\bd[On-Premises]
\textbf{On-premises} is a cloud computing service model where all the components are managed by the customer on
premises.
\ed

In the ``pre-cloud'' era all the infrastructure stack components were purchased and managed by the customer. While
the advantages to this is that it allowed for major flexibility, in order for the organization to put this together
by themselves they were looking at huge costs.

\bd[Data Center Hosted]
\textbf{Data center hosted} is a cloud computing service model where the data center is hosted for the customer by a
vendor, and the customer is responsible for managing the rest of the infrastructure stack components.
\ed

Just before cloud became big, there was a model where the data center was hosted for you so a vendor would come along,
and they would take care of everything in regard to the data center. \v

In the cloud computing era providers offer their services according to individual customer's needs and requirements
of abstraction. The three most common service models are grouped under the umbrella of ``XaaS'', where ``X'' stands
for anything, and the ``aaS'' stands for ``as a service''. We will now see them one by one, starting from the lowest
level of abstraction to the highest.

\bd[Infrastructure As A Service (IaaS)]
\textbf{Infrastucture As A Service (IaaS)} is a cloud computing service model where all the layers from data center
up to virtualization are managed by the vendor, and the customer is responsible for managing the rest of the layers.
\ed

IaaS is the most basic service model which emulates the computer hardware by providing the storage, network, servers and
virtualization, freeing the user from maintaining an on-premise data center. In essense, IaaS is a virtual machine in
the cloud. One sets up, configures, and manages the instances of virtual machines that run on the provider's
infrastructure, through with high-level APIs. \v

\be
A great example of IaaS in GCP is Google Compute Engine (GCE), which is a service that lets you run virtual machines on
Google's infrastructure.
\ee

\bd[Container As A Service (CaaS)]
\textbf{Container As A Service (CaaS)} is a cloud computing service model where all the layers from data center up to
container are managed by the vendor, and the customer is responsible for managing the runtime, data, and application
layers.
\ed

CaaS is a service that allows you to run containerized applications on the cloud without having to manage the underlying
infrastructure. CaaS is designed to support the complete container lifecycle: building, testing, deploying, managing,
and updating.

\be
A great example of CaaS in GCP is Google Kubernetes Engine (GKE), which is a service that lets you run containerized
applications on Google's infrastructure.
\ee

\bd[Platform As A Service (PaaS)]
\textbf{Platform As A Service (PaaS)} is a cloud computing service model where all the layers from data center up to
runtime are managed by the vendor, and the customer is responsible for managing the data and application layers.
\ed

PaaS allows customers to provision, instantiate, run, and manage a computing platform and one or more applications,
without the complexity of building and maintaining the infrastructure typically associated with developing and launching
the application. PaaS is designed to support the complete web application lifecycle: building, testing, deploying,
managing, and updating.

\be
A great example of PaaS in GCP is Google App Engine (GAE), which is a service that lets you build and run applications
on Google's infrastructure.
\ee

\bd[Software As A Service (SaaS)]
\textbf{Software As A Service (SaaS)} is a cloud computing service model where all the layers from data center up to
application are managed by the vendor, and the customer is free to use the application.
\ed

SaaS, also known as ``on-demand software'', or ``web-hosted software'', is usually licensed on a subscription basis
and is centrally hosted. SaaS apps are typically accessed by users of a web browser and became common delivery models
for many business applications. SaaS has been incorporated into the strategy of nearly all enterprise software companies.

\be
A great example of SaaS in GCP is Google Workspace, which is a suite of cloud computing, productivity and collaboration
tools, software and products developed by Google.
\ee

\bd[Function As A Service (FaaS)]
\textbf{Function As A Service (FaaS)} is a cloud computing service model where the customer is responsible only for
the application layer, and the vendor is responsible for the rest of the layers.
\ed

FaaS is a serverless computing service allowing developers to execute individual functions in response to specific
events, without the complexity of building and maintaining the needed infrastructure.

\be
A great examples of FaaS in GCP is Cloud Run which is a service that lets you run serverless containers on Google's
infrastructure.
\ee

Here follows a figure summarizing all service models.

\fig{service_models}{0.42}

\subsection{Geography}

\bd[Data Center]
A \textbf{data center} is a facility composed of networked computers and storage that cloud computing providers use to
provide their cloud computing services.
\ed

Data centers are spread across the globe and user requests are routed to the appropriate data center based on the
location of the user. This is done to ensure that the user gets the best possible performance and minimal latency. \v

Data centers are organized in zones, regions, and multi-regions, based on their geographical location.

\bd[Zone]
A \textbf{zone} is the smallest geographical entity in the global infrastructure, acting as a deployment area within a
region, considered as a single failure domain within a region. They are symbolized by a single small letter: \code{a},
\code{b}, etc.
\ed

To deploy fault-tolerant applications with high availability and help protect against unexpected failures, users
deploy their applications across multiple zones in a region. As a best practice resources should always be deployed
in zones that are closest to the users for optimal latency.

\bd[Region]
\textbf{Regions} are the second, largest geographical entities in the global infrastructure, representing independent
geographic areas that consist of zones. They are symbolized by a name that describes the general location of the region,
such as \code{central1} or \code{west2}.
\ed

A region can be considered as a collection of zones. The intercommunication between zones within a region is under five
milliseconds so rest assured that data is always traveling at optimal speeds.

\bd[Multi-Region]
\textbf{Multi-regions} are the largest geographical entities in the global infrastructure, representing geographical
areas that contain two or more regions. They are symbolized by a name that describes the general location of the
multi-region, such as \code{us} or \code{eu}.
\ed

Multi-regions allows services to maximize redundancy and distribution within and across regions. They are also used
for global load balancing and disaster recovery. \v

Zones, regions and multi-regions are logical abstractions of the underlying physical resources provided in one or more
physical data centers. By combining them in the following standardize format: \code{multi\_region-region-zone}, one can
specify the exact location of the underlying data center (e.g. \code{us-central1-a}, \code{europe-west2-b}, etc.).

\fig{gcp11}{0.45}

\section{Google Cloud Platform}

Up to this point we discussed cloud computing in general. From now on, and for the remaining of this chapter, we will
be focusing specifically on GCP\@.

\bd[Google Cloud Platform (GCP)]
\textbf{Google Cloud Platform} (\textbf{GCP}) offered by Google, is a suite of cloud computing services that runs on
the same infrastructure that Google uses internally for its end-user products.
\ed

\subsection{Console}

Before we dive into the details of GCP, we need to understand how to interact with it, so we can follow along with the
examples that we will see in the coming sections. The main interface for interacting with GCP is the GCP console.

\bd[GCP Console]
\textbf{GCP console} is a web-based user interface (UI) for managing GCP computing services.
\ed

GCP console allows to create, manage, and monitor computing services. The console is the easiest way to get started
with GCP, and it is recommended for new users. However, in these notes we will focus on the GCP SDK, which is more
powerful and flexible.

\subsection{SDK}

\bd[GCP SDK]
\textbf{GCP SDK} is a command-line interface (CLI) for managing GCP computing services.
\ed

GCP SDK is so powerful that it can be used to perform all (and even more) tasks that one can do with the GCP console.
It is recommended for advanced users who are comfortable working with CLIs. \v

GCP SDK can be installed via Homebrew:
\begin{bash}
# install GCP SDK via Homebrew
brew install google$-$cloud$-$sdk
\end{bash}

To make sure that everything is working properly, one can check the current version of GCP SDK by:
\begin{bash}
# see version of GCP SDK
gcloud version
\end{bash}

This command will show the version of the GCP SDK that is currently installed on the system, and it will also show a
list of ``components'' together with their installed status and their version.

\bd[Component]
A \textbf{component} is an installable part of the GCP SDK that provides a specific set of functionalities, acting as
mental boundaries among various GCP computing services.
\ed

Upon installation of GCP SDK, the following components are installed by default:
\bit
\item \textbf{gcloud}: Default GCP SDK tools for interacting with GCP resources.
\item \textbf{bq}: BigQuery CLI tool for working with data in BigQuery.
\item \textbf{gsutil}: Cloud Storage CLI tool for performing tasks related to Cloud Storage.
\item \textbf{core} GCP SDK core libraries used internally by the GCP SDK tools.
\eit

The full list of available components is long, and it keeps being updated so it is impossible to include an exhaustive
list here. To see the list of all components that are available and their installed status:
\begin{bash}
# see components that are available and their installed status
gcloud components list
\end{bash}

To install a component:
\begin{bash}
# install component
gcloud components install <component>
\end{bash}

To update a component:
\begin{bash}
# update component
gcloud components update <component>
\end{bash}

To remove a component:
\begin{bash}
# remove component
gcloud components remove <component>
\end{bash}

Due to the endless possibilities of what one can do in GCP, the list of commands of GCP SDK, including its components,
is enormous, and it's getting longer every day. Hence, writing down an exhaustive list of GCP SDK commands is impossible.
Throughout the notes we will see many examples of GCP SDK commands, so that the reader can get a good understanding of
how to use it. \v

All GCP SDK commands, however, obey the same generic form. This generic from follows a nested approach, matching GCP
console's nesting, going from ``big'' to ``small'':
\begin{bash}
# generic GCP SDK command format
gcloud | (release*) | (component) | (subcomponent*) | (operation) | (target*) | $--$(flags*)
\end{bash}

where:
\bit
\item \textbf{release}: (optional) Refers to the command's release status. The default value is \code{GA} standing for
``General Availability'' and it is omitted. The non-default values are \code{alpha} and \code{beta}.
\item \textbf{component}: (required) Refers to the main GCP service by mentioning the component of the operation (more
on this later).
\item \textbf{subcomponent}: (optional) Refers to the subcomponent of the operation (if any).
\item \textbf{operation}: (required) Refers to the operation of the component that will be performed on the target.
\item \textbf{target}: (optional) Refers to the target resource that the operation will be performed on.
\item \textbf{flags}: (optional) Refers to the flags that can be used to modify the operation.
\eit

\be
For example, the command \code{gcloud compute instances list --project=my-project} lists all the instances in the
project \code{my-project}, where \code{compute} is the component, \code{instances} is the subcomponent, \code{list}
is the operation, and \code{--project} is the flag.
\ee

Probably the most important command is the one for getting help. In order to do so, one can use the flag
\code{--help} at any \code{gcloud} level:
\begin{bash}
# get help
gcloud $--$help
gcloud | (component) $--$help
...
\end{bash}

\subsubsection{Configurations}

Although this section contains some concepts that we will introduce later, we will include it here since it is related
to the GCP SDK and its initial setup. Maybe it is worth revisiting this section after reading the rest of the chapter,
since some terms will make more sense then. \v

Let's begin by defining the conecept of ``properties''.

\bd[Properties]
\textbf{Properties} are settings that define the behavior of the GCP SDK\@.
\ed

\be
One can use properties to define a per-product or per-service setting such as the account used by the GCP SDK for
authorization, the default region to use when working with Compute Engine resources, or the option to turn off
automatic GCP SDK component update checks. (All of these concepts will be introduced in later sections). Properties
can also be used to define GCP SDK preferences like verbosity level and prompt configuration for GCP SDK commands.
\ee

The GCP SDK supports some flags that have the same effect as properties. However, while flags affect command behavior
on a per-invocation basis, properties allow to maintain the same settings across command executions. Note that flags
override properties when both are set.

\be
For example, the GCP SDK supports both the \code{--project} flag and \code{project} property. If one sets the
\code{project} property to \code{my-project} and run a command that uses the \code{--project} flag, the flag will
override the property.
\ee

Having defined properties, we can now define the concept of ``configuration''.

\bd[Configuration]
A \textbf{configuration} is a named set of GCP SDK properties.
\ed

Configurations allow to define a set of properties together as a named group.\footnote{To avoid confusion, within a
given configuration, properties are further subdivided into categories called ``sections''. Sections are used to
organize properties into logical groups.} They are stored by default in the user config directory in
\code{$~$/.config/gcloud}. \v

\bd[Default Configuration]
The \textbf{default configuration} is the initial active configuration.
\ed

GCP SDK uses a configuration named ``default'' as the initial active configuration. Note that there is nothing special
about the initial default configuration; it is created as a convenience. One can name this, and any additional
configurations, however they'd like. The default configuration, however, is suitable for most use cases. Usually, if
one is working with just one project, the default configuration should be sufficient. \v

The default configuration is created during the \code{gcloud auth login} command which authorizes access to GCP\@, and
we will see in the next section. However, \code{gcloud auth login} does not let you set other properties other than the
account. For this reason, there is another command \code{gcloud init} which authorizes access (exactly same behavior as
\code{gcloud auth login}) and on top of that it performs other common setup steps:
\begin{bash}
# authorize and configure the GCP SDK at the same time
gcloud init
\end{bash}

One can create additional configurations and switch between them as required. This is useful if one is working with
multiple accounts or projects, or if they want to perform generally independent tasks:
\begin{bash}
# create a configuration
gcloud config configurations create <configuration>
\end{bash}

Once a new configuration is created, it is added to the list of available configurations:
\begin{bash}
# display all available configurations
gcloud config configurations list
\end{bash}

As it makes sense, only one of your multiple configurations can be active at a given time. The active configuration
is the configuration whose properties will govern the current behavior of the GC SDK\@:
\begin{bash}
# activate a configuration
gcloud config configurations activate <configuration>
\end{bash}

Of course, one can set and get all properties for any given configuration:
\begin{bash}
# define a property for the current configuration
gcloud config set <property> <value>
\end{bash}

\begin{bash}
# display the value of a property for current configuration
gcloud config get <property>
\end{bash}

\begin{bash}
# display the value of all properties for current configuration
gcloud config list
\end{bash}

The list of all available properties one can set within a configuration is long, and it keeps being updated, so it is
impossible to include an exhaustive list here. However, here is a list of some of the most common properties that one
can set within a configuration:
\bit
\item \textbf{account}: The account to use for authorization.
\item \textbf{project}: The project to use for billing and resource management.
\item \textbf{region}: The default region to use for resource management.
\item \textbf{zone}: The default zone to use for resource management.
\item \textbf{verbosity}: The verbosity level of the GCP SDK output.
\item \textbf{console\_log\_format}: Controls the format used to display log messages to the console. Valid values are:
    \bit
    \item \textbf{standard}: (default) Simplified log messages are displayed on the console.
    \item \textbf{detailed}: More detailed messages are displayed on the console.
    \eit
\eit

Lastly, one can delete a configuration:
\begin{bash}
# delete a configuration
gcloud config configurations delete <configuration>
\end{bash}

Notice however, that one cannot delete an active configuration. They first need to switch to another configuration by
activating it with \code{gcloud config configurations activate} and then delete the configuration they want to delete.

\subsection{Resources}

\bd[Resource]
A \textbf{resource} is an abstract term representing all the individual components of GCP that are used to provide
cloud computing services.
\ed

Resources can either be ``service level'' or ``account level''.

\bd[Service Level Resource]
A \textbf{service level resource} is a resource that is used to process workloads, and is the fundamental component
that makes up all GCP computing services.
\ed

\be
Some examples of service level resources are: virtual machines, databases, and more.
\ee

\bd[Account Level Resource]
An \textbf{account level resource} is a resource that sits above the service level resources, and is used to manage
access control and permissions for groups of related resources.
\ed

\be
Some examples of account level resources are: organization, folder, project, and more.
\ee

Resources are organized hierarchically using a parent-child relationship. All resources except for the highest resource
in a hierarchy have exactly one parent. At the lowest level, service level resources are the fundamental components that
make up all GCP computing services. This hierarchy is designed to map an organization's operational structure and to
manage access control and permissions for groups of related resources, giving organizations better management of
permissions and access control. \v

Building the structure from the top down we start off with the ``organization''.

\bd[Organization]
The \textbf{organization} resource is a required, account level resourse that represents an organization, and is the
root node in the GCP resource hierarchy.
\ed

The organization resource is the hierarchical ancestor of folder and project resources, and it is a prerequisite to
use both of them.

\bd[Folder]
\textbf{Folder} resource is an additional, optional, account level resourse actings as grouping mechanism and an
isolation boundary between projects.
\ed

Folder resources can be seen as sub-organizations within the organization resource, and thus, they can be used to model
different legal entities, departments, or teams within a company. Each folder resource can include other sub-folders to
represent different sub-entities. While a folder can contain multiple subfolders, a folder can have exactly one parent.

\bd[Project]
A \textbf{project} resource is a required, account level resource that forms the basis for creating, enabling, and
using all GCP service level resources.
\ed

The project resource is the base-level organizing entity, and it is usually the first resource one creates upon starting
using GCP. All GCP resources must belong to a project, and each project is the parent of all service level resources.
Organization and folder resources may contain multiple projects. \v

All projects consist of the following:
\bit
\item \textbf{Project ID}: A unique identifier for the project resource.
\item \textbf{Project Number}: A unique, read-only number assigned to the project resource upon creation.
\item \textbf{Project Name}: A user-assigned, mutable name for the project resource.
\item \textbf{Project Lifecycle State}: The lifecycle state of the project resource.
\item \textbf{Project Labels}: A collection of labels that can be used for filtering projects.
\item \textbf{Project Creation Time}: The time when the project resource was created.
\eit

\fig{gcp12}{0.5}

One can use the GCP SDK to interact with projects. Here are some useful commands:
\begin{bash}
# list all available projects
gcloud projects list
\end{bash}

\begin{bash}
# create a new project
gcloud projects create <project-id>
\end{bash}

\begin{bash}
# delete a project
gcloud projects delete <project-id>
\end{bash}

\begin{bash}
# set a default GCP project to work on
gcloud config set project <project-id>
\end{bash}

\begin{bash}
# display metadata for a project
gcloud projects describe <project-id>
\end{bash}

\subsection{Billing}

\bd[Billing Account]
\textbf{Billing account} is an account level resource used to define who pays for a given set of GCP resources.
\ed

A billing account can be linked to one or more projects, and it also carries billing specific roles and permissions to
control accessing and modifying billing related functions that are established by identity and access management (IAM)
roles that we will see in a while. It is directly connected to a payment profile which includes a payment instrument
to which costs are charged.

\bd[Payment Profile]
\textbf{Payment profile} is an account level resource used to store information about the individual or organization
that is legally responsible for costs generated by all Google services (not just for GCP), and payment methods.
\ed

The payment profile contains contact information, payment methods and settings, and a unique numeric ID which
appears on account invoices and other documents. It connects to all of various Google services such as GCP, Google
Ads, and many more. It stores various payment methods like credit cards, debit cards, and bank accounts. Payments
profile are divided into two types:
\bit
\item \textbf{Individual Payment Profile}: Used for personal use.
\item \textbf{Business Payment Profile}: Used for business use.
\eit

\subsection{APIs}

\bd[GCP APIs]
\textbf{GCP APIs} are application programmatic interfaces to GCP services.
\ed

GCP APIs are a key part of GCP, allowing to interact with its services and resources. One could use the GCP console
API Library to browse available APIs and discover the ones that best meet their business needs. Once they find a
suitable API, then they need to enable it. Some APIs are enabled by default, but in order to use an API that is not
enabled by default, one must enable it for their project. To enable an API for a project using the GCP console, one
could simply go to the GCP console API Library and select the API thry want to enable and click ``ENABLE''. From the
same page one can also disable an API for a project if it no longer in use, to avoid misuse and accidental billing
charges, since some APIs charge for usage. \v

GCP APIs can be accessed directly from the GCP console, the GCP SDK, or from applications through client libraries.

\bd[Client Library]
A \textbf{client library} is a set of code specific to one programming language that simplifies the process of making
GCP API requests.
\ed

Client libraries make it easier to access GCP APIs from a supported language. While one can use GCP APIs directly by
making raw requests to the server, client libraries provide simplifications that significantly reduce the amount of
code one needs to write. \v

Client libraries are the recommended option for accessing the APIs programmatically, where available, since they:
\bit
\item Provide idiomatic code in each language to make Cloud APIs simple and intuitive to use.
\item Provide a consistent style across client libraries to simplify working with GCP APIs.
\item Handle all the low-level details of communication with the server, including authentication.
\item Can be installed using familiar package management tools such as \code{npm} and \code{pip}.
\eit

\section{Identity \& Access Management (IAM)}

\bd[Identity \& Access Management (IAM)]
\textbf{Identity \& Access Management} (\textbf{IAM}) is a framework providing a set of methods to manage policies,
that access control by defining who (identity) has what access (role) for which resource.
\ed

IAM is a very important concept in GCP and one needs to understand it well. It contains a lot of layers and definitions,
so we will expain everything step by step.

\subsection{Principal}

\bd[Principal]
A \textbf{principal}, (historically known as a \textbf{member}), represents an identity that can access a resource.
\ed

In IAM, one grant access to principals, in order to access a resource. Each principal has its own identifier, which is
typically an email address. Principals can be of many different types. In what follows we will take a close look to
each one of them, since they are an integral part of IAM and GCP\@.

\subsubsection{Google Account}

\bd[Google Account]
A \textbf{Google account} represents the identity of any physical person who interacts with GCP by using an account
they created with Google.
\ed

In simple words, a Google account is the usual account in Google that common users create. In the context of GCP it can
be a developer, an administrator, or any other person who interacts with GCP\@. A Google account is identified by its
email address (e.g.\ @gmail.com), which is unique to the account. \v

Once a Google account is created, it can be used to access all GCP services. In order to do so, the user that is being
represented by the Google account must authenticate and authorize the GCP SDK. GCP use OAuth2\footnote{OAuth2 (short
for open authorization) is an open standard authorization framework, commonly used as a way for internet users to grant
applications access to their information on other websites without providing any passwords. Designed specifically to
work with HTTP, OAuth2 allows credentials to be issued to third-party clients by an authorization server, with the
approval of the resource owner. The authorization server responds with an access token, which the application can use
to authenticate with the service and access resources. The application then presents the token to the resource server
to gain access to the resources.} for both authentication and authorization. To authorize with a Google account one can
use the following GCP SDK command:
\begin{bash}
# authorize access to a Google account
gcloud auth login
\end{bash}

During authorization, a set of credentials are obtained from GCP and stored locally, in the directory:
\code{$~$/.config/gcloud}. One can also revoke the account credentials at any time:
\begin{bash}
# revoke access to a Google account
gcloud auth revoke <account>
\end{bash}

Upon authorization the account becomes active and GCP SDK uses the credentials to access GCP\@.
\begin{bash}
# get the active account
gcloud config get account
\end{bash}

One can have any number of accounts for a single GCP SDK installation, but only one active at a time.
\begin{bash}
# list all accounts with stored credentials
gcloud auth list
\end{bash}

\begin{bash}
# set an account active
gcloud config set account <account>
\end{bash}

An important thing to underastand is that everything we descirbed so far, and all the commands we introduced, are used
to authenticate and authorize the GCP SDK to access GCP resources in the CLI. \v

However, when one is working outside the CLI with third-party clients and libraries, those clients and libraries won't
be able to be authenticated and authorized automatically with the GCP SDK credentials created during
\code{gcloud auth login} and stored in \code{$~$/.config/gcloud}. Instead, they need to have their own credentials,
that are obtained through a different strategy called ``application default credentials''.

\bd[Application Default Credentials (ADC)]
\textbf{Application default credentials} (\textbf{ADC}) is a strategy used by the authentication libraries to
automatically find credentials based on the application environment.
\ed

The authentication libraries make ADC available to thrid-party clients that need them in order to authenticate with
GCP\@. When one uses ADC, their code can run in either a development or production environment without changing how
the application authenticates to GCP. \v

In simple words, by making use of ADC one can solve the problem of wanting to run an application locally while the
application needs access to GCP, and it cannot access their credentials directly. ADC emulates the way production
environment accessing GCP resources, hence, there is no need to change anything in production. \v

The reccomended way to set up ADC is by providing the Google account credentials created during \code{gcloud auth login}
by running:
\begin{bash}
# create application default credentials
gcloud auth application$-$default login
\end{bash}

This command places a JSON file called ``application\_default\_credentials.json'' containing the provided Google account
credentials in \code{$~$/.config/gcloud/application\_default\_credentials.json}. Important to notice that the
credentials one provides to ADC by using the GCP SDK are distinct from the actual credentials the GCP SDK uses to
authenticate to GCP\@. \v

An important thing: this local ADC ``application\_default\_credentials.json'' file is associated with the Google
account, and not the GCP SDK configuration. Changing to a different GCP SDK configuration might change the identity
used by the GCP SDK, but it does not affect the local ADC file. \v

If one no longer needs the ADC credentials file, they can revoke them by using the following command which deletes the
``application\_default\_credentials.json'':
\begin{bash}
# revoke application default credentials
gcloud auth application$-$default revoke
\end{bash}

To summarize: when one signs in to the GCP SDK, they use the \code{gcloud auth login} command to authenticate a
principal to the GCP SDK. The GCP SDK uses that principal for authentication and authorization to manage GCP
resources within the CLI. This is the GCP SDK authentication configuration. When one uses the GCP SDK to configure
ADC, they use the \code{gcloud auth application-default login} command. This command uses the principal they provide
to configure ADC for local environment. This is the ADC configuration. The GCP SDK authentication configuration is
distinct from the ADC configuration. They can use the same principal or different principals. The GCP SDK does not
use ADC to access GCP resources.

\fig{gcp5}{0.52}

\subsubsection{Service Account}

\bd[Service Account]
A \textbf{service account} represents the identity of an application or compute workload which interacts with GCP
instead of an individual end user.
\ed

In simple words, a service account is used as the identity of an application rather than a person, intended to represent
a non-human user that needs to authenticate and be authorized to use GCP resources, without an actual human user being
involved. Service accounts do not have passwords, and cannot log in via browsers. \v

Applications use service accounts to make authorized API calls by authenticating as the service account itself. When
one runs an application that's on GCP, the code runs as the account they specify by using the service account to
authenticate between the application and GCP services so that the users aren't directly involved. When an application
authenticates as a service account, it has access to all resources that the service account has permission to access.
As with a Google account, similarly a service account is identified by its email address, which is unique to the account
and its form depends on the type of service account, as we will see later. \v

One can create as many service accounts as needed to represent the different logical components of an application. All
service accounts are managed by IAM, and each service account is located in a project. Once a service account is
created, it cannot be moved to a different project. \v

There are 3 different types of service accounts: ``user-managed service accounts'', ``default service accounts'', and
``Google-managed service accounts''.

\bd[User-Managed Service Accounts]
\textbf{User-managed service accounts} are service accounts created and managed by users.
\ed

User-managed service accounts are the most common type of service accounts. \v

When one creates a user-managed service account in a project, they choose a name for the service account. This name
appears in the email address that identifies the service account, which is of the format:
\code{<service-account-name>@<project-id>.iam.gserviceaccount.com}

\bd[Default Service Accounts]
\textbf{Default service accounts} are user-managed service accounts created automatically by Google upon enabling
certain GCP services, and managed by the user.
\ed

If an application runs in a GCP environment that has a default service account, the application can either use this
default service account, or alternatively, one can create their own user-managed service account and use it to
authenticate. It is always recommended to use user-managed service accounts instead of default service accounts. \v

When default service accounts are created, they appear with a predefined email address which format depends on the
type of service account: \code{<project-id>@appspot.gserviceaccount.com} for App Engine, and
\code{<project-number>-compute@developer.gserviceaccount.com} for Compute Engine.

\bd[Google-Managed Service Accounts]
\textbf{Google-managed service accounts} are service accounts created and managed by Google, allowing services to
access resources on user's behalf.
\ed

Google-managed service accounts are exclusively created and managed by Google. They are not located in any project,
they are not listed anywhere, and a user cannot access them. \v

Since, as we already mentioned, it is always recommended to use user-managed instead of default or Google-managed
service accounts, from now on we will focus on user-managed service accounts. For saving space, we will refer to them
as service accounts. \v

Through the GCP SDK, one can interact with service accounts. Here are some useful commands:
\begin{bash}
# create a service account
gcloud iam service$-$accounts create <service$-$account> $-$$-$display$-$name=<display$-$name> $-$$-$description=<description>
\end{bash}

\begin{bash}
# update a service account
gcloud iam service$-$accounts update <service$-$account> $-$$-$display$-$name=<display$-$name> $-$$-$description=<description>
\end{bash}

\begin{bash}
# disable a service account
gcloud iam service$-$accounts disable <service$-$account>
\end{bash}

\begin{bash}
# enable a service account
gcloud iam service$-$accounts enable <service$-$account>
\end{bash}

\begin{bash}
# delete a service account
gcloud iam service$-$accounts delete <service$-$account>
\end{bash}

\begin{bash}
# get metadata for a service account
gcloud iam service$-$accounts describe <service$-$account>
\end{bash}

\begin{bash}
# list all a service accounts
gcloud iam service$-$accounts list
\end{bash}

Once a service account is created, it is then ready for use. Remember that service accounts are created in order for
applications to make authorized API calls by authenticating as the service account themselves. Hence, the most common
way to let an application authenticate as a service account is to attach a service account to the service level resource
running the application, upon creation. More on how exactly this is done will be discussed in the next section, where
we will go through the various GCP service level resources.

\be
For example, one can attach a service account to a Compute Engine instance so that applications running on that instance
can authenticate as the service account.
\ee

Now, let's move on to the very important concept of impersonating a service account.

\bd[Impersonation]
\textbf{Impersonation} is the act of allowing a user to authenticate AS a service account, and access resources that the
service account has permission to access.
\ed

The way to impersonate a service account, and thus authenticate as one, is by creating a ``user-managed key pair'' for
a service account.

\bd[User-Managed Key Pair]
A \textbf{user-managed key pair} is a pair of public and private RSA keys that can be used to authenticate as a service
account.
\ed

Google stores only the public portion of a user-managed key pair. The private key, along with some more information,
is stored in a JSON file called ``service account key'' that is downloaded to the user's local machine.

\bd[Service Account Key]
A \textbf{service account key} is a JSON file that contains the following information that can be used to authenticate
as a service account:
\bit
\item \textbf{Type}: The type of the key, which is always \code{service\_account}.
\item \textbf{Project ID}: The project ID that the service account belongs to.
\item \textbf{Private Key ID}: The private key ID\@.
\item \textbf{Private Key}: The private key itself.
\item \textbf{Client Email}: The email address of the service account.
\item \textbf{Client ID}: The client ID of the service account.
\item \textbf{Auth URI}: The authentication URI\@.
\item \textbf{Token URI}: The token URI\@.
\item \textbf{Auth Provider X509 Cert URL}: The authentication provider X509 certificate URL\@.
\item \textbf{Client X509 Cert URL}: The client X509 certificate URL\@.
\eit
\ed

One can create service account keys and store them to a specified output file through GCP SDK:
\begin{bash}
# create a service account key for a specific service account and store it to an output file
gcloud iam service$-$accounts keys create <output$-$file> $-$$-$iam$-$account=<service$-$account>
\end{bash}

To see all the service account keys that are created for a service account, including their ids, creation time, and
expiration time, one can use the following command:
\begin{bash}
# list all service account keys for a specific service account
gcloud iam service$-$accounts keys list $-$$-$iam$-$account=<service$-$account>
\end{bash}

To delete a service account key, one can use the following command:
\begin{bash}
# delete a service account key
gcloud iam service$-$accounts keys delete <key$-$id> $-$$-$iam$-$account=<service$-$account>
\end{bash}

Once a service account key is created, it can be used to authenticate as the service account by using the command:
\begin{bash}
# authorize access with a service account key
gcloud auth activate$-$service$-$account <service$-$account> $-$$-$key$-$file=<key$-$file>
\end{bash}

One can verify that the service account key is active by using the \code{gcloud auth list} command that we saw earlier,
and lists all accounts with stored credentials. \v

In general, service account keys are a security risk if they aren't managed correctly. Users are responsible for the
security of the service account keys and for other management operations, such as key rotations. Users should also
follow the best practices for managing service account keys. \v

Following best practices, one is advised to not use service account keys at all, if possible. Instead, the reccomended
workflow is to first create a service account and attach it to the service level resource that will run the application
in production, and then use ADC to authenticate the application locally in the development environment. If for some
reason one cannot create ADC through their Google account, they can create them through the service account by using:
\begin{bash}
# create application default credentials through a service account
gcloud auth application$-$default login $-$$-$impersonate$-$service$-$account <service$-$account>
\end{bash}

If neither of these options are available, then one can use service account keys as a last resort.

\subsubsection{Google Group}

\bd[Google Group]
A \textbf{Google group} is a named collection of Google accounts and service accounts.
\ed

Google groups are a convenient way to apply access controls to a collection of users. One can grant and change
access controls for a whole group at once instead of granting or changing access controls one at a time for
individual users or service accounts. One can also easily add principals to and remove principals from a Google
group instead of updating an allow policy to add or remove users. Google Groups don't have login credentials, and
cannot be used to establish an identity to access a resource. Every Google group has a unique email address that's
associated with the group.

\subsubsection{Google Workspace Account}

\bd[Google Workspace]
A \textbf{Google workspace account} represents a virtual group of all Google accounts that it contains.
\ed

Google workspace accounts are associated with an organization's internet domain name (such as ``example.com''). When
a Google account for a new user is created, (such as ``username@example.com''), that Google account is added to the
virtual group for the Google Workspace account. Like Google Groups, Google Workspace accounts cannot be used to
establish identity, but they enable convenient permission management.

\subsubsection{Cloud Identity Domain}

Although outside the scope of these notes, it is worth mentioning that there is another type of principal that is
used in GCP, called ``Cloud Identity Domain''. For sake of completeness, we will define it here.

\bd[Cloud Identity Domain]
A \textbf{cloud identity domain} helps customers manage their identities centrally, along with providing secure
authentication and authorization to applications and devices.
\ed

A Cloud Identity domain is like a Google Workspace account, because it represents a virtual group of all Google
Accounts in an organization. However, Cloud Identity domain users don't have access to Google Workspace applications
and features.

\subsubsection{allAuthenticatedUsers}

\bd[allAuthenticatedUsers]
The value \textbf{allAuthenticatedUsers} is a special identifier that represents all service accounts and all users on
the internet who have authenticated with a Google account.
\ed

This identifier includes accounts that aren't connected to a Google Workspace account, such as personal Gmail accounts.
Users who aren't authenticated, such as anonymous visitors, aren't included.

\subsubsection{allUsers}

\bd[allUsers]
The value \textbf{allUsers} is a special identifier that represents anyone who is on the internet, including
authenticated and unauthenticated users.
\ed

Some GCP service levere resources require authentication before a user can access the resource. For these resources,
\code{allUsers} includes only authenticated users. In many cases, granting access to all users is no more of a security
risk than granting access only to authenticated users, so it is recommended to use \code{allUsers} rather than
\code{allAuthenticatedUsers}.

\subsection{Permissions, Roles \& Policies}

\bd[Permissions]
\textbf{Permissions} determine what operations are allowed on a resource.
\ed

Permissions are represented in the form of \code{service.resource.verb}, and they often correspond 1-to-1 with REST API
methods. That is, each GCP service has an associated set of permissions for each REST API method that it exposes. The
caller of that method needs those permissions to call that method.

\be
The permission \code{pubsub.topics.publish} allows a user to publish (verb) from a topic (resource) in pubsub
(service). For a user to call the \code{topics.publish()} method, they must have this permission.
\ee

Although someone would expect that permissions are granted to principals, the truth is that one doesn't grant
permissions to principals directly. Instead, one identifies ``roles'' that contain the appropriate permissions, and
then grant those roles to the principal.

\bd[Role]
A \textbf{role} is a collection of permissions.
\ed

When one grants a role to a principal, they grant them all the permissions that the role contains.

Each role has (among others) the following components:
\bit
\item \textbf{Title}: A human-readable title for the role used to identify the role in GCP\@.
\item \textbf{Name}: A unique name for the role used to identify the role in GCP\@.
\item \textbf{ID}: A unique identifier for the role used to identify the role in GCP\@.
\item \textbf{Description}: A human-readable description of the role.
\item \textbf{Permissions}: The permissions included in the role.
\eit

There are several kinds of roles that we will see now.

\bd[Basic Role]
A \textbf{basic role} is a role historically available in the GCP, with the name \code{roles/<role-name>}.
\ed

There are 3 basic roles: ``viewer'', ``editor'', and ``owner''.

\bd[Viewer]
\textbf{Viewer} role (\code{roles/viewer}), has permissions for read-only actions that do not affect state.
\ed

\bd[Editor]
\textbf{Editor} role (\code{roles/editor}), has all viewer permissions, plus permissions for actions that modify state.
\ed

\bd[Owner]
\textbf{Owner} role (\code{roles/owner}), has all editor permissions and permissions for managing roles and permissions
for a project and all resources within the project, and setting up billing for a project.
\ed

All 3 basic roles include thousands of permissions across all GCP services. In production environments, it is
reccomended to not grant basic roles, unless there is no alternative, but instead grant the most limited ``predefined''
or ``custom'' roles that meet the underlying needs.

\bd[Predefined Role]
A \textbf{predefined role} gives granular and finer-grained access control than the basic roles. Predefined roles have
a name in the format \code{roles/<service>.<role-name>}.
\ed

Predefined roles are created and maintained by Google. Google automatically updates their permissions as necessary,
such as when GCP adds new features or services. One can grant multiple roles to the same principal, at any level
of the resource hierarchy.

\bd[Custom Role]
A \textbf{custom role} is a user-define role with tailored permissions that meet specific needs. Custom roles have a
name in the format \code{projects/<project-id>/roles/<role-name>}.
\ed

Custom roles are created within a project or organization, and they allow to bundle one or more supported permissions
to meet specific needs that predefined roles don't meet. Custom roles are not maintained by Google; when new permissions,
features, or services are added to GCP, custom roles will not be updated automatically. \v

As we already mentioned, one creates roles to bundle permissions together, and then grant those roles to principals.
This is done through the use of ``bindings''.

\bd[Binding]
A \textbf{binding} binds one or more principals to a single role.
\ed

Finally, we come to the concept of policies which is nothing more than a collection of bindings.

\bd[Policy / Allow Policy / IAM Policy]
A \textbf{policy}, or \textbf{allow policy}, or \textbf{IAM policy} is a collection of bindings defining and enforcing
what roles are granted to which principals.
\ed

Each policy is attached to a resource. When one wants to define who (principal) has what type of access (role) on a
resource, they create a policy and attach it to the resource. \v

\fig{gcp15}{0.59}

Policies can be written in \code{JSON} or \code{YAML} formats.

\be
An example of a policy written in YAML:
\begin{block}
bindings:
- members:
  - user:mike@example.com
  - group:admins@example.com
  - domain:google.com
  - serviceAccount:my-project-id@appspot.gserviceaccount.com
  role: roles/resourcemanager.organizationAdmin
- members:
  - user:eve@example.com
  role: roles/resourcemanager.organizationViewer
  condition:
    title: expirable access
    description: Does not grant access after Sep 2020
    expression: request.time < timestamp('2020-10-01T00:00:00.000Z')
etag: BwWWja0YfJA=
version: 3
\end{block}
\ee

Once a policy is created, it is attached to a project by using the GCP SDK command:
\begin{bash}
# set policy for a resource
gcloud projects set$-$iam$-$policy <project-id> <policy$-$file>
\end{bash}

\begin{bash}
# get policy for a resource
gcloud projects get$-$iam$-$policy <project-id>
\end{bash}

\begin{bash}
# remove policy for a resource
gcloud projects remove$-$iam$-$policy <project-id> <policy$-$file>
\end{bash}

Policies can be set at any level in the resource hierarchy, i.e.\ at organization, folder, project, or resource level.
Resources inherit the allow policies of all of their parent resources. The effective policy for a resource is the
union of the policies set on that resource and the policies inherited from higher up in the hierarchy. \v

This policy inheritance is transitive; in other words, resources inherit policies from the project, which inherit
policies from folders, which inherit policies from the organization. Therefore, organization level policies also
apply at the resource level. \v

Policies for child resources inherit from policies for their parent resources. For example, if you grant
the Editor role to a user for a project, and grant the Viewer role to the same user for a child resource, then the
user still has the Editor role grant for the child resource. If you change the resource hierarchy, the policy
inheritance changes as well. \v

Going full circle back to IAM, recall from the definition that IAM is a framework providing a set of methods to manage
policies. These methods are:
\bit
\item \textbf{setIamPolicy}: Sets the IAM policy for a resource.
\item \textbf{getIamPolicy}: Gets the IAM policy for a resource.
\item \textbf{testIamPermissions}: Tests the permissions of a principal against a resource.
\eit

When an authenticated principal attempts to access a resource, IAM checks the resource's policy to determine whether
the action is permitted. With IAM, every API method across all GCP services is checked to ensure that the principal
making the API request has the appropriate permission to use the resource. \v

Although as we saw GCP SDK provides commands to interact with IAM, the most efficient way to do is to use the GCP
console, where one can easily manage policies for all resources in a project or organization.

\section{Google Compute Engine (GCE)}

\bd[Google Compute Engine (GCE)]
\textbf{Google Compute Engine} (\textbf{GCE}) is an infrastructure as a service (IaaS) component of GCP which offers
self-managed virtual machines (VMs).
\ed

Some of the benefits of using GCE are:
\bit
\item \textbf{Extensibility}: GCE integrates with other GCP technologies to extend beyond the basic computational
capability to create more complex and sophisticated applications.
\item \textbf{Scalability}: One can scale the number of compute resources as needed without having to manage the
underlying infrastructure. This is useful for businesses that experience sudden increases in traffic, because they can
quickly add more resources to handle the increase and remove the resources after they are no longer needed.
\item \textbf{Reliability}: Google's infrastructure is highly reliable, with a 99.9\% uptime guarantee.
\item \textbf{Cost-effectiveness}: GCE offers a variety of pricing options, and it follows a pay-as-you-go model, where
one only pays for the resources that they use, without up-front costs.
\eit

\bd[Instance]
An \textbf{instance} is a VM hosted on GCE\@, carrying a unique and permanent name, and location
\ed

Instances can be configured in different ways allowing flexibility to fulfill the needs of each project:
\bit
\item \textbf{Location Selection}: Google offers worldwide regions for deploying GCE resources. Once can choose a
region that best fits the requirements of the workload based on factors such as: region-specific restrictions, user
latency by region, latency requirements of the application, amount of control over latency, and balance between low
latency and simplicity.
\item \textbf{Machine Configuration}: There are several machine families once can choose from, and each family is
further organized into machine series and machine types. One can choose the machine family, series, and type that
best fits the requirements of the workload. There are 3 types of machine families:
\bit
\item \textbf{General-purpose}: Offers several machine series with the best price-performance ratio for a variety of
workloads.
\item \textbf{Compute-optimized}: Has the highest performance per core on GCE and is optimized for compute-intensive
workloads.
\item \textbf{Memory-optimized}: Ideal for the most memory-intensive workloads.
\eit
\fig{gcp16}{0.35}
\item \textbf{Operating System}: One can choose from a variety of operating systems, including public images, private
custom images, and marketplace images.
\item \textbf{Storage Options}: One can choose from a variety of storage options:
\bit
\item \textbf{Persistent Disk}: High-performance and redundant network storage. Each volume is striped across hundreds
of physical disks.
\item \textbf{Hyperdisk}: The fastest redundant network storage for GCE, with configurable performance and volumes that
can be resized dynamically. Each volume is striped across hundreds of physical disks.
\item \textbf{Local SSD}: Physical drives that are attached directly to the same server as a compute instance. They
offer better performance, but are not durable. If the instance is shut down, then the Local SSD disks are deleted.
\eit
\eit

Last but not least, when it comes to billing of instances, GCP follows a resource based method i.e.\ each individual
vCPU and each GB of memory is billed separately. All vCPUs, GPUs, and GB of memory are charged by the second with a
minimum of 1 minute. Another major factor of billing is the so called ``instance uptime'' which is the number of
seconds between when one starts an instance and when the instance is terminated.

\subsection{gcloud compute}

GCE can be accessed using the GCP console, the GCP SDK, or using a REST-based API. It can also be used through a variety
of programming languages available with client libraries. As usual, in these notes we will focus on the GCP SDK\@.

\begin{bash}
# create an instance with a specific configuration
gcloud compute instances create <instance> $-$$-$zone=<zone> $-$$-$machine$-$type=<machine$-$type> $-$$-$image=<image>
\end{bash}

\begin{bash}
# list all instances
gcloud compute instances list
\end{bash}

\begin{bash}
# start an instance
gcloud compute instances start <instance>
\end{bash}

\begin{bash}
# stop an instance
gcloud compute instances stop <instance>
\end{bash}

\begin{bash}
# delete an instance
gcloud compute instances delete <instance>
\end{bash}

Once an instance is created, we can connect to it through a secure shell (SSH) protocol. To do so, first we need to
create a pair of public-private RSA keys and copy the public key to the clipboard:
\begin{bash}
# create public$-$private key pair
ssh$-$keygen $-$f <filename> && cat <public$-$key> | pbcopy
\end{bash}

The, we can add the public key to the instance's metadata:
\begin{bash}
# add public key to instance's metadata
gcloud compute instances add-metadata <instance> --metadata-from-file ssh-keys=<public$-$key>
\end{bash}

Now we can SSH into the instance by running:
\begin{bash}
# connect to an instance
ssh <username>@<IP>
\end{bash}

\section{Google Kubernetes Engine (GKE)}

\textbf{Important}:~\textit{This section requires a basic understanding of Kubernetes. If you are not familiar with
Kubernetes, we suggest you to first read the corresponding ``Kubernetes'' (\ref{sec:kubernetes}) section in the
``Tecnologies'' chapter of these notes.}

\bd[Google Kubernetes Engine (GKE)]
\textbf{Google Kubernetes Engine} (\textbf{GKE}) is a container as a service (CaaS) component of GCP which offers
a managed environment for deploying, managing, and scaling containerized applications, powered by Kubernetes.
\ed

Some of the benefits of using GKE are:
\bit
\item \textbf{Platform Management}: GKE manages the underlying infrastructure, including the nodes and the control
plane, so that one can focus on deploying and managing applications.
\item \textbf{Improved Security Posture}: GKE provides a hardened node operating system, built-in security measures,
automatic upgrades to new GKE versions, and integrated security posture monitoring tooling.
\item \textbf{Cost Optimization}: GKE offers a pay-as-you-go pricing model, where one only pays for the resources
that they use.
\item \textbf{Reliability and Availability}: GKE offers a 99\% monthly uptime service level objective (SLO), and
proactive monitoring and recommendations to mitigate potential workload disruptions.
\eit

A GKE environment consists of of a control plane and multiple nodes (worker machines) which are GCE instances (VMs),
that are grouped together to form a GKE cluster. GKE manages the entire underlying infrastructure of clusters,
including the control plane, nodes, and all system components. \v

The control plane is the unified endpoint for the cluster running all the necessary processes including managing
what runs on all of the cluster's nodes, scheduling and managing apps' lifecycle, and managing network and storage
resources. \v

The user packages apps into containers, and deploys sets of containers as pods to the nodes. Then, the user uses
Kubernetes to interact with the GKE cluster through the control plane and perform processes including administering,
scaling, and monitoring the cluster and its nodes.

\fig{gcp6}{0.5}

\subsection{Kubectl}

\bd[Kubectl]
\textbf{Kubectl} is the GCP SDK component which allows to interact with GKE clusters.
\ed

Kubectl is not one of the default components of GCP SDK and it needs to be installed separately:
\begin{bash}
# install kubectl
gcloud components install kubectl
\end{bash}

To begin, with the \code{kubectl create deployment} command one can create everything they need:
\begin{bash}
# create a deployment with a given name from a given image
kubectl create deployment <name> --image=<image>
\end{bash}

As we have already said in the Kubernetes section, deployment is a blueprint that has all the information needed for
creating the pod. The command above is the most basic configuration for a deployment where we are just stating the
name and the image, and the rest is just the defaults. However, in most cases we want to specify a lot of thing in
the deployment. More on that, in the next section. \v

In case we want to change something in the deployment we can use the \code{kubectl edit deployment} command:
\begin{bash}
# edit a deployment
kubectl edit deployment <name>
\end{bash}

Similarly to delete a deployment:
\begin{bash}
# delete a deployment
kubectl delete deployment <name>
\end{bash}

With the \code{kubectl get} command one can display a table of the most important information about the specified
component:
\begin{bash}
# display important information about the component in specified format
kubectl get <component> $-o$ <format>
\end{bash}

The component can be any component we introduced in the Kubernetes section: \code{nodes}, \code{pod}, \code{services},
\code{deployment}, \code{replicaset}, etc. Including the output flag \code{-o} one can get the information in a
desired output format. Among the most useful output formats are: \code{json}, \code{yaml}, \code{name},
\code{go-template} and \code{wide}.

One can get a detailed description of a pod, including related resources such as events or controllers by using the
\code{kubectl describe} command:
\begin{bash}
# show details of a specific pod
kubectl describe <pod>
\end{bash}

When it comes to troubleshooting, if something goes wrong in the pod, one can see its logs by:
\begin{bash}
# batch$-$retrieve logs present at the time of execution
kubectl logs <pod>
\end{bash}

More often than not one wants to actually connect to the pod and use the terminal directly in the container. We can
achieve this with a combination of tty \code{-t} flag which allocates a pseudo-TTY and interactive \code{-i} flag which
keeps \code{STDIN} open:
\begin{bash}
# connect to a running pod
kubectl exec $-$i $-$t <pod>
\end{bash}

\subsection{Configuration Files}

Previously, with the \code{kubectl create deployment} command, we gave a minimalistic example of creating a deployment,
where we just specified the name and the image. As we also mentioned, in most cases we want to specify a lot of things
in the deployment. Obviously writing all this configuration down in command line will be impractical. Because of that,
in practice one would usually work with the so-called ``configuration files''.

\bd[Configuration File]
A \textbf{configuration file} is a file that defines the configuration for a Kubernetes object.
\ed

Kubernetes configuration files are more often than not \code{YAML} files, stored in version control systems before
being pushed to the cluster (this allows us to quickly roll back a configuration change if necessary). \v

The format of a configuration file is very specific:
\bit
\item \textbf{apiVersion}: The file starts with the API version that we want to use in the first line.
\item \textbf{kind}: It continues with the kind of component we want to create (e.g.\ deployment, pod, service, etc) in
the second line.
\item \textbf{metadata}: The next couple of lines are the metadata of the component, including the name of the
component, a label to distinguish it among other components and many other things.
\item \textbf{specs}: The next part is the actual specifications of the component, where we configure the actual objects
that make up the component.
\item \textbf{status}: The last part is the status of the component, which is automatically generated by Kubernetes.
\eit

\begin{block}
apiVersion: <api-version>
kind: <component>
metadata:
    name: <component-name>
    labels:
        <label-name>: <label-value>
    $\dots$
specs:
    $\dots$
status:
    $\dots$
\end{block}

Once all the specifications are gathered in the configuration file, one just tells kubectl to execute that configuration
file by using the \code{kubectl apply} command which takes the configuration file as a parameter and performs the
instructions in it:
\begin{bash}
# create cluster through a configuration file
kubectl apply $-$f <configuration$-$file>
\end{bash}

In real life, one interacts with Kubernetes through configuration files, so from now on we will be doing everything
related to Kubernetes through configuration files. So for example, in order to delete a cluster, one would simply
delete the configuration file that created the cluster:
\begin{bash}
# delete a cluster by deleting the configuration file
kubectl delete $-$f <configuration$-$file>
\end{bash}

As a last note, a project may have multiple configuration files (one for each component), or a single configuration file
with many components (either as nested configurations or separated by dashes). \v

Let's see an example of a project with built with configuration files to get a better understanding.

\subsection*{Application: A Mongo Database With UI Build With Configuration Files}

In this example we will deploy two applications: a Mongo database (MongoDB) and a Mongo Express UI (Mongo Express).
First things first, let's create a secret configuration map which we will call \code{mongo-secret.yaml} to store the
credentials for the Mongo database:
\begin{block}
apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: dXNl-cm5hbWU=
    mongo-root-password: cGFzc3dvc-mQ=
\end{block}

Before anything we need to create the secret component inside the cluster in order to be available for the next
configuration files:
\begin{bash}
# create secret component
kubectl apply $-$f mongo$-$secret.yaml
\end{bash}

Now we will create the MongoDB pod and an internal service for other components inside the cluster to be able to talk
to it. We will do that via a configuration file which it will have a nested configuration template for creating the
MongoDB pod, and attached to it will have a service configuration file (since it makes sense to be put together). Let's
call this configuration file \code{mongo-db.yaml}:
\begin{block}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
spec:
  selector:
    app: mongodb
  ports:
    - protocol: TCP
      port: 27017
      targetPort: 27017
\end{block}

Now we are ready to create the MongoDB components (pod and service) inside the cluster:
\begin{bash}
# create MongoDB components
kubectl apply $-$f mongo$-$db.yaml
\end{bash}

Now we want to create the Mongo Express UI. Before that, we're going to need a database URL of Mongo database (so that
Mongo Express can connect to it) and credentials of the database. Credentials are already in place with the secret
component, however will create a config map component that contains the database URL. Let's name this configuration
file \code{mongo-configmap.yaml}:
\begin{block}
apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-configmap
data:
  database_url: mongodb-service
\end{block}

As with the secret component, before anything we need to create the config map component inside the cluster in order
to be available for the next configuration files:
\begin{bash}
# create mongo database component
kubectl apply $-$f mongo$-$configmap.yaml
\end{bash}

Now we can create the Mongo Express component through another deployment configuration file which we will call it
\code{mongo-express.yaml}
\begin{block}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-express
  labels:
    app: mongo-express
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-express
  template:
    metadata:
      labels:
        app: mongo-express
    spec:
      containers:
      - name: mongo-express
        image: mongo-express
        ports:
        - containerPort: 8081
        env:
        - name: ME_CONFIG_MONGODB_ADMIN_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: ME_CONFIG_MONGODB_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
        - name: ME_CONFIG_MONGODB_SERVER
          valueFrom:
            configMapKeyRef:
              name: mongodb-configmap
              key: database_url
\end{block}

Finally, we create the Mongo Express in the same way as MongoDB\@.

%TODO: Write the Google App Engine (GAE) section

\section{Cloud Run}

\bd[Cloud Run]
\textbf{Cloud Run} is a function as a service (FaaS) component of GCP which offers a managed platform for running
containerized applications on Google's scalable infrastructure.
\ed

Cloud Run supports containerized image applications written in any programming language. While Cloud Run requires the
application to be packaged in a container image, it doesn't require the developer to build the container image
themselves, since one can use the source-based deployment option that builds the container automatically, using the
best practices for the language in use. \v

On top of that, Cloud Run was built to work well together with other services on GCP, so one can build full-featured
applications. In short, Cloud Run allows developers to spend their time writing their code, and very little time
operating, configuring, and scaling their Cloud Run service. Onr doesn't have to create a cluster or manage
infrastructure in order to be productive with Cloud Run\@. \v

Cloud Run is a good fit for use cases serving requests, built to handle multiple instances of the app running
simultaneously, that do not require a local persistent file system, and more than 8 CPU and 32 GiB of memory per
instance. \v

To enable the Cloud Run Admin API for the project:
\begin{bash}
# enable Cloud Run Admin API
gcloud services enable run.googleapis.com
\end{bash}

After Cloud Run is enabled, the Compute Engine default service account is automatically created. \v

Cloud Run requires Cloud Build\footnote{For more information on Cloud Build check the corresponding chapter
(\ref{sec:cloud_build}).} to build container images, so one needs to enable it. In order for Cloud Build to be able to
build sources, we need to grant the \code{Cloud Build Service Account} role to the Compute Engine default service
account by running:
\begin{bash}
# grant Cloud Build Service Account role to Compute Engine default service account
gcloud projects add$-$iam$-$policy$-$binding <project$-$id>
    $-$$-$member=serviceAccount:<project$-$number>$-$compute@developer.gserviceaccount.com
    $-$$-$role=roles/cloudbuild.builds.builder
\end{bash}

Cloud Run also requires Artifact Registry\footnote{For more information on Artifact Registry check the corresponding
chapter (\ref{sec:artifact_registry}).} to store container images, so one needs to enable it. In order to be able to
push images to the Artifact Registry, we need to grant the \code{Storage Object Admin} role to the Compute Engine
default service account by running:
\begin{bash}
# grant Storage Object Viewer role to the Compute Engine default service account
gcloud projects add$-$iam$-$policy$-$binding <project$-$id>
    $-$$-$member=serviceAccount:<project$-$number>$-$compute@developer.gserviceaccount.com
    $-$$-$role=roles/storage.objectViewer
\end{bash}

Last but not least, a good (but optional) idea, is to set the default Cloud Run region with the GCP SDK properties to
avoid prompts from the command line:
\begin{bash}
# set the default Cloud Run region
gcloud config set run/region <region>
\end{bash}

On Cloud Run, code can run in 3 different ways that we will examine separately:
\bit
\item As a \textbf{service}: A long-lived container that listens for incoming requests, events, or functions.
\item As a \textbf{function}: A special form of service, including a container that runs a single function in response
to an event.
\item As a \textbf{job}: A container that runs a single job to completion.
\eit

\subsection{Cloud Run Service}

\bd[Cloud Run Service]
\textbf{Cloud Run service} is used to run code continuously, responding to requests, events, or functions.
\ed

\fig{gcr}{0.45}

A Cloud Run service provides the infrastructure required to run a reliable, long-lived container that listens for
incoming HTTP requests, Pub/Sub messages, or other events, and serves responses. User's responsibility is to make
sure the application listens on a port and handles requests. Possible use cases include websites and applications,
APIs and microservices, streaming data processing, asynchronous workloads, and AI inference. Some of the features of
Cloud Run services are:

\bit
\item \textbf{Unique HTTPS Endpoint}: Every Cloud Run service is provided with an HTTPS endpoint on a unique
subdomain of the \code{*.run.app} domain. This endpoint is automatically generated by Cloud Run and is based on the
service name and the region where the service is deployed. Cloud Run takes care of all the underlying infrastructure
needed.

\item \textbf{Fast Request-Based Auto Scaling}: Cloud Run is built to rapidly and automatically add and remove
instances to handle all incoming requests or to handle increased CPU utilization outside requests. For this to work,
the billing must be set to ``instance-based billing'' where one is charged for the entire lifetime of an instance,
without per-request fees. On the other hand one can choose the ``request-based billing'' where one is charged
per-request, and when an instance is not processing requests, they're not charged. \v

A service can rapidly scale out to one thousand instances. One can utilize even more than one thousand instance by
requesting a quota increase. On the other hand, if one is concerned about costs or overloading downstream systems,
they can also limit the maximum number of instances to less than one thousand. \v

If demand decreases, Cloud Run removes idle containers, and if there are no incoming requests to the service, even
the last remaining instance will be removed. This behavior is commonly referred to as ``scale to zero'' and is
useful for services that have low or intermittent traffic. When a service scales to zero, it doesn't consume any
resources, and one doesn't pay for any instances. When a new request comes in, Cloud Run automatically starts a new
instance to handle the request. This negatively impacts the response time for these first requests, depending on how
fast the container becomes ready to handle requests. To make sure a service doesn't scale to zero instances, one can
configure Cloud Run to keep a minimum amount of instances active.

\item \textbf{Built-In Traffic Management}: Every deployment creates a new immutable revision. One can route
incoming traffic to the latest revision, roll back to a previous revision, or split traffic to multiple revisions at
the same time, to perform a gradual rollout. This is useful if one wants to reduce the risk of deploying a new
revision. They can start with sending 1\% of requests to a new revision, and increase that percentage gradually as
they gain confidence in the new revision.

\item \textbf{Private and Public Services}: A Cloud Run service can be reachable from the internet, or can restrict
access by specifying an access policy using Cloud IAM, by using ingress settings to restrict network access (this is
useful if one wants to allow only internal traffic from the VPC and internal services, or by allowing only
authenticated users with Identity-Aware Proxy (IAP).

\item \textbf{Disposable Container Filesystem}: Instances on Cloud Run are disposable. Every container has an
in-memory, writable filesystem overlay, which is not persisted if the container shuts down. Cloud Run decides
independently when to stop sending requests to an instance and shut it down. This means that the container must be
stateless, and any data that needs to be persisted must be stored in an external data store. One can achieve this by
integrating with Cloud Storage.
\eit

Now, let's move on and see how we can deploy an application to Cloud Run. First and foremost, as we have already
seen, in order to be able to deploy in Cloud Run, one has to enable the Cloud Run Admin API, the Cloud Build API,
and the Artifact Registry API, and assign the necessary roles to the Compute Engine default service account. \v

Secondly, one needs (of course) an application to run. While Cloud Run allows the code to be built in any
programming language, in these notes we will be assuming an application built in Python. On top of that, Cloud Run
also expects the application to be stateless (not reyling on a persistent local state) and to be listening for
requests. One can choose among HTTP requests, Pub/Sub messages, or events. In these notes we will be assuming a web
server application that listens for HTTP requests. (One can use any web server framework they like, such as Flask,
Django, FastAPI, etc). A very important caveat is that the service must listen for requests on the port specified by
the \code{PORT} environment variable, which be default is the port \code{8080}. \v

Finally, as we already mentioned, Cloud Run requires a container image to run. This step can be done in two different
ways, depending on the deployment method one picks: ``source-based deployment'' or ``container-based deployment''.

\subsubsection{Deploying A Cloud Run Service With Source-Based Deployment}

\bd[Source-Based Deployment]
\textbf{Source-based deployment} is a deployment method where Cloud Run automatically builds a container image from
source code and deploys it to the Cloud Run service.
\ed

Source-based deployment is the easiest way to deploy an application to Cloud Run, as it doesn't require the developer
to build the container image themselves. Cloud Run automatically builds the container image from the source code
using Cloud Build, and deploys it to the Cloud Run service. \v

In order to do so, the user must include a \code{Procfile} in the root of the project, which specifies the command that
starts the application:
\begin{block}
web: <command>
\end{block}

\be
For example, for a Python application, an option for the command would be \code{web: python3 app.py}.
\ee

Then, one can deploy the application to Cloud Run using the following command:
\begin{bash}
# deploy an application to Cloud Run using source-based deployment
gcloud run deploy <service> $-$$-$source=<source>
\end{bash}

Upon successful completion, a message is displayed along with the URL of the deployed service. One can then see the
Cloud Run service listed by running:
\begin{bash}
# list all Cloud Run services
gcloud run services list
\end{bash}

\begin{bash}
# show details of a specific Cloud Run service
gcloud run services describe <service>
\end{bash}

\begin{bash}
# delete a Cloud Run service
gcloud run services delete <service>
\end{bash}

While source-based deployment is the easiest way to deploy an application to Cloud Run, it is generally not recommended
for production workloads, since it it lacks some of the flexibility of ``container-based deployment''.

\subsubsection{Deploying A Cloud Run Service With Container-Based Deployment}

\bd[Container-Based Deployment]
\textbf{Container-based deployment} is a deployment method where one builds a container image and deploys it to the
Cloud Run service.
\ed

Container-based deployment is the most flexible (and the go-to way) to deploy an application to Cloud Run, as it allows
the developer to build the container image themselves, and to customize the container image as needed. One can pick any
container technology they like, however, in these notes we will be working exclusively with Docker.\footnote{For more
information on Docker check the corresponding chapter (\ref{sec:docker}).} \v

Cloud Run expects the docker container images to be stored in the Artifact Registry, so we need to build a repository
in the Artifact Registry to store the docker container images:
\begin{bash}
# create a repository in the Artifact Registry
gcloud artifacts repositories create <repository>
    $-$$-$repository$-$format=docker
    $-$$-$repository$-$mode=standard
    $-$$-$description=<description>
    $-$$-$immutable$-$tags
    $-$$-$async
\end{bash}

Optionally we can also configure Docker to get access to Artifact Registry:
\begin{bash}
# configure Docker to authenticate to Artifact Registry
gcloud auth configure$-$docker <location>$-$docker.pkg.dev
\end{bash}

As we descirbed in the Docker section, the way to containerize an application (and what Cloud Run expects) is by
creating a \code{Dockerfile} in the root of the project and build the container image by running:
\begin{bash}
# build the container image locally
docker build . $-$$-$tag <image$-$url>
\end{bash}

where the \code{$<$image-url$>$}'' is the URL of the container image and must satisfy the following format:
\begin{block}
<location>$-$docker.pkg.dev/<project$-$id>/<repository>/<image>:<tag>
\end{block}

Then, one can push the container image to the Artifact Registry:
\begin{bash}
# push the container image to the Artifact Registry
docker push <image$-$url>
\end{bash}

Once the docker container image is pushed to the Artifact Registry, one can see it by using the commands described
in the Artifact Registry section. One can deploy the application to Cloud Run by:
\begin{bash}
# deploy an application to Cloud Run using container$-$based deployment
gcloud run deploy <service> $-$$-$image=<image$-$url> $-$$-$allow$-$unauthenticated
\end{bash}

The \code{--allow-unauthenticated} flag assigns the \code{Cloud Run Invoker IAM} role to \code{allUsers}, hence it
allows unauthenticated invocations. This is useful for public services. Conversely, one can use the flag
\code{--no-allow-unauthenticated} to disallow unauthenticated invocations. \v

As with the case of source-based deployment, upon successful completion, a message is displayed along with the URL
of the deployed service. \v

If one needs to redeploy a new revision of the service, they can simply run the same command. Cloud Run will
automatically create a new revision, will add a \code{--revision-suffix} to the service name, and will route incoming
traffic to the new revision. The user has the option to choose their own revision name by using the
\code{--revision-suffix} flag. \v

As the reader can see, this process contains a lot of manual steps that one can automate by using Cloud Build. To do so,
one needs to create a \code{cloudbuild.yaml} file with all the aforementioned steps:
\begin{block}
steps:

    $\#$ Build the container image
    - name: 'gcr.io/cloud-builders/docker'
      entrypoint: 'docker'
      args: ['build', '.', '--tag', '<image-url>']

    $\#$ Push the container image to Artifact Registry
    - name: 'gcr.io/cloud-builders/docker'
      entrypoint: 'docker'
      args: ['push', '<image-url>']

    $\#$ Deploy container image to Cloud Run
    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      entrypoint: gcloud
      args: ['run', 'deploy', '<service>', '--image', '<image-url>','--allow-unauthenticated']
\end{block}

Then, one can build, push, and deploy the container image using Cloud Build:
\begin{bash}
# build, push, and deploy the container image using Cloud Build
gcloud builds submit
\end{bash}

On top of that, one can also automate the deployment process by using Cloud Build triggers, as described in the
Cloud Build section (\ref{subsec:cloud_build_triggers}).

\subsection{Cloud Run Functions}

A special form of Cloud Run services are the so-called ``Cloud Run functions''\footnote{Cloud Run Functions used to be
a sepate service called ``Cloud Functions''. This service was integrated into Cloud Run and now it is called `` Cloud
Run Functions''.}.

\bd[Cloud Run Functions]
A \textbf{Cloud Run function} is a Cloud Run service that is designed to run a single function in response to an event.
\ed

Cloud Run functions are lightweight, stateless, and ephemeral containers designed to run a single function in response
to an event, rather than continuously listening for incoming requests. That makes them useful for running code that
performs a single task. \v

The use case boundary between Cloud Run services and Cloud Run functions is not always clear, but as a rule of
thumb, one would pick Cloud Run functions over Cloud Run services when they have small, single-purpose functions
that respond to events, and they prefer a simpler deployment model without the overhead of a full container setup. \v

Due to their simplistic nature, Cloud Run functions are also quite simple to implement. One can write a function in
any programming language, as long as it listens for events on the port specified by the \code{PORT} environment
variable, which by default is the port \code{8080}. In these notes, as usual, we will be assuming a function written in
Python. \v

The only things one needs to deploy a Cloud Run function in Python is a `requirements.txt and a `main.py` file. \v

Starting with the \code{requirements.txt}, this is the usual requirements file that lists the dependencies of the
function, including the \code{functions\_framework} library.

\bd[Functions Framework]
\textbf{Functions Framework} is an open source library for writing portable Python functions that run in Cloud Run
environment, developed by Google.
\ed

Functions Framework requires:
\bit
\item \textbf{Entry Point}: An entry point for the function, which is the particular code that is executed when
the Cloud Run function is invoked. For Python, this is the name of the function as defined in the \code{main.py} file.
One specifies  entry point during deployment.
\item \textbf{Signature Type}: The signature type of the function, which is the type of the function that is executed
when the Cloud Run function is invoked. It can be one of the two types:
\bit
\item \textbf{HTTP Functions}: A function that registers an HTTP handler function. Used when the function needs to
have a URL endpoint and respond to HTTP requests.
\item \textbf{CloudEvents Functions}: A function that registers a CloudEvents handler function. Used when the function
should be triggered directly in response to events, such as messages on a Pub/Sub topic or changes in a Cloud Storage
bucket.
\eit
\eit

Moving on to the \code{main.py} file, this is the file that contains the function itself. As we already mentioned,
the name of the function acts as the entry point, and for the signature type to be declared, the function must be
wrapped with the \code{@functions\_framework.http} decorator for HTTP functions, or the
\code{@functions\_framework.cloud\_event} decorator for CloudEvents functions. In both cases, the function must have
an argument called \code{request} for HTTP functions, or \code{cloud\_event} for CloudEvents functions, that
represents the incoming request or event. \v

Once the function is written, one can deploy it to Cloud Run using the following command:
\begin{bash}
# deploy a function to Cloud Run
gcloud run deploy <function> $-$$-$source=. $-$$-$function=<function> $-$$-$base$-$image=<base$-$image>
\end{bash}

Upon successful completion, a message is displayed along with the URL of the deployed function. Since Cloud Run
functions are simply a special form of Cloud Run services, one can manipulate them in the same way as Cloud Run
services, by using the commands described in the Cloud Run service section. \v

Optionally, after deploying a function, one can add triggers to the function. Given that a trigger is a highly
customizable object, it requires a lot of parameters to be set, which is why it is recommended to use the GCP Console
to add triggers to a function, instead of the GCP SDK\@.

\subsection{Cloud Run Job}

\bd[Cloud Run Job]
A \textbf{Cloud Run job} is used to run code that performs work (a job) and quits when the work is done.
\ed

While Cloud Run services (and functions) are designed to run continuously, responding to requests or events, a Cloud
Run job provides the infrastructure required to run code that performs unattended work to completion and then stops,
without listening for incoming requests. \v

A Cloud Run job can start one instance to run the code (that's a common way to run a script), or many identical,
independent instances in parallel, known as an ``array job'', to process a large number of independent tasks in
parallel. Possible use cases include running a script or tool, performing highly parallelized processes, and running
scheduled or recurring jobs.

\fig{gcr6}{0.4}

The way to deploy a Cloud Run job is similar to the way to deploy a service or a function. First, one needs to build a
container image and push it to the Artifact Registry in the same way they would do for a service or a function. Then,
one can deploy the job to Cloud Run using a slightly different command:
\begin{bash}
# deploy a job to Cloud Run
gcloud run jobs deploy <job> $-$$-$image=<image$-$url>
\end{bash}

Upon successful completion, a message is displayed. One can then see the Cloud Run job listed by running:
\begin{bash}
# list all Cloud Run jobs
gcloud run jobs list
\end{bash}

\begin{bash}
# show details of a specific Cloud Run job
gcloud run jobs describe <job>
\end{bash}

\begin{bash}
# delete a Cloud Run job
gcloud run jobs delete <job>
\end{bash}

Opposite to services and functions, once the job is deployed, it is not executed immediately (unless one uses the
\code{--execute-now} flag during deployment). Instead, one needs to execute the job manually by running:
\begin{bash}
# execute a job on Cloud Run
gcloud run jobs execute <job>
\end{bash}

Finally, one can also schedule a job to run at a specific time by using the ``Cloud Scheduler'' service.

\bd[Cloud Scheduler]
\textbf{Cloud Scheduler} is a fully managed cron job service that allows one to schedule jobs.
\ed

With Cloud Scheduler one can set up scheduled units of work to be executed at defined times or regular intervals.
These work units are commonly known as ``cron jobs''.

\bd[Cron Job]
A \textbf{cron job} is a time-based job scheduler in Unix-like operating systems. Cron jobs are used to schedule
commands or scripts to run periodically and at fixed intervals.
\ed

Each cron job created using Cloud Scheduler is sent to a target according to a specified schedule, where the work
for the task is accomplished. \v

Cloud Scheduler is designed to provide "at least once" delivery; that is, the job will run at least once per
scheduled execution. In some rare circumstances, it is possible for a job to run multiple times in association with
a single instance of the schedule. \v

To enable the Cloud Scheduler API for the project:
\begin{bash}
# enable Cloud Scheduler API
gcloud services enable cloudscheduler.googleapis.com
\end{bash}

One can create a job in Cloud Scheduler to trigger the Cloud Run job at a specific time by using:
\begin{bash}
# create a job in Cloud Scheduler
gcloud scheduler jobs create http <scheduler$-$job$-$name> \
    $-$$-$schedule="<cron$-$schedule>" \
    $-$$-$uri=<cloud$-$run$-$job$-$url> \
    $-$$-$http$-$method=POST \
    $-$$-$oauth$-$service$-$account$-$email=<service$-$account$-$email>
\end{bash}

\section{Vertex AI}

Recall from the ``Development Process'' section of the ``Machine Learning Opearations'' chapter
(\ref{sec:development_process}) that the development process of a machine learning system consists of: ``Scoping'' ,
``Data'' ,``Modeling'' ,``Deployment'' ,``Monitoring \& Maintenance''. Leaving outside ``Scoping'', GCP offers a
service called ``Vertex AI'' that provides tools for all the remaining, technical, individual steps.

\bd[Vertex AI]
\textbf{Vertex AI} is an ML platform for training, deploying, and scaling ML models and AI applications, combining data
engineering, data science, and ML engineering workflows, and enabling team collaboration using a common toolset.
\ed

To enable the Vertex AI API for the project:
\begin{bash}
# enable Vertex AI API
gcloud services enable aiplatform.googleapis.com
\end{bash}

The list of services Vertex AI provides is quite extensive, and keeps growing. Here we will go through some of them,
categorizing them according to the steps of the development process they belong.

\subsection{Data}

\bd[Vertex AI Datasets]
\textbf{Vertex AI Datasets} is a service for importing, creating, managing, and generating statistics and visualizations
on datasets.
\ed

Vertex AI Datasets is not a service that is used frequently, since it can be replaced by other services such as
BigQuery, or Google Cloud Storage. \v

On top of Vertex AI Datasets, Vertex AI provides the ``Vertex AI Feature Store'' service for storing and serving
features.

\bd[Vertex AI Feature Store]
\textbf{Vertex AI Feature Store} is a service for importing, organizing, monitoring,and serving features for ML models.
\ed

As of now, Vertex AI Feature Store is the only feature store solution provided by GCP\@. However, one can use other
open source solutions such as Feast. \v

Last but not least, for the initial phase of exploring, visualizing, and manipulating data, Vertex AI provides the
``Vertex AI Workbench'' service.

\bd[Vertex AI Workbench]
\textbf{Vertex AI Workbench} is a fully managed Jupyter notebook-based development environment.
\ed

Vertex AI Workbench's notebooks integrate with all necessary GCP services, including Vertex AI itself, BigQuery and
Cloud Storage. In this way, Vertex AI Workbench allows fast access and exploration of data from within a Jupyter
notebook and it automates recurring updates of models by using scheduled executions of notebook's code as a step in
Vertex AI Pipeline that will be discussed later. \v

To start using Vertex AI Workbench one needs to create a new instance. One can use the GCP SDK for that, but given
that the creation of a new instance is a highly configurable process with a couple of parameters, it is recommended
to use the GCP Console. All Vertex AI Workbench instancess are prepackaged with JupyterLab and have a preinstalled
suite of deep learning packages, including support for TensorFlow and PyTorch frameworks. They also support the
ability to sync with a GitHub repository. All instances are protected by Google Cloud authentication and authorization.
\v

While Vertex AI Workbench is a good fit for teams that want to work together on the same project, as it provides a
centralized environment for data exploration and model development, it is another ``optional'' service, since one can
simply work on their local machine with Jupyter notebooks.

\subsection{Modeling}

When it comes to modeling, Vertex AI provides a couple of services ranging from high level solutions that provides a
point-and-click interface for using pre-built models, to low level solutions that provides a fully managed service for
training custom models. \v

Starting with the high level solutions, Vertex AI provides the ``Model Garden'' service.

\bd[Model Garden]
\textbf{Model Garden} is a service for discovering, testing, customizing, and deploying Vertex AI and other selected
open-source models.
\ed

Model Garden provides a collection of pre-built models that are ready to be used, and can be customized and deployed
to production. Model Garden is a good fit for teams that want to use pre-built models, as it provides a centralized
location for discovering, testing, and deploying models. \v

Moving on to the low level solutions, Vertex AI provides the ``Vertex AI Training'' service for training custom models.

\bd[Vertex AI Training]
\textbf{Vertex AI Training} is a service for training ML models using the preferred ML framework.
\ed

Vertex AI Training works with the so called ``Vertex AI  Training Pipelines''.

\bd[Vertex AI Training Pipeline]
A \textbf{Vertex AI Training Pipeline} is a managed Kubeflow service which helps orchestrate complex data and ML
workflows.
\ed

Vertex AI Training Pipelines are the primary model training workflow in Vertex AI\@. Vertex AI Training Pipelines
provide a couple of features including support for the Kubeflow Pipelines SDK, TensorFlow Extended (TFX), and the
ability to deploy vertically scalable components as containers. \v

Vertex AI Training provides two model training options:
\bit
\item \textbf{AutoML}: For training tabular, image, text, or video data without writing code or preparing data splits.
\item \textbf{Custom Training}: For complete control over the training process, including using the preferred ML
framework and writing own training code.
\eit

Vertex AI Training is yet another optional (and quite limiting and vendor-locking) service of Vertex AI. Alternatively
one can simply use Kubeflow on GKE or even better Airflow in Composer (more on that later) which gives more flexibility
and control over the training frameworks one can use. \v

On top of Vertex AI Training, Vertex AI provides two more modeling related services: ``Vertex AI Experiments'' and
``Vertex AI Metadata''.

\bd[Vertex AI Experiments]
\textbf{Vertex AI Experiments} is a service for tracking and comparing ML experiments.
\ed

\bd[Vertex AI Metadata]
\textbf{Vertex AI Metadata} is a service for managing the lifecycle of metadata consumed and produced by ML workflows,
by storing parameters and artifacts to enable lineage tracking, auditing, model reproducibility.
\ed

As of now, Vertex AI Experiments and Metadata are the only experiment and metadata tracking solution provided by GCP\@.
However, one can use other open source solutions such as MLflow.

\subsection{Deployment}

Moving on to deployment, Vertex AI provides a couple of services that helps with the deployment of models. Let's start
with the ``Vertex AI Model Registry'' service.

\bd[Vertex AI Model Registry]
\textbf{Vertex AI Model Registry} is a centralized registry for ML models.
\ed

In Vertex AI Model Registry one can store all trained ML models for versioning, regardless of whether they were
custom models or models generated by using AutoML capabilities. It also integrates with validation and deployment
features, offering the ability to compare models. \v

As of now, Vertex AI Model Registry is the only model registry solution provided by GCP\@. However, one can use other
open source solutions such as MLflow. \v

Moving on to serving models, Vertex AI provides the ``Vertex AI Online Prediction'' and ``Vertex AI Batch Prediction''
services.

\bd[Vertex AI Online Prediction]
\textbf{Vertex AI Online Prediction} is a service for getting predictions from a trained model in real-time.
\ed

Vertex AI Online Prediction works with the so called ``Vertex AI Endpoints''.

\bd[Vertex AI Endpoints]
\textbf{Vertex AI Endpoints} is a managed model serving capability for real-time prediction use cases.
\ed

Vertex AI Endpoints features include a configurable serving infrastructure, autoscaling capabilities, and the
ability to detect the number of performance issues relating to increased prediction latency, capacity bottlenecks, \v

\bd[Vertex AI Batch Prediction]
\textbf{Vertex AI Batch Prediction} is a service for getting predictions from a trained model in batch.
\ed

Vertex AI Batch Prediction intakes a group of prediction requests and outputs the results to a specified location. Batch
prediction does not require an immediate response and can process accumulated data with a single request. \v

Both Vertex AI Online Prediction and Vertex AI Batch Prediction services are optional services, since one can simply
deploy their model to Cloud Run and get predictions from there.

\subsection{Monitoring \& Maintenance}

Last but not least, Vertex AI provides the ``Vertex AI Monitoring'' service.

\bd[Vertex AI Monitoring]
\textbf{Vertex AI Monitoring} is a service for monitoring deployed models in production.
\ed

Vertex Al Monitoring helps automate the monitoring of deployed models in production to identify performance issues
proactively. Key features include drift detection and training-serving skew detection.

\section{Cloud Build}\label{sec:cloud_build}

\bd[Cloud Build]
\textbf{Cloud Build} is a fully managed service that executes builds on GCP\@.
\ed

Cloud Build can import source code from a variety of repositories or cloud storage spaces, execute builds following
specifications, fetch dependencies, run unit tests, static analyses, and integration tests, and produce artifacts. \v

To enable the Cloud Build API for the project:
\begin{bash}
# enable Cloud Build API
gcloud services enable cloudbuild.googleapis.com
\end{bash}

After the Cloud Build API is enabled, the Cloud Build default service account is automatically created.

\subsection{Build Config File}

One can interacth with Cloud Build by creating a ``build config file''.

\bd[Build Config File]
A \textbf{build config file} is a YAML\footnote{Build config files can also be written in JSON format, but in these
notes we will stick with YAMl.} file that provides instructions to Cloud Build on what tasks to perform.
\ed

One starts by creating the build config file. The file must be named \code{cloudbuild.yaml} and must be located in the
root directory of the project, as Cloud Build looks for the build config file in the root directory of the source code.
\begin{bash}
# create a build config file
touch cloudbuild.yaml
\end{bash}

Cloud Build executes a build as a series of steps defined in the \code{steps} section of the build config file. Each
of the steps in the build config file defines a part of the task you want to perform. Executing steps is analogous
to executing commands in a script.
\begin{block}
steps:
\end{block}

Each step is run in a Docker container, specified by the \code{name} field in the step, which is the only required
field.
\begin{block}
steps:
    - name: <image-url>
\end{block}

The value of the \code{$<$image-url$>$} points to the image URL of a ``cloud builder''.

\bd[Cloud Builder]
\textbf{Cloud builder} is a container image with common tools and languages installed in it.
\ed

Cloud Build provides and maintains a list of pre-built cloud builders such as \code{gcr.io/cloud-builders/docker} for
docker, \code{gcr.io/cloud-builders/gcloud} for gcloud, etc. On top of that, Cloud Build allows the usage of any
publicly available image as a cloud builder. \v

To use a cloud builder, one specifies the image URL in the \code{name} field of \code{steps} in the build config file,
then uses the \code{entrypoint} field to specify the command that the cloud builder should run, and then the \code{args}
field to specify the arguments that the command should take (if the builder in the name field does not have an
\code{entrypoint}, the first element in \code{args} is used as the entrypoint).
\begin{block}
steps:
    - name: <image-url>
      entrypoint: '<entrypoint>'
      args: ['<arg1>', '<arg2>', $\dots$]
\end{block}

Finally, one can include any additional build configuration fields. For the complete list of fields one can include
see the \href{https://cloud.google.com/build/docs/build-config-file-schema}{Build Configuration Overview}. \v

Once the build config file is created, one can run the build by using the following command:
\begin{bash}
# run a build using the build config file
gcloud builds submit $-$$-$config <build$-$config$-$file$-$path> <source$-$path>
\end{bash}

If one does not specify the \code{--config} flag, Cloud Build assumes that the build config file and the source code
are in the current working directory.

\subsection{Cloud Build Triggers}\label{subsec:cloud_build_triggers}

Cloud Build uses ``cloud build triggers'' to enable CI/CD automation.

\bd[Cloud Build Trigger]
A \textbf{Cloud Build trigger} is a configuration that tells Cloud Build to automatically execute a build when certain
conditions are met.
\ed

Cloud Build triggers are useful for automating the build process. One can choose among various triggers types:
\bit
\item \textbf{Repository Event Triggers}: Execute builds on repository events hosted in all widely used hosts such as
GitHub, Bitbucket, and Cloud Source Repositories.
\item \textbf{Pub/Sub Triggers}: Execute builds when a message is published to a Pub/Sub topic.
\item \textbf{Webhook Triggers}: Execute builds when a webhook is sent to a specific URL\@.
\item \textbf{Manual Triggers}: Execute builds manually from the Cloud Build console.
\eit

In these notes we will focus on repository event triggers, and more specifically on GitHub repository event triggers. \v

First and foremost, one needs to connect the GitHub repository to Cloud Build. To do so, one needs to run the following
command:
\begin{bash}
# connect the GitHub repository to Cloud Build
gcloud builds connections create github <connection$-$name> $-$$-$region=<region>
\end{bash}

and follow the instructions. \v

Once the connection has been established, one can link a specific repository to the Github connection by:
\begin{bash}
# connect to a specific repository
gcloud builds repositories create <repo$-$name>
    $-$$-$remote$-$uri=<repo$-$uri>
    $-$$-$connection=<connection$-$name>
    $-$$-$region=<region>
\end{bash}

Finally, one can create a trigger by using the \code{gcloud builds triggers create} command. Given that a trigger is a
highly customizable object, it requires a lot of parameters to be set, which is why it is recommended to use the GCP
Console to add triggers to a function, instead of the GCP SDK\@.

\section{Artifact Registry}\label{sec:artifact_registry}

In a previous chapter we defined an artifact as an item generated during a process (\ref{def:artifact}). GCP provides a
service that stores, manages, and secures artifacts called ``Artifact Registry''.

\bd[Artifact Registry]
\textbf{Artifact Registry} is a service that centrally stores, manages, and secures artifacts.
\ed

To enable the Artifact Registry API:
\begin{bash}
# enable Artifact Registry API
gcloud services enable artifactregistry.googleapis.com
\end{bash}

To disable the Artifact Registry API:
\begin{bash}
# disable Artifact Registry API
gcloud services disable artifactregistry.googleapis.com
\end{bash}

The way Artifact Registry works is by creating repositories to store artifacts. One can create multiple repositories
and each one carries some specific characteristics with the most important ones being:
\bit
\item \textbf{Format}: Each repository is associated with a specific artifact format. One can choose among
many different supported formats including Docker container images, Python and Node.js packages, and many others.
\item \textbf{Mode}: Each repository has a mode, and each mode serves a different purpose, so one cannot change
the repository mode after its creation. There are 3 diffent repository modes>
\bit
\item \textbf{Standard}: Standard repositories are regular Artifact Registry repositories for private artifacts where
one can upload and download artifacts directly from the repositories.
\item \textbf{Remote}: Remote repositories are read-only repositories that act as proxies to store artifacts from either
standard Artifact Registry repositories or external sources such as Docker Hub, the Python Package Index (PyPI), etc.
\item \textbf{Virtual}: Virtual repositories are read-only repositories that act as single access points to download,
install, or deploy artifacts of the same format from other standard, remote, or virtual repositories.
\eit
\item \textbf{Location}: Each repository is associated with a specific location, which is a GCP region or multi-region
where the repository's artifacts are stored. One can choose the location that best fits the requirements of the workload
based on factors such as latency, availability, and costs.
\item \textbf{Description}: Each repository has a description that provides information about the repository's purpose,
contents, or other relevant details.
\eit

In the vast majority of cases, one will be using standard repositories to store private artifacts. To create a standard
repository in the Artifact Registry use the following command:
\begin{bash}
# create a repository in the Artifact Registry
gcloud artifacts repositories create <repository>
    $-$$-$repository$-$format=docker
    $-$$-$repository$-$mode=standard
    $-$$-$location=<location>
    $-$$-$description=<description>
\end{bash}

To update a repository decription in the Artifact Registry use the following command:
\begin{bash}
# update a repository decription in the Artifact Registry
gcloud artifacts repositories update <repository>
    $-$$-$project=<project-id>
    $-$$-$location=<location>
    $-$$-$description=<description>
\end{bash}

To view the repositories in the Artifact Registry use the following command:
\begin{bash}
# list all repositories in the Artifact Registry
gcloud artifacts repositories list
\end{bash}

To describe a repository in the Artifact Registry use the following command:
\begin{bash}
# describe a repository in the Artifact Registry
gcloud artifacts repositories describe <repository>
\end{bash}

To delete a repository in the Artifact Registry use the following command:
\begin{bash}
# delete a repository in the Artifact Registry
gcloud artifacts repositories delete <repository>
\end{bash}

Last but not least, one can view and manipulate the contents of a repository in the Artifact Registry.

\begin{bash}
# list all artifacts in a repository
gcloud artifacts files list
    $-$$-$project=<project-id>
    $-$$-$repository=<repository>
    $-$$-$location=<location>
\end{bash}

\begin{bash}
# download an artifact from a repository
gcloud artifacts files download
    $-$$-$project=<project-id>
    $-$$-$repository=<repository>
    $-$$-$location=<location>
    $-$$-$destination=<destination>
    <artifact>
\end{bash}