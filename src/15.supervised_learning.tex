%! suppress = EscapeUnderscore
%! suppress = EscapeUnderscore
%! suppress = EncloseWithLeftRight
%Supervised learning is one of the four basic categories of machine learning, and it consists of a family of models
%and techniques that we will introduce in this chapter. First let's start with a formal definition of supervised
%learning.
%
%\section{Basic Definitions}
%
%\bd[Supervised Learning]
%\textbf{Supervised learning} is the machine learning task of learning a function that maps an input to an output
%based on examples of ``input - output'' pairs called a ``training set''.
%\ed
%
%\fig{supervisedmodel}{0.6}
%
%Some more specific notation that we will be using throughout supervised learning:
%\bit
%\item Input variables, or attributes, or features: $x$.
%\item The $i^{th}$ feature: $x_{i}$.
%\item The $j^{th}$ training example: $x^{(j)}$.
%\item The $i^{th}$ feature of the $j^{th}$ training example: $x_{i}^{(j)}$.
%\item Output variables, or targets, or classes, or labels: $y$.
%\item The $i^{th}$ target: $y^{(i)}$.
%\item Total number of training examples: $m$.
%\item Total number of features: $n$.
%\eit
%
%Now let's dive in the models and techniques of supervised learning, starting with one of the most basic ones called
%``linear regression''.
%
%\section{Linear Regression}
%
%\bd[Linear Regression]
%\textbf{Linear regression} is a linear approach to modelling the relationship between a dependent variable (target)
%and one or more independent variables (features).
%\ed
%
%\bd[Simple / Multiple Linear Regression]
%When there is only one independent variable (feature) then the model is called \textbf{simple linear regression}. For
%more than one independent variables (features) the process is called \textbf{multiple linear regression}.
%\ed
%
%\bd[Univariate / Multivariate Linear Regression]
%When only one dependent variable (target) is predicted then the model is called \textbf{univariate linear regression}.
%For more than one correlated, dependent variables (targets) being predicted, the process is called
%\textbf{multivariate linear regression}. \ed
%
%In linear regression the hypothesis function $h$ is a linear combinations of the features:
%\bse
%h(x) = w_0 + w_{1} x_1 + \ldots + w_n x_n
%\ese
%
%where $w_0$ is called ``bias'' or ``intercept'' and the rest $w_i$'s are called ``weights''. We usually refer
%to weights and bias as the ``parameters'' of the regression (or the model) and they are the ones that we try to
%determine through the training examples by using a learning algorithm. Once we find them then $h$ is ready to predict
%new inputs with unknown outcomes.
%
%\fig{linearregression}{0.4}
%
%\v
%
%It is a usual procedure to define $x_0 = 1$, so the linear regression hypothesis function can be rewritten as:
%\bse
%h(x) = w_0 x_0 + w_1 x_1 + \ldots + w_n x_n = \sum_{i=0}^{n} w_i x_i
%\ese
%
%Moreover, by defining the feature vector $\boldsymbol{x}$ and parameter vector $\boldsymbol{w}$ as:
%\bse
%\boldsymbol{x} = \begin{bmatrix} x_{0} \\ x_{1} \\ \vdots \\ x_{n} \end{bmatrix}, \qquad
%\boldsymbol{w} = \begin{bmatrix} w_{0} \\ w_{1} \\ \vdots \\ w_{n} \end{bmatrix}
%\ese
%
%we can rewrite the linear regression the hypothesis function in the very simple form of:
%\bse
%h(x) = \boldsymbol{w}^{\intercal} \boldsymbol{x}
%\ese
%
%Notice that linear regression does not ``allow'' any polynomial terms of second or higher degree, thus the naming.
%However, what if our data is more complex than a straight line? Surprisingly, we can use a linear model to fit
%nonlinear data. The way to do this is to add powers of each feature as new features, and then move on with linear
%regression. This technique is called ``polynomial regression''. \v
%
%Now that we have a hypothesis function, we need a rule in order to be able to find the parameters $\boldsymbol{w}$.
%This rule can be obtained through the probabilistic interpretation of linear regression. \v
%
%More precisely, after having obtained the parameters $\boldsymbol{w}$, the hypothesis will fit the data in the best
%possible way but, since as we said we are dealing with probabilistic systems, we will still have some errors
%$\epsilon$. In other words, for each training example the following formula will apply:
%
%\bse
%y^{(i)} = h(x^{(i)}) + \epsilon^{(i)} = \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + \epsilon^{(i)}
%\ese
%
%where $\boldsymbol{x}^{(i)}$ is the corresponding training example feature vector:
%\bse
%\boldsymbol{x}^{(i)} = \begin{bmatrix} x_{0}^{(i)} \\ x_{1}^{(i)} \\ \vdots \\ x_{n}^{(i)} \end{bmatrix}
%\ese
%
%At this point we will make one assumption which needs to be valid in order for the linear regression to be valid.
%Namely, we assume that \textbf{the errors $ \epsilon^{(i)}$ are independent and identically distributed following a
%normal distribution with mean 0 and variance $\sigma^2$}:
%\bse
%\epsilon^{(i)} \sim N(0, \sigma^2)
%\ese
%
%which means that the probability distribution of the errors is given by:
%\bse
%P( \epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi \sigma^2}} exp \Big( - \frac{(\epsilon^{(i)})^{2}}{2 \sigma^2} \Big)
%\ese
%
%From the assumption of the errors follows:
%\begin{align*}
%&y^{(i)} = \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + \epsilon^{(i)} \Rightarrow \\ &\epsilon^{(i)} =
%y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}
%\end{align*}
%
%By substituting the error back to the probability:
%{\setlength{\jot}{10pt}
%\begin{align*}
%&P( \epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi \sigma^2}}
%\cdot exp \Big( - \frac{(\epsilon^{(i)})^2}{2 \sigma^2} \Big) \Rightarrow \\
%& P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w}) = \frac{1}{\sqrt{2 \pi \sigma^2}}
%\cdot exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big)
%\end{align*}}
%
%In other words we get that the conditional distribution of $y^{(i)}$ given $\boldsymbol{x}^{(i)}$ and
%$\boldsymbol{w}$ is a normal distribution with mean $\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}$ and variance
%$\sigma^2$:
%\bse
%y^{(i)} \sim N(\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}, \sigma^2)
%\ese
%
%\v
%
%Given the probability distribution of $y^{(i)}$ as a function of the parameters, we can now use the principle of
%maximum likelihood that we developed in parametric inference chapter, in order to find the rule that will give us the
%best parameters $\boldsymbol{w}$. \v
%
%For the likelihood we get:
%\bse
%\mathcal{L} (\boldsymbol{w} |y ) = P(y^{(1)}, y^{(2)}, \ldots, y^{(m)} | \boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)},
%\ldots, \boldsymbol{x}^{(m)} ; \boldsymbol{w}) = \prod_{i=1}^{m} P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w})
%\ese
%
%where we used the fact that $\epsilon^{(i)}$ are independent. \v
%
%By substituting the probability:
%\bse
%\mathcal{L} (\boldsymbol{w} |y ) = \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot exp \Big( - \frac{(y^{(i)} -
%\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big)
%\ese
%
%Subsequently, for the log-likelihood:
%{\setlength{\jot}{10pt}
%\begin{align*}
%l (\boldsymbol{w} |y ) &= \ln \mathcal{L} (\boldsymbol{w} |y ) \\
%&= \ln \Big[ \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot
%exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big) \Big] \\
%&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot
%exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big) \Big] \\
%&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \Big] + \sum_{i=1}^{m}
%\ln \Big[ exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big] \Big) \\
%&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \Big] +
%\sum_{i=1}^{m} \Big[ - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big]
%\end{align*}}
%
%According to the principle of maximum likelihood, the best parameters can be found by maximizing the log-likelihood.
%The first term of the log-likelihood is just a constant term, so it does not contribute at all to the maximization,
%and the same holds for the denominator of the second term. Hence:
%{\setlength{\jot}{10pt}
%\begin{align*}
%\boldsymbol{w} &= \argmax_{\boldsymbol{w}} [ l (\boldsymbol{w} |y )] \\
%&= \argmax_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} ( - (y^{(i)}
%- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}) ^2) \Big] \\
%&= \argmax_{\boldsymbol{w}} \Big[ - \sum_{i=1}^{m} (y^{(i)}
%- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2) \Big]\\
%&= \argmin_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} (y^{(i)}
%- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2 \Big]
%\end{align*}}
%
%At this point we can formally define the following function.
%
%\bd [Mean Squared Error Loss Function]
%\textbf{Mean squared error loss function} (MSE) $J(\boldsymbol{w})$ is defined as:
%\bse
%J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2
%\ese
%\ed
%
%\v
%
%Hence, the principle of maximum likelihood translates to finding the parameters $\boldsymbol{w}$ that minimize the
%MSE loss function. The intuition behind the minimization of the MSE loss function is straight forward since what we
%are actually doing is minimizing the square of the errors between the prediction and the actual outcome (square
%because only the magnitude of the error is important and not the sign). By minimizing as much as possible the errors
%we will eventually get the best line that fits the data. \v
%
%We can group together all training examples in one matrix and all training labels in one vector as: \v
%\bse
%X = \begin{bmatrix}
%\left(\boldsymbol{x}^{(1)}\right)^{\intercal} \\
%\left(\boldsymbol{x}^{(2)}\right)^{\intercal} \\
%\vdots \\
%\left(\boldsymbol{x}^{(m)}\right)^{\intercal}
%\end{bmatrix} =
%\begin{bmatrix}
%x_{0}^{(1)} & x_{1}^{(1)} & \ldots & x_{n}^{(1)} \\
%x_{0}^{(2)} & x_{1}^{(2)} & \ldots & x_{n}^{(2)} \\
%\vdots & \vdots & \ddots & \ldots \\
%x_{0}^{(m)} & x_{1}^{(m)} & \ldots & x_{n}^{(m)}
%\end{bmatrix}, \qquad
%\boldsymbol{y} = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix}
%\ese
%
%\v
%
%By doing so then we can write the MSE loss function in the simple form of:
%\bse
%J(\boldsymbol{w}) = \frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^2 =
%\frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^{\intercal} (X \boldsymbol{w} - \boldsymbol{y})
%\ese
%
%Beside the MSE loss function that is derived directly through the principal of maximum likelihood, in machine
%learning is quite common to use some variations of MSE depending on the problem. Here we will introduce the most
%basic of them.
%
%\bd [Root Mean Squared Error Loss Function]
%\textbf{Root mean squared error loss function} (RMSE) $J(\boldsymbol{w})$ is defined as:
%\bse
%J(\boldsymbol{w}) = \sqrt{MSE} = \sqrt{\frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} -
%\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}
%\ese
%\ed
%
%RMSE is a frequently used measure of the differences between values predicted by a model and the values observed, and
%it is probably the most easily interpreted statistic, since it has the same units as the data, so it is a better
%measure of goodness of fit than a correlation coefficient. RMSE is simply the square root of the average of squared
%errors, or in other words it is the average distance of a data point from the fitted line, measured along a vertical
%line. The effect of each error on RMSE is proportional to the size of the squared error, thus larger errors have a
%disproportionately large effect on RMSE. Consequently, RMSE is sensitive to outliers. \v
%
%RMSE is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the
%data. In general, a lower RMSE is better than a higher one. However, comparisons across different types of data would
%be invalid because the measure is dependent on the scale of the numbers used.
%
%\bd [Mean Bias Error]
%\textbf{Mean bias error loss function} (MBE) $J(\boldsymbol{w})$ is defined as:
%\bse
%J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})
%\ese
%\ed
%
%MBE captures the average bias in the prediction and is usually not used as a measure of the model error as high
%individual errors in prediction can also produce a low MBE. MBE is primarily used to estimate the average bias in the
%model and to decide if any steps need to be taken to correct the model bias. MBE can convey useful information, but
%should be interpreted cautiously because positive and negative errors will cancel out.
%
%\bd [Mean Absolute Error]
%\textbf{Mean absolute error loss function} (MAE) $J(\boldsymbol{w})$ is defined as:
%\bse
%J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} | y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} |
%\ese
%\ed
%
%MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It's
%the average over the test sample of the absolute differences between prediction and actual observation where all
%individual differences have equal weight. If the absolute value is not taken (the signs of the errors are not
%removed), the average error becomes the MBE\@.
%
%\bd [Mean Absolute Percentage Error]
%\textbf{Mean absolute percentage error loss function} (MAPE) $J(\boldsymbol{w})$ is defined as:
%\bse
%J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} \Big{|} \frac{y^{(i)} -
%\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}}{y^{(i)}} \Big{|}
%\ese
%\ed
%
%MAPE (also known as mean absolute percentage deviation (MAPD)) is the mean of the absolute percentage errors of
%forecasts. Error is defined as actual or observed value minus the forecasted value. Percentage errors are summed
%without regard to sign to compute MAPE. This measure is easy to understand because it provides the error in terms of
%percentages. Also, because absolute percentage errors are used, the problem of positive and negative errors canceling
%each other out is avoided. Consequently, MAPE has managerial appeal and is a measure commonly used in forecasting.
%The smaller the MAPE the better the forecast.
%
%\section{Optimization Techniques}
%
%Given the MSE loss function (or any other loss function), the goal of machine learning is to optimize it (usually
%minimize it) in order to obtain the best parameters that fit the data. Optimizing loss functions is one of the
%biggest parts of machine learning, and we can do so with the so called ``optimization techniques''.
%
%\bd [Optimization Techniques]
%\textbf{Optimization techniques} are techniques used for finding the optimum solution or unconstrained maxima or
%minima of continuous and differentiable functions. These are analytical methods and make use of differential calculus
%in locating the optimal solution.
%\ed
%
%\subsection{Normal Equation}
%
%Probably the most straight forward optimization technique is the so called ``normal equation''. Since we are
%looking a minimum for $J(\boldsymbol{w})$ the natural thing to do, is to simply calculate the derivative with respect
%to the parameter vector and then set it to zero (as we did when we introduced the principle of maximum likelihood). \v
%
%It is more handy to use the vector form of MSE loss function, so for the derivative we get:
%{\setlength{\jot}{10pt}
%\begin{align*}
%\nabla_{\boldsymbol{w}} J(\boldsymbol{w})
%&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w} -
%\boldsymbol{y})^{\intercal} (X \boldsymbol{w} - \boldsymbol{y}) \Big] \\
%&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ \Big( (X \boldsymbol{w})^{\intercal} -
%\boldsymbol{y}^{\intercal} \Big) \Big( X \boldsymbol{w} - \boldsymbol{y} \Big) \Big] \\
%&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w})^{\intercal} (X \boldsymbol{w}) -
%(X\boldsymbol{w})^{\intercal} \boldsymbol{y} - \boldsymbol{y}^{\intercal}(X \boldsymbol{w}) +
%boldsymbol{y}^{\intercal} \boldsymbol{y} \Big] \\
%&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w})^{\intercal} (X \boldsymbol{w}) -
%2 (X\boldsymbol{w})^{\intercal} \boldsymbol{y} + \boldsymbol{y}^{\intercal}\boldsymbol{y} \Big] \\
%&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ \boldsymbol{w}^{\intercal} X^{\intercal} X \boldsymbol{w} -
%2\boldsymbol{w}^{\intercal} X^{\intercal} \boldsymbol{y} +\boldsymbol{y}^{\intercal} \boldsymbol{y} \Big] \\
%&= \frac{1}{2m} \Big[ 2 X^{\intercal} X \boldsymbol{w} - 2 X^{\intercal}\boldsymbol{y} \Big] \\
%&= \frac{1}{m} \Big[ X^{\intercal} X \boldsymbol{w} - X^{\intercal}\boldsymbol{y} \Big]
%\end{align*}}
%
%By setting the derivative to 0 we obtain:
%{\setlength{\jot}{10pt}
%\begin{align*}
%& \nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = 0 \Rightarrow \\
%& \frac{1}{m} \Big[ X^{\intercal} X \boldsymbol{w} - X^{\intercal} \boldsymbol{y} \Big] = 0 \Rightarrow \\
%& X^{\intercal} X \boldsymbol{w} - X^{\intercal} \boldsymbol{y} = 0 \Rightarrow \\
%& X^{\intercal} X \boldsymbol{w} = X^{\intercal} \boldsymbol{y} \Rightarrow \\
%& \underbrace{(X^{\intercal} X)^{-1} (X^{\intercal} X)}_{I} \boldsymbol{w} =
%(X^{\intercal} X)^{-1} X^{\intercal} \boldsymbol{y} \Rightarrow \\
%& \boldsymbol{w} = (X^{\intercal} X)^{-1} X^{\intercal} \boldsymbol{y}
%\end{align*}}
%
%This final expression is called ``Normal Equation'', and it is an exact analytical solution that gives the
%parameter vector. \v
%
%Despite the fact that normal equation gives an exact analytical result, computing the seemingly harmless inverse of
%an $((n+1) \times m) \times (m \times (n+1) ) = (n+1) \times (n+1)$ matrix is, with today's most efficient computer
%science algorithm, of cubic time complexity ( in other words, if you double the number of features, you multiply the
%computation time by roughly $2^3$). This means that as the dimensions of $X$ increase (mainly the number of features),
%the amount of operations required to compute the final result increases in a cubic trend. If $X$ was rather small,
%then using the normal equation would be feasible. \v
%
%In practise, for the vast majority of any industrial application with large datasets, the normal equation would take
%extremely, sometimes nonsensically, long. This is the reason why normal equation is almost never used. Now let's move
%on the the most standard optimization technique used today called ``gradient descent''.
%
%\subsection{Gradient Descent}
%
%Gradient descent, and all its improvements and alternatives, is the most used optimization technique in machine
%learning and deep learning. It is a generic optimization algorithm capable of finding optimal solutions to a wide
%range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a
%cost function by starting with some random initial values for the parameters and calculating the value of the loss
%function based on them, and finally updating them based on the following relation:
%
%\bd[Gradient Descent Update Rule]
%\bse
%\boldsymbol{w} \coloneqq \boldsymbol{w} - \alpha \nabla_{\boldsymbol{w}} J(\boldsymbol{w})
%\ese
%\ed
%
%Since the derivative is positive when $J$ is upwards slopping and negative when it is downwards slopping, the minus
%sign makes sure that we always update the parameters towards the direction that minimizes $J$. Once the minimum is
%reached then $J$ is at a global optimum so the derivative is 0 and further updates are not possible. Gradient descent
%is over and the best parameters have been found.
%
%\vspace{-9pt}
%
%\fig{gradientdescent}{0.22}
%
%The hyperparameter $\alpha$ is called ``learning rate'' and defines how big or small steps we take after each
%iteration of gradient descent. If $\alpha$ is too large, we might fail to find the minimum due to oscillations around
%it. If $\alpha$ is too small then gradient descent might take too much time to reach the minimum of $J$. Tuning
%learning rate in a ``right'' value is a topic by itself, and it is quite heavily researched today.
%
%\bd[Learning Rate]
%The \textbf{learning rate} is a tuning parameter in an optimization algorithm that determines the step size at each
%iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired
%information overrides old information, it metaphorically represents the speed at which a machine learning model
%``learns''.
%\ed
%
%\vspace{-12pt}
%
%\fig{alpha}{0.7}
%
%Of course, not all cost functions look like nice, regular bowls. There may be holes, ridges, plateaus, and all sorts
%of irregular terrains, making convergence to the minimum difficult. In general gradient descent works best with
%convex functions, where if you pick any two points on the curve of a convex function, the line segment joining them
%never crosses the curve. This implies that there are no local minima, just one global minimum. They are also
%continuous functions with slopes that never changes abruptly. Hence, in convex functions gradient descent is
%guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not
%too high). \v
%
%Coming back to our case, let's find the update rule specifically for linear regression. Fortunately, the MSE cost
%function of linear regression happens to be a convex function. The only thing missing is the derivative of MSE loss
%function $J$. However, in the previous chapter with normal equation we showed that:
%\bse
%\nabla_{\boldsymbol{w}} J(\boldsymbol{w})
%= \frac{1}{m} \Big(X^{\intercal} X\boldsymbol{w} - X^{\intercal} \boldsymbol{y} \Big)
%= \frac{1}{m} X^{\intercal} \Big( X \boldsymbol{w}-\boldsymbol{y} \Big)
%\ese
%
%\v
%
%Hence, the update rule reads:
%\bse
%\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal} \Big( X \boldsymbol{w} -\boldsymbol{y} \Big)
%\ese
%
%\v
%
%As we already mentioned there are many improvements and modified algorithms based on gradient descent philosophy. We
%are going to cover a lot of them in these notes. For now, let's start with 3 basics versions of gradient descent.
%\bit
%\item \textbf{Batch Gradient Descent} Batch gradient descent is actually the one we just saw. As we see in gradient
%descent, the whole training set $X$ is used in order to make just one update of the parameters. We usually refer to
%the whole training set as the ``batch''. For that reason, the usual terminology for what we have seen so far is
%batch gradient descent, meaning that the whole batch is used in order to update the parameter. \v
%
%Notice that in batch gradient descent the algorithm goes over the entire data once before updating the parameters
%because this is the true gradient of the loss as derived earlier (sum of the gradients of the losses corresponding to
%each data point). Since there are no approximations each step guarantees that the loss will decrease. However, it
%carries a flipside. As the number of training examples grows the dimensions of $X$ grows and using the whole training
%set for every iteration becomes computationally expensive. For really large dataset with a million of points in the
%training data, in order to make just one update to the parameters the algorithm needs to make a million of calculations.
%Obviously this is very slow.
%\item \textbf{Mini-Batch Gradient Descent} In mini-batch gradient descent we divide the whole dataset to $b$ subsets
%of $\frac{m}{b}$ training examples each, called ``mini-batches'', and we update the parameters using each of the
%mini-batches in each iteration.
%
%\item \textbf{Stochastic Gradient Descent} In stochastic gradient descent, which can be seen as an extreme case of
%mini-batch gradient descent where $b=m$, we only use one training example per iteration to update the parameters. In
%every iteration we are estimating the total gradient based on just one single data point that we pick randomly hence
%the name ``stochastic''. One has to keep in mind that since this is an approximation there is no guarantee that each
%step will decrease the loss function. \v
%
%In fact, during stochastic gradient descent one observes many oscillations in the loss function due to the greediness
%of the decisions. Each point is trying to push the parameters in a direction most favourable to it (without being
%aware of how this affects other points). A parameter update which is locally favourable to one point may harm other
%points (its almost as if the data points are competing with each other). This fact provides no guarantee that each
%local greedy move reduces the global error. However, obviously, is much much faster that batch and mini-batch
%gradient descent. \v
%
%In a way, mini-batch gradient descent tries to strike a balance between the goodness of batch gradient descent and
%speed of stochastic gradient descent.
%\eit
%
%In general, in most supervised learning models batch gradient descent works just fine so we don't need the
%alternative techniques we just introduced. However, in more complicated models, such as deep learning models, these
%techniques can be really useful. For this reason we will meet again these techniques in the chapter of deep learning.
%
%\section{Feature Engineering}
%
%There are many possible features to use in your model. The process of choosing what information to use and how to
%extract this information into a format usable by your machine learning models is called ``feature engineering''.
%
%\bd[Feature Engineering]
%\textbf{Feature engineering} is the process of choosing what information (features) to use and how to extract this
%information into a format usable by a machine learning model.
%\ed
%
%For complex tasks the number of features used can go up to millions while, for domain-specific tasks you might need
%subject matter expertise to be able to come up with useful features. We usually refer to the set of all possible
%features as the ``feature space''.
%
%\bd[Feature Space]
%The \textbf{feature space} is the $N$ dimensional space defined by the set of $N$ possible features, not including
%the target.
%\ed
%
%In general, because of the importance and the ubiquity of feature engineering in machine learning projects, there
%have been many techniques developed to streamline the process. In this section we will introduce some of the most
%common techniques ones.
%
%\subsection{Feature Selection}
%
%\bd[Feature Selection]
%\textbf{Feature selection} is the process of reducing the size of feature space by selecting a subset of relevant
%features for use in model construction.
%\ed
%
%Feature selection techniques are used for several reasons like, simplifying models by reducing complexity to make
%them easier to interpret, avoiding the curse of dimensionality, and enhancing generalization by reducing overfitting.
%There are also production-related reasons like reducing resource requirements, speeding up predicting and training
%times, and minimizing both training and inference costs. \v
%
%There are some general guidelines that can be followed when selecting features, like relevance and independence.
%Starting with relevance, we want to remove features that don't influence the outcome. In general it's a good practice
%to explore and visualize the data in order to identify useless information. In this step we can drop the attributes
%that provide no useful information for the task, drop variables that have a very low variation (i.e.\ not too much
%information), and drop variables that have very low correlation with the target. \v
%
%Moving on to independence, we want to remove features that are highly correlated with each other. The reason is that
%highly correlated features provide redundant information and they can cause overfitting. In order to do so, we use
%a set of different methods called ``filter methods'', ``wrapper methods'', and ``embedded methods''.
%
%\bd[Filter Methods]
%\textbf{Filter methods} are feature selection methods that use statistical techniques to evaluate the relationship
%between each feature and the target variable. They are generally univariate and consider the feature independently,
%or with regard to the dependent variable.
%\ed
%
%The most common filter methods are:
%\bit
%\item \textbf{Pearson Correlation}: Used for linear correlation between the features.
%\item \textbf{Kendall Tau Rank Correlation}: Used for monotonic relationships and small sample sizes.
%\item \textbf{Spearman's Rank Correlation Coefficient}: Used for monotonic relationships.
%\item \textbf{Mutual Information}: Used for non-linear relationships.
%\item \textbf{F-Test}: Used for calculating the F-statistic between categorical features and the target.
%\item \textbf{Chi-Squared Test}: Used for calculating the chi-squared statistic between categorical features and the
%target.
%\eit
%
%\bd[Wrapper Methods]
%\textbf{Wrapper methods} are feature selection methods that use a machine learning algorithm to evaluate the
%performance of the model with a given subset of features.
%\ed
%
%Wrapper methods are generally computationally expensive and consider the selection of a set of features as a search
%problem. The most common wrapper methods are:
%\bit
%\item \textbf{Forward Selection}: Starts with an empty set of features and adds features one by one until the
%termination criterion is reached.
%\item \textbf{Backward Elimination}: Starts with the full set of features and removes features one by one until the
%termination criterion is reached.
%\item \textbf{Recursive Feature Elimination}: Starts with the full set of features and recursively removes features
%one by one until the termination criterion is reached.
%\eit
%
%\bd[Embedded Methods]
%\textbf{Embedded methods} are feature selection methods that use machine learning algorithms that have built-in
%feature selection methods.
%\ed
%
%Embedded methods are generally computationally expensive and consider the selection of a set of features as part of
%the learning process.
%
%\subsubsection{Missing Values}
%
%One of the first things you might notice when dealing with data in production is that some values are missing.
%However, not all types of missing values are equal. There are three types of missing values.
%
%\bd[Missing Not At Random (MNAR)]
%\textbf{Missing Not At Random} (\textbf{MNAR}) is when the reason a value is missing is because of the nature of the
%value itself.
%\ed
%
%\bd[Missing At Random (MAR)]
%\textbf{Missing At Random} (\textbf{MAR}) is when the reason a value is missing due to another observed variable.
%\ed
%
%\bd[Missing Completely At Random (MCAR)]
%\textbf{Missing Completely At Random} (\textbf{MCAR}) is when there's no pattern in when the value is missing.
%\ed
%
%No matter, which of the three types of missing values you have to deal with, when encountering missing values, you
%have to take some actions, otherwise the model will fail to be trained. The two thing that you can do is to either
%remove the missing values (deletion) or fill in the missing values with certain values (imputation).
%
%\bd[Deletion]
%\textbf{Deletion} is the technique of removing missing values completely.
%\ed
%
%Deletion works well when variables have a very high percentage of missing values. \v
%
%One way to delete is column deletion: if a variable has too many missing values, just remove that variable. The
%drawback of this approach is that you might remove important information and reduce the accuracy of your model. \v
%
%Another way to delete is row deletion: if a sample has missing value(s), just remove that sample. This method can
%work when the missing values are completely at random (MCAR) and the number of examples with missing values is small.
%However, removing rows of data can also remove important information that your model needs to make predictions,
%especially if the missing values are not at random (MNAR). On top of that, removing rows of data can create biases in
%your model, especially if the missing values are at random (MAR). \v
%
%Even though deletion is tempting because it's easy to do, deleting data can lead to losing important information and
%introduce biases into your model. If you don't want to delete missing values, you will have to impute them, which
%means ``fill them with certain values''.
%
%\bd[Imputation]
%\textbf{Imputation} is the technique of filling in missing values with certain values.
%\ed
%
%Deciding which ``certain values'' to use is the hard part. One common practice is to fill in missing values with
%their defaults. Another common practice is to fill in missing values with a statistic such as the mean, median, or
%mode. Both practices work well in many cases, but sometimes they can cause hair-pulling bugs. In general, you want to
%avoid filling missing values with possible values. \v
%
%Multiple techniques might be used at the same time or in sequence to handle missing values for a particular set of
%data. Regardless of what techniques you use, one thing is certain: there is no perfect way to handle missing values.
%With deletion, you risk losing important information or accentuating biases. With imputation, you risk injecting your
%own bias into and adding noise to your data, or worse, data leakage.
%
%\bd[Data Leakage]
%\textbf{Data leakage} refers to the phenomenon when a form of the label leaks into the set of features used for making
%predictions, and this same information is not available during inference.
%\ed
%
%Data leakage during imputation might occur if the mean or median is calculated using entire data instead of just the
%train split. This type of leakage can be prevented by using only statistics from the train split to fill in missing
%values in all the splits.
%
%\subsection{Feature Scaling}
%
%Machine learning algorithms don't perform well when the input numerical attributes have very different scales. Hence,
%before inputting features into models, it's important to scale them to be similar ranges. This process is called
%``feature scaling''.
%
%\bd[Feature Scaling]
%\textbf{Feature scaling} is the process of scaling the features to get all attributes to have the same scale.
%\ed
%
%This is one of the simplest things you can do that often results in a performance boost for your model. More
%specificaly some of the benefits of feature scaling is that it helps neural nets converge faster, avoids null errors
%during training and the model learns the right weights for each feature. On the other hand, neglecting to do so can
%cause your model to make gibberish predictions. \v
%
%There are two common ways to get all attributes to have the same scale: ``normalization'' and ``standardization''.
%
%\bd[Normalization]
%\textbf{Normalization} is the process where the values are shifted and rescaled so that they end up ranging from 0 to 1
%following the formula:
%\bse
%x^\prime = \frac{x - \min(x)}{\max(x) - \min(x)}
%\ese
%\ed
%
%Normalization is also called ``min-max scaling''. It is very sensitive to outliers in the data. If there are
%outliers in the data, they will be mapped to a very small or large value, and the rest of the data will be mapped to
%a very small interval.
%
%\fig{normalization}{0.5}
%
%\bd[Standardization]
%\textbf{Standardization} is the process where the values are shifted and rescaled so that they end up having zero mean
%and unit variance following the formula:
%\bse
%x^\prime = \frac{x - \bar{x}}{\sigma}
%\ese
%\ed
%
%\fig{standardization}{0.5}
%
%Unlike normalization, standardization does not bound values to a specific range, which may be a problem for some
%algorithms. However, standardization is much less affected by outliers. \v
%
%As it is obvious by now, scaling requires global statistics like min, max, mean, and variance of your data. One
%common mistake is to use the entire training data to generate global statistics before splitting it into different
%splits, leaking the mean and variance of the test samples into the training process, allowing a model to adjust its
%predictions for the test samples. This information isn't available in production, so the model's performance will
%likely degrade. This type of leakage is similar to the type of leakage caused by imputation, and in order to be
%avoided, always split your data first before scaling, then use the statistics from the train split to scale all the
%splits. Some even suggest that we split our data before any exploratory data analysis and data processing, so that we
%don't accidentally gain information about the test split. \v
%
%Last but not least, another technique usually used in feature scaling is the so called ``discretization''.
%
%\bd[Discretization]
%\textbf{Discretization} is the process of turning a continuous feature into a discrete feature.
%\ed
%
%\fig{cvsd.png}{0.5}
%
%With discretization instead of having to learn an infinite number of possible incomes, our model can focus on
%learning only a certain amount of categories (buckets), which is a much easier task to learn. This technique is
%supposed to be more helpful with limited training data.
%
%\subsection{Feature Crossing}
%
%\bd[Feature Crossing]
%\textbf{Feature crossing} is the technique to combine two or more features to generate new features.
%\ed
%
%Feature crossing technique is useful to model the nonlinear relationships between features hence, it's essential for
%models that can't learn or are bad at learning nonlinear relationships, such as linear regression (that we just saw),
%logistic regression (that we will see right after), and tree-based models (that we will see in the end of this chapter).
%Feature crossing is less important in neural networks (that we will see in next chapters) due to their natures of being
%able to learn nonlinear relationships. \v
%
%A caveat of feature crossing is that it can make your feature space blow up. Another caveat is that because feature
%crossing increases the number of features models use, it can make models overfit to the training data.
%
%\subsection{Feature Importance \& Generalization}
%
%Generally, adding more features leads to better model performance. Usually the list of features used for a model in
%production only grows over time. However, more features doesn't always mean better model performance. Having too many
%features can be bad both during training and serving your model for many reasons. \v
%
%First of all, the more features you have, the more opportunities there are for data leakage. Secondly, too many
%features can cause overfitting. From a technical perspective, too many features can increase memory required to serve
%a model, which, in turn, might require you to use a more expensive machine/instance to serve your model. Also, too
%many features can increase inference latency when doing predictions. Last but not least, useless features become
%technical debts. Whenever your data pipeline changes, all the affected features need to be adjusted accordingly. \v
%
%In general, there are two factors you might want to consider when evaluating whether a feature is good for a model:
%``feature importance'' and ``feature generalization''.
%
%\bd[Feature Importance]
%\textbf{Feature importance} is the measure of how much information a feature provides to a model.
%\ed
%
%\bd[Feature Generalization]
%\textbf{Feature generalization} is the measure of how well a model performs on data it hasn't seen before.
%\ed
%
%\section{Logistic Regression}
%
%\bd[Logistic Regression]
%\textbf{Logistic regression} (or classification) is a statistical model that is used for the classification of a
%discrete dependent variable (target) to a specific label (class) from a set of labels (classes).
%\ed
%
%\bd[Binary / Multinomial Classification]
%\textbf{Binary} classification is the problem of classifying instances into one of two classes. Classifying instances
%into three or more classes is called \textbf{multinomial} (or multiclass) classification.
%\ed
%
%\bd[Multilabel Classification]
%\textbf{Multilabel} classification is a variant of the classification problem where multiple labels may be assigned
%to each instance. Multilabel classification is a generalization of multinomial classification, since there is no
%constraint on how many of the classes the instance can be assigned to. \ed
%
%\v
%
%\fig{classification}{0.55}
%
%For now we will focus on binary classification, and later we will make a comment on multinomial classification. \v
%
%In binary classification, since the output is binary and can take only the values 0 or 1, the hypothesis function of
%the linear regression is not a valid approximator for logistic regression since it produces a continuous set of
%outputs. So our first step is to find a suitable hypothesis function $h$ for logistic regression. The hypothesis
%function that we actually use in logistic regression is the logistic function (part of a broader family called
%sigmoid functions that we will introduce in deep learning chapter) and it is given by:
%\bse
%h(\boldsymbol{x}) = \frac{1}{1 + exp(- \boldsymbol{w}^{\intercal} \boldsymbol{x})}
%\ese
%
%\v
%
%From now on, in order to save space, we will write $h(\boldsymbol{x})$ instead of the actual expression for the
%logistic function.
%
%\vspace{-5pt}
%
%\fig{sigmoid}{0.18}
%
%\v
%
%Hence, in logistic regression, the hypothesis function computes a weighted sum of the input features (plus a bias
%term), but instead of outputting the result directly like the linear regression hypothesis function does, it outputs
%the logistic of this result. Notice that $h(\boldsymbol{x}) < 0.5$ when $\boldsymbol{w}^{\intercal} \boldsymbol{x} <
%0$, and $h(\boldsymbol{x}) \geq 0.5$ when $\boldsymbol{w}^{\intercal} \boldsymbol{x} \geq 0$, so a logistic
%regression model predicts $1$ if $\boldsymbol{w}^{\intercal} \boldsymbol{x}$ is positive and 0 if it is negative.
%(The argument of the logistic function is often called ``logit''). \v
%
%It is worth mentioning that one can alter the discrimination threshold of 0.5 to any value between 0 and 1. This is
%quite usual in logistic regression models when obtaining the correct class is more important than a possible
%misclassification. \v
%
%Now let's try to give a meaning to the hypothesis function. As we can see the logistic function produces results in
%the interval $[0,1]$. It is quite close to what we need, but not exactly so, since we do not need all the values
%between 0 and 1. For this reason we will interpret the hypothesis function of logistic regression as a probability
%measure of the target to belong to class 1. The closest to 0 the hypothesis function, the more unlikely for the
%target to belong in class 1 (Hence, it belongs to class 0) and the closest to 1 the more likely to belong to the class
%1. Hence, by defining a threshold (say at 0.5) the idea is that for $h (\boldsymbol{x}) <0.5$ the algorithm will
%predict 0 and for $h(\boldsymbol{x}) \geq 0.5$ the algorithm will predict 1. Based on this intuition, we can write:
%\bse
%h(\boldsymbol{x}) = P(y=1 | \boldsymbol{x}; \boldsymbol{w})
%\ese
%
%Of course, since the output must be either 0 or 1 we get:
%\begin{align*}
%& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) + P(y=1 | \boldsymbol{x}; \boldsymbol{w}) = 1 \Rightarrow \\
%& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) = 1 - P(y=1 | \boldsymbol{x}; \boldsymbol{w}) \Rightarrow \\
%& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) = 1 - h(\boldsymbol{x})
%\end{align*}
%
%We can combine these two probabilities in one in the following way:
%\bse
%P(y | \boldsymbol{x}; \boldsymbol{w}) = h(\boldsymbol{x})^y \cdot (1 - h (\boldsymbol{x}))^{(1-y)}
%\ese
%
%So in logistic regression the output follows a Bernoulli distribution with parameter $h(\boldsymbol{x})$. Now that we
%have a probability distribution, similarly to the linear regression, we can use the principle of the maximum
%likelihood in order to obtain the best parameters that maximize the likelihood. Thus, we will obtain the loss
%function for the logistic regression case. \v
%
%By making again the assumption that we are dealing with independent and identically distributed random variables, for
%the likelihood is:
%\bse
%\mathcal{L} (\boldsymbol{w} |y ) = P(y^{(1)}, y^{(2)}, \ldots, y^{(m)} | \boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)},
%\ldots, \boldsymbol{x}^{(m)} ; \boldsymbol{w}) = \prod_{i=1}^{m} P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w})
%\ese
%
%By substituting the probability:
%\bse
%\mathcal{L} (\boldsymbol{w} |y )
%= \prod_{i=1}^{m} h(\boldsymbol{x}^{(i)})^{y^{(i)}} \cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})}
%\ese
%
%Subsequently for the log-likelihood:
%{\setlength{\jot}{10pt}
%\begin{align*}
%l (\boldsymbol{w} |y ) &= \ln \mathcal{L} (\boldsymbol{w} |y ) \\
%&= \ln \Big[ \prod_{i=1}^{m} h(\boldsymbol{x}^{(i)})^{y^{(i)}}
%\cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big] \\
%&= \sum_{i=1}^{m} \ln \Big[ h(\boldsymbol{x}^{(i)})^{y^{(i)}}
%\cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big] \\
%&= \sum_{i=1}^{m} \Big[ \ln \Big( h(\boldsymbol{x}^{(i)})^{y^{(i)}}\Big)
%+ \ln \Big( (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big) \Big] \\
%&= \sum_{i=1}^{m} \Big[ y^{(i)} \cdot \ln h(\boldsymbol{x}^{(i)})
%+ (1 - {y^{(i)}}) \cdot \ln (1 - h (\boldsymbol{x}^{(i)})) \Big]
%\end{align*}}
%
%Once again, according to the principle of maximum likelihood, the best parameters can be found by maximizing the
%log-likelihood. Hence:
%{\setlength{\jot}{10pt}
%\begin{align*}
%\boldsymbol{w} &= \argmax_{\boldsymbol{w}} [l (\boldsymbol{w} |y )] \\
%&= \argmax_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} \Big( (y^{(i)}
%\cdot \ln h(\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \Big] \\
%&= \argmin_{\boldsymbol{w}} \Big[ - \sum_{i=1}^{m} \Big( y^{(i)}
%\cdot \ln h(\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \Big]
%\end{align*}}
%
%where in the last step we did the usual trick of multiplying the whole expression with a minus sign and switching the
%optimization problem from maximizing the quantity to minimizing it. \v
%
%At this point we can formally define the following logistic regression loss function.
%
%\bd [Cross Entropy Loss Function]
%\textbf{Cross entropy loss function} (also called log loss) is defined as:
%\bse
%J(\boldsymbol{w}) = - \frac{1}{m} \sum_{i=1}^{m} \Big( y^{(i)} \cdot \ln h (\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}})
%\cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
%\ese
%\ed
%
%The cross entropy is the loss function of the logistic regression in the similar way where MSE loss function is the
%loss function for linear regression. The name ``cross entropy'' comes from the definition of cross entropy which is
%the average amount of information needed to identify an event between two probability distributions $p$ and $q$ over
%the same underlying set of events. \v
%
%Notice that the only possible values of $y^{(i)}$ is 0 or 1. This means that in any case, one of the terms $y^{(i)}$
%or $(1-y^{(i)})$ in $J$ will vanish and the other one will be equal to 1. So in the end the only thing that is
%actually part of the loss is the logarithm of the hypothesis function, which given that the hypothesis function is a
%logistic function which is always between 0 and 1, the logarithm is always negative. With the overall negative sign
%the loss turns positive and this is what we want to minimize. \v
%
%We are not going to write a vectorized form for the cross entropy loss function for two reasons. First of all, not
%all matrices have a logarithm and those matrices that do have a logarithm may have more than one logarithm. So one
%has to be careful when uses the vectorized form of logistic regression because it carries logarithms of matrices.
%Secondly, derivatives of logarithms of non square matrices sometimes are not defined. Since we need to calculate the
%derivative of $J$ we might get problems. For this reason we will use the non vectorized form for the calculations,
%however we will express the final result in a vectorized form. \v
%
%As in the linear case, the principle of maximum likelihood leads to the minimization of the cross entropy loss
%function in order to obtain the best parameters. We will examine the same techniques that we developed for the
%gradient descent in the linear case, i.e.\ normal equation and gradient descent.
%
%\subsection{Normal Equation}
%
%As we already mentioned, since we want to minimize a function the straight forward way of doing that is to calculate
%the derivative and then set it to 0. However, in the case of logistic regression the normal equation does not apply
%since there is no closed analytical solution of $\nabla_{\boldsymbol{w}} J (\boldsymbol{w})=0$. The only way for
%solving the optimization problem is through gradient descent.
%
%\subsection{Gradient Descent}
%
%Gradient descent works fine in logistic regression and, as with the MSE cost function in linear regression, it also
%happens to be a convex function. We will follow exactly the same steps as in linear regression:
%\begingroup
%\allowdisplaybreaks
%{\setlength{\jot}{10pt}
%\begin{align*}
%\nabla_{\boldsymbol{w}} J(\boldsymbol{w})
%&= - \frac{1}{m} \nabla_{\boldsymbol{w}} \Big[\sum_{i=1}^{m} \Big( y^{(i)} \cdot \ln h(\boldsymbol{x}^{(i)})
%+ (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)}))\Big) \Big] \\
%&= - \frac{1}{m} \sum_{i=1}^{m} \Big( y^{(i)} \cdot \nabla_{\boldsymbol{w}} \ln h(\boldsymbol{x}^{(i)})
%+ (1 - {y^{(i)}}) \cdot \nabla_{\boldsymbol{w}} \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \\
%&= - \frac{1}{m} \sum_{i=1}^{m} \Big( y^{(i)} \cdot \frac{\nabla_{\boldsymbol{w}}
%h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})} - (1 - {y^{(i)}}) \cdot \frac{\nabla_{\boldsymbol{w}}
%h(\boldsymbol{x}^{(i)})}{1 - h(\boldsymbol{x}^{(i)})} \Big)\\
%&= - \frac{1}{m} \sum_{i=1}^{m} \Big( \frac{y^{(i)}}{h(\boldsymbol{x}^{(i)})}
%- \frac{1 - {y^{(i)}}}{1 - h(\boldsymbol{x}^{(i)})} \Big) \cdot \nabla_{\boldsymbol{w}} h(\boldsymbol{x}^{(i)}) \\
%&= - \frac{1}{m} \sum_{i=1}^{m} \Big( \frac{y^{(i)} \cdot (1 - h(\boldsymbol{x}^{(i)}) - (1 - {y^{(i)}})
%\cdot h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)}) \cdot (1- h(\boldsymbol{x}^{(i)}))} \Big)
%\cdot \nabla_{\boldsymbol{w}} \Big[ \frac{1}{1 + exp(- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})} \Big] \\
%&= - \frac{1}{m} \sum_{i=1}^{m} \Big( \frac{y^{(i)} - y^{(i)} \cdot h(\boldsymbol{x}^{(i)})
%- h(\boldsymbol{x}^{(i)}) + {y^{(i)}} \cdot h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})
%\cdot (1 - h(\boldsymbol{x}^{(i)}))} \Big) \cdot \Big( \frac{(-1)
%\cdot exp(- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})
%\cdot (-\boldsymbol{x}^{(i)})}{(1 + exp(-\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}))^2} \Big) \\
%&= - \frac{1}{m} \sum_{i=1}^{m} \Big( \frac{y^{(i)} - h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})
%\cdot (1 - h(\boldsymbol{x}^{(i)}))} \Big) \cdot \Big( h(\boldsymbol{x}^{(i)})^2
%\cdot \frac{1 - h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})} \cdot \boldsymbol{x}^{(i)} \Big) \\
%&= \frac{1}{m} \sum_{i=1}^{m} \frac{h(\boldsymbol{x}^{(i)}) - y^{(i)})}{h(\boldsymbol{x}^{(i)})
%\cdot (1 - h (\boldsymbol{x}^{(i)}))} \cdot h(\boldsymbol{x}^{(i)})
%\cdot (1- h(\boldsymbol{x}^{(i)}))\cdot\boldsymbol{x}^{(i)} \\
%&= \frac{1}{m} \sum_{i=1}^{m} (h(\boldsymbol{x}^{(i)}) - y^{(i)}) \cdot \boldsymbol{x}^{(i)}
%\end{align*}}
%\endgroup
%
%Hence, the update rule reads:
%\bse
%\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} \sum_{i=1}^{m}
%\Big( \frac{1}{1 + exp(- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})} - y^{(i)} \Big) \cdot \boldsymbol{x}^{(i)}
%\ese
%
%\v
%
%Or in vectorized form:
%\bse
%\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal}
%\Big( \frac{1}{1+exp(-X \boldsymbol{w})}-\boldsymbol{y} \Big)
%\ese
%
%\v
%
%At this point, notice that gradient descent can be generalized into one model for both linear and logistic regression
%since the update rule for both of them can be be written in one coherent way as:
%\bse
%\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal} \Big( h(X)-\boldsymbol{y} \Big)
%\ese
%
%where one has to use either MSE loss function or cross entropy loss function depending on the regression problem.
%
%\subsection{Multinomial Classification}
%
%Multinomial (or multiclass) classification is the problem of classifying instances into one of three or more classes.
%(Multinomial classification should not be confused with multilabel classification, where multiple labels are to be
%predicted for each instance.) There are many techniques to deal with this problem which can be categorized into the
%following three categories:
%\bit
%\item \textbf{Transformation To Binary} Which can be sub-categorized into the following sub-categories:
%\bit
%\item \textbf{One VS Rest (OvR), or One VS All (OvA), or One Against All(OAA)} OvR involves training a single
%classifier per class, with the samples of that class as positive samples and all other samples as negatives. This
%strategy requires the base classifiers to produce a real-valued confidence score for its decision, rather than just a
%class label due to the fact that discrete class labels alone can lead to ambiguities, where multiple classes are
%predicted for a single sample. Although this strategy is popular, it is a heuristic that suffers from several problems.
%\item \textbf{One VS One (OvO)} In OvO one trains $\frac{K (K - 1)}{2}$ binary classifiers for a $K$-way multinomial
%problem, where each receives the samples of a pair of classes from the original training set, and must learn to
%distinguish these two classes. At prediction time, a voting scheme is applied: all $\frac{K (K - 1)}{2}$ classifiers
%are applied to an unseen sample and the class that got the highest number of ``$+1$'' predictions gets predicted by the
%combined classifier. Like OvR, OvO suffers from ambiguities in that some regions of its input space may receive the
%same number of votes.
%\eit
%\item \textbf{Extension From Binary} Extension from binary techniques extend the existing binary classifier to solve
%multinomial classification problems. Several algorithms have been developed based on neural networks, decision trees,
%k-nearest neighbors, naive Bayes, support vector machines and extreme learning machines to address multi-class
%classification problems.
%\item \textbf{Hierarchical Classification} Hierarchical classification tackles the multinomial classification problem
%by dividing the output space into a tree. Each parent node is divided into multiple child nodes and the process is
%continued until each child node represents only one class. Several methods have been proposed based on hierarchical
%classification.
%\eit
%
%As a final comment, it is worth mentioning that logistic regression itself can be generalized to support multiple
%classes directly, without having to train and combine multiple binary classifiers. The final result is called
%``softmax regression''. However, since softmax regression is used heavily in deep learning we will develop its
%corresponding theory in the deep learning chapter.
%
%\section{Generalized Linear Model}
%
%As we showed, in linear regression the target follows a normal distribution while in logistic regression the target
%follows a Bernoulli distribution. We can generalize both regressions in one coherent model called ``generalized
%linear model'' in which the target is allowed to follow a broad family of probability distributions.
%
%\bd[Generalized Linear Model]
%\textbf{Generalized linear model} (GLM) is a model that allows the dependent variable to follow an exponential family
%of probability distributions of the form:
%\bse
%P(y | \eta) = b(y) \cdot exp(\eta^{\intercal} T(y) - a(\eta))
%\ese
%\ed
%
%By picking specific values for $b$, $\eta$, $T$ and $a$ we end up with different distributions (including linear and
%logistic regression). Then we simply assume independence and apply the principle of maximum likelihood to obtain a
%loss function, in order to minimize it and find the best parameters.
%
%\section{Errors}
%
%Since $h$ is an estimator of $f$, the theory we developed in the chapter of parametric inference for estimators also
%holds for $h$. In other words, for the hypothesis function, which acts as an estimator for $f$, we can define
%quantities such as MSE (not the loss function but they coincide), sampling deviation, bias and variance. We can then
%use these quantities in order to evaluate how well a machine learning model performs. Let us see now the definitions
%of these quantities adjusted for the case of machine learning where the estimator is $h$.
%
%\subsection{Point-Wise, Overall, In-Sample \& Out-Of-Sample Error}
%
%Starting from the corresponding MSE (again not the loss function), in machine learning case we define the following
%quantities:
%
%\bd[Point-Wise Error]
%We define the \textbf{point-wise error} $e$ as a function of the real target function $f$ and the hypothesis function
%$h$ at point $x$:
%\bse
%e = e( f(x), h(x) )
%\ese
%\ed
%
%For example, for linear regression we could use $e( f(x), h(x) ) = (f(x) - h (x))^2$ while for logistic regression $e
%( f(x), h(x) ) = [f(x) \neq h(x)]$. Given point-wise error we can generalize to overall error.
%
%\bd[Overall Error]
%We define the \textbf{overall error} $E$ as the average over all point-wise errors $e( f(x), h(x) )$ at every point $x$.
%\ed
%
%We distinguish between two kind of overall errors: the in-sample error and the out-of-sample error.
%
%\bd[In-Sample Error]
%We define the \textbf{in-sample error} $E_{in}$ as the average of point-wise errors of the dataset that the model was
%trained:
%\bse
%E_{in} = \frac{1}{m} \sum_{i=1}^m e( f(x^{(i)}), h(x^{(i)}) )
%\ese
%\ed
%
%In other words in-sample error shows how well the model performs on the data used to build it.
%
%\bd[Out-Of-Sample Error]
%We define the \textbf{out-of-sample error} $E_{out}$ as a the expected value of point-wise errors of new data:
%\bse
%E_{out} = E_{x}[e( f(x), h(x) )]
%\ese
%\ed
%
%In other word out-of-sample error show how well the model generalizes to predictions for data it has not seen before.
%It makes sense that in order for $h$ to work well out of sample, so it can predict, it must be $E_{out} \approx 0$.
%
%\subsection{Bias \& Variance}
%
%Back in parametric inference chapter, we introduced the bias $B$ of an estimator $\hat{\theta}$, as the difference
%between the expected value of the estimator and the actual true parameter we want to estimate, $B =
%E[\hat{\theta}_{n}] - \theta$. Coming to our case where our estimator is $\hat{\theta} = h(x)$ and the true parameter
%is the target function $\theta = f(x)$, for the bias we get:
%
%\bd[Bias]
%We define the \textbf{bias} $B$ of the hypothesis function $h$ as the quantity:
%\bse
%B = E[h(x)] - f(x)
%\ese
%\ed
%
%With the expected value we assume that we train the model many times with different data each time, and we take the
%expected value of the functions that the model spits each time. The bias error is an error from erroneous assumptions
%in the learning algorithm. When we are dealing with high bias, formally we can say that the hypothesis set $H=\{h\}$
%was not big enough in order to contain function that can approximate well the target function $f$. So our best
%approximation for $f$ is still a bad one that cannot fit the data well.
%
%\vspace{-5pt}
%
%\fig{bias}{0.4}
%
%\vspace{-5pt}
%
%High bias can cause an algorithm to miss the relevant relations between features and target outputs and fail to
%capture the underlying structure of the dataset. We call this a case of \textbf{underfitting}, since the model fails
%to fit the given dataset well.
%
%\vspace{-10pt}
%
%\fig{underfitting}{0.8}
%
%\vspace{-10pt}
%
%Underfitting is one of the two main problems that a machine learning model can have and it leads to a high in-sample
%error $E_{in}$ which subsequently leads to a high out-of-sample error $E_{out}$. Hence, even that the problem is
%coming from the in-sample error it leads to not being able to generalize for out-of-sample data. \v
%
%In parametric inference chapter, we also defined the variance of an estimator $\hat{\theta}_{n}$ as the expected
%value of the square difference of the estimator from the expected value of the estimator: $Var = E[(\hat{\theta}_{n}
%- E[\hat{\theta}_{n}])^2]$. Coming back to machine learning for the variance we get:
%
%\bd[Variance]
%We define the \textbf{variance} $B$ of the hypothesis function $h$ as the quantity:
%\bse
%Var = E_{x}[(h(x) - E_{x}[h(x)])^2]
%\ese
%\ed
%
%Again, with the variance we assume that we train the model many times with different data each time, and we take the
%variance (i.e.\ the spread) of the functions that the model spits each time. The variance is an error from
%sensitivity to small fluctuations in the training set.
%
%\vspace{-5pt}
%
%\fig{variance}{0.45}
%
%\vspace{-5pt}
%
%When we are dealing with high variance, informally we can say that the hypothesis set $H=\{h\}$ is very big so in
%order to compensate the spread of dataset the model finds a function that fits the particular data very well but
%fails to generalize to new data. We call this a case of \textbf{overfitting}, since the model fails to generalize to
%new data.
%
%\vspace{-10pt}
%
%\fig{overfitting}{0.83}
%
%\vspace{-10pt}
%
%Overfitting leads to a very low in-sample $E_{in} \approx 0$, since it does a very good job on fitting the data.
%However, it fails to generalize, hence, to predict new data, which leads to a very high out-of-sample error $E_{out}$. \v
%
%Back in parametric inference we also showed that the mean squared error can be decomposed to bias and variance, and
%of course the same holds in our case since for the out of sample error of linear regression we can show:
%\begingroup
%\allowdisplaybreaks
%{\setlength{\jot}{5pt}
%\begin{align*}
%E_{out} &= E_{x} \Big[ e(f(x),h(x)) \Big] \\
%&= E_{x} \Big[ \Big( f(x) - h(x) \Big)^2 \Big] \\
%&= E_{x} \Big [ \Big( f(x) - h(x) + E_{x}[h(x)] - E_{x}[h(x)] \Big) ^2 \Big] \\
%&= E_{x} \Big[ \Big( \Big( f(x) - E_{x}[h(x)] \Big) + \Big( E_{x}[h(x)] - h(x) \Big) \Big)^2 \Big] \\
%&= E_{x} \Big[ \Big( f(x) - E_{x}[h(x)] \Big)^{2} + 2 \Big( f(x) - E_{x}[h(x)] \Big)
%\Big( E_{x}[h(x)] - h(x) \Big) + \Big( E_{x}[h(x)] - h(x) \Big)^2 \Big] \\
%&= E_{x} \Big[ \Big( f(x) - E_{x}[h(x)] \Big)^{2} \Big] + E_{x} \Big[ 2 \Big( f(x) - E_{x}[h(x)] \Big)
%\Big(E_{x}[h(x)] - h(x) \Big) \Big] + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
%&= \Big( f(x) - E_{x}[h(x)] \Big)^{2} + 2 \Big( f(x) - E_{x}[h(x)] \Big)
%\Big( E_{x} \Big[ E_{x}[h(x)] - h(x) \Big] \Big) + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2}\Big] \\
%&= \Big( f(x) - E_{x}[h(x)] \Big)^{2} + 2 \Big( f(x) - E_{x}[h(x)] \Big) \Big( E_{x}[h(x)] - E_{x}[h(x)] \Big)
%+ E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
%&= \Big( f(x) - E_{x}[h(x)] \Big)^{2} + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
%&= B^2 + Var
%\end{align*}}
%\endgroup
%
%Hence, the out-of-sample error is actually a combination of bias and variance. So in order to have a model that
%generalizes well, we have to keep both of them low. However, since they are of opposite nature, the more we reduce
%bias the more variance increases and vice versa. This is the so called \textbf{bias-variance trade off}. The goal in
%machine learning is to balance this trade off so the model fits the data well and doesn't fail to generalize.
%
%\fig{overunderfitting}{0.55}
%
%Let's sum up. In the graph above we see that in the case of high bias (underfitting) we have restricted our model to
%linear predictors, however the data do not follow a linear trend, hence, the hypothesis set is too small and the model
%cannot find a good curve to fit the data. On the other hand, in the case of high variance (overfitting) the
%hypothesis set is so big allowing complex predictors so the model managed to find a high degree polynomial that fit
%the data really good, however it will fail to generalize since it depends a lot on the initial values of the data and
%it is sensitive to fluctuations of them. Finally, in the last graph we have a good balance of bias and variance and
%the model found a good curve. One has to keep in mind that increasing a model's complexity will typically increase
%its variance and reduce its bias. Conversely, reducing a model's complexity increases its bias and reduces its
%variance. This is why it is called a ``trade-off''. \v
%
%It is worth mentioning that we have neglected the so-called ``irreducible error'' for the equations. This part is
%due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g
%fix the data sources, such as broken sensors, or detect and remove outliers).
%
%\section{Evaluation}
%
%Evaluation is about how good a model generalizes to new data, trying to answer the question ``how do we know that our
%machine learning model is any good?''. After applying the learning algorithm to the data and having obtained a
%hypothesis $h$, the machine learning model is ready to make new predictions. However, before that, we have to evaluate
%the model by analysing the errors that we introduced in the previous chapter.\footnote{Ideally, the evaluation
%methods should be the same during both development and production. But in many cases, the ideal is impossible because
%during development, you have ground truth labels, but in production, you don't.} \v
%
%The starting part is the in-sample and out-of-sample errors that we defined previously:
%\bse
%E_{in} = \frac{1}{m} \sum_{i=1}^m e( f(x^{(i)}), h(x^{(i)}) ) \qquad \text{and} \qquad E_{out} = E_{x}[e( f(x), h(x) )]
%\ese
%
%In general, for the error function $e$ we use the corresponding loss function $J$ that we used to train the model
%(although some times we can use variations of it), since it is a function of the target and hypothesis functions as
%$e$, and it is a really good measure of error:
%\bse
%E_{in} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )
%\qquad \text{and} \qquad E_{out} = E_{x}[J( f(x), h(x) )]
%\ese
%
%where here the notation $J^{{(i)}}$ means the error coming from the $i^{th}$ training example. Hence, now $E_{in}$ is
%calculated with the data that we trained the model, so it's a very good measure of how well the model performs in the
%data that it was trained on. The problem comes from $E_{out}$ since we don't know how to compute this expected value.
%Unsurprising we will perform the usual trick of substituting the expected value with the average so:
%\bse
%E_{in} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) ) \qquad \text{and} \qquad
%E_{out} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )
%\ese
%
%Of course, since we estimate the expected value with an average that brings an error to the estimation of the
%out-of-sample error. However, for our purposes we assume that this error is neglectful, and from now on we will treat
%the estimated out-of-sample error as the actual out-of-sample error. In general we have to keep in mind though that
%out-of-sample error carries an error. \v
%
%The question that arises is what data are we going to use for $E_{out}$. Using the same data that we trained the
%model is a really bad idea since, first of all, we will simply get $E_{out} = E_{in}$ and secondly the model already
%knows the correct answers of the data since we used them to train it, and the evaluation will be biased. \v
%
%In order to overcome this problem, we split the dataset (before training the model) into two parts: training set and
%test set. Then we use the first to train the model and obtain $E_{in}$ and the latter to evaluate its performance and
%obtain $E_{out}$. Since the model is trained with the train set, it has never seen the test set so the estimation of
%the out-of-sample error with the evaluation set will be unbiased. \v
%
%One of the things to consider is the proportions of splitting the dataset into training and test sets. This again is
%an area of heavy research, but in general in machine learning we usually split them in a proportion of ``80\% -
%20\%''. In other areas of machine learning like deep learning where we usually have a very large amount of data we use
%splitting rules of ``99\% - 1\%''. But we will address this issue in details in deep learning chapter. \v
%
%Hence, before training we split the dataset as:
%\bit
%\item Training Dataset: $\{x_{\text{train}}^{(i)}, y_{\text{train}}^{(i)}\}, \:\:\: i = 1,2,\ldots,m_{\text{train}}
%\qquad$ (usually about 80\% of initial dataset). \v
%\item Test Dataset: $\{x_{\text{test}}^{(i)}, y_{\text{test}}^{(i)}\}, \:\:\: i = 1,2,\ldots,m_{\text{test}} \qquad$
%(usually about 20\% of initial dataset). \v
%\eit
%
%And subsequently the errors become:
%\bse
%E_{in} = \frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} J^{{(i)}}(f(x^{(i)}), h(x^{(i)}) ) \qquad \text{and}
%\qquad E_{out} = \frac{1}{m_{\text{test}}} \sum_{i=1}^{m_{\text{test}}} J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )
%\ese
%
%\v
%
%From now on we will be referring to the first expression as $J_{\text{train}} = E_{in}$ and to the second one as
%$J_{\text{test}} = E_{out}$. \v
%
%So for example for linear regression we would have:
%\bse
%J_{\text{train}} = \frac{1}{2m_{train}} \sum_{i=1}^{m_{train}} (y^{(i)}
%- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2 \qquad \text{and} \qquad J_{\text{test}}
%= \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}} (y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2
%\ese
%
%\v
%
%While for logistic regression we would have:
%\bse
%J_{\text{train}} = - \frac{1}{m_{train}} \sum_{i=1}^{m_{train}} \Big( y^{(i)}
%\cdot \ln h(\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
%\ese
%
%\v
%
%and:
%\bse
%J_{\text{test}} = - \frac{1}{m_{test}} \sum_{i=1}^{m_{test}} \Big( y^{(i)}
%\cdot \ln h(\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
%\ese
%
%\v
%
%Coming to a more applied side of machine learning, it is worth mentioning that it is very important to not use the
%test dataset until the very end when the final model is ready. The test set is actually something we use in order to
%provide it as a performance measure together with the model and not something to use for gaining insights or
%improving the model. For the later, what people usually do is to further split the train set to a ``new'' (even
%smaller) train set, and a so called ``evaluation set'' (using again a proportion of ``80\% - 20\%'') and then the
%use the new smaller train set to train the model and the evaluation set in order to improve the model (e.g.\ tune the
%parameters, find best features, etc). There are many best practices on how to use the evaluation set (e.g.\ K-fold
%cross validation, grid search, etc) however these techniques are of an applied nature and they get a bit out of topic
%for these notes. Long story short, once you are confident about your final model, measure its performance on the test
%set to estimate the generalization error. Don't tweak your model after measuring the generalization error since you
%would just start overfitting the test set. \v
%
%Coming back to the theoretical aspect of machine learning, now that we have $J_{\text{train}} $ and $J_{\text{eval}}
%$ we know how well the models performs in and out of sample. However, we can also use them in order to find if the
%model suffers from underfitting or overfitting. There are two ways that we can do so. \v
%
%The first way, is by gradually increasing the complexity of the model (higher polynomial degrees so bigger hypothesis
%set), training the model for each complexity level and calculate both $J_{train}$ and $J_{eval}$ for each model. Then
%by plotting out the different values for different levels of complexity we usually end up with the following graph:
%
%\vspace{-10pt}
%
%\fig{eval}{0.4}
%
%\vspace{-10pt}
%
%In the high bias area both $J_{train}$ and $J_{eval}$ are high. This means that the model does not fit the training
%data well hence, it fails to generalize. This is the case of underfitting. In the high variance area, $J_{train}$ is
%low but $J_{eval}$ is high. This means that the model fits the training data well but fails to generalize to unseen
%data. This is the case of overfitting. Hence, by using the graph we can diagnose both cases!\v
%
%The second way to diagnose the problem of the model, is through the so called ``learning curves''.
%
%\bd[Learning Curve]
%A \textbf{learning curve} is a graphical representation of how an increase in learning comes from greater experience;
%or how the more someone performs a task, the better they get at it.
%\ed
%
%Informally, a learning curve is the relation between error (as expressed in loss function) and training examples $m$.
%By plotting this relation for both $J_{train}$ and $J_{eval}$ we end up with two learning curves and by their
%relative position we can diagnose if our model suffers from high bias or high variance. \v
%
%More specifically, when the learning curves of $J_{train}$ and $J_{eval}$ do not have a large gap between them as $m$
%increases we are usually dealing we a case of high bias and underfitting.
%
%\vspace{10pt}
%
%\fig{lchighbias}{0.6}
%
%\vspace{10pt}
%
%On the other hand when there is a large gap between the two curves we are dealing with the case of high variance and
%overfitting.
%
%\vspace{10pt}
%
%\fig{lchighvariance}{0.5}
%
%\vspace{10pt}
%
%Once we detect the problem then we have to make some changes in order to fix them! Here are some of the techniques
%that we follow:
%\bit
%\item \textbf{For high bias (underfitting)}:
%\bit
%\item Increase model complexity.
%\item Increase number of features.
%\eit
%\item \textbf{For high variance (overfitting)}:
%\bit
%\item Decrease model complexity.
%\item Decrease number of features.
%\item Find more training examples.
%\item Regularization (next section).
%\eit
%\eit
%
%\section{Regularization}
%
%As we saw in the previous section, when we allow a very broad hypothesis set with many higher order terms the model
%might find a hypothesis function $h$ that gives a 0 in-sample error but fails to generalize. This is due to high
%variance, i.e.\ large dependence on the very specific dataset used for training, and we call this a case of
%overfitting. A way to deal with overfitting is a collection of techniques that undergo with the name
%``regularization''.
%
%\bd[Regularization]
%\textbf{Regularization} is the process of adding information in order to solve an ill-posed problem and to prevent
%overfitting.
%\ed
%
%There are many different regularization techniques. We will begin with four main ones called ``Ridge regression''
%(or ``Tikhonov regression'' or ``L2 regularization''), ``Lasso regression'' (or ``L1 regularization''),
%``elastic net'', and ``early stopping''.
%
%\subsection{Ridge Regression / Tikhonov Regularization (L2 Regularization)}
%
%The reason of overfitting is that the parameters $\boldsymbol{w}$ are free to get any value. With regularization we
%penalize the parameters by imposing an extra constraint on $\boldsymbol{w}$ of the form:
%\bse
%\boldsymbol{w}^{\intercal} \boldsymbol{w} \leq C
%\ese
%
%where $C$ is a constant defined by us and it controls the effect of regularization. It is called ``L2
%regularization'' because the quantity $\boldsymbol{w}^{\intercal} \boldsymbol{w}$ is actually the squared L2 norm of
%the vector $\boldsymbol{w}$:
%\bse
%||\boldsymbol{w}||^2_2 = \boldsymbol{w}^{\intercal} \boldsymbol{w}
%\ese
%
%Hence, now the optimization problem becomes to minimize the loss function $J (\boldsymbol{w})$ subject to the above
%mentioned constraint. According to Appendix~\ref{ch:constrained-optimization} in order to do so we define the
%Lagrangian:
%\bse
%\mathcal{L} (\boldsymbol{w}) = J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{\intercal} \boldsymbol{w}
%\ese
%
%where $\lambda$ is the Lagrange multiplier, and then we solve the equation:
%\bse
%\nabla_{\boldsymbol{w}} \mathcal{L} (\boldsymbol{w}) = 0
%\ese
%
%For example, for linear regression where $J(\boldsymbol{w})$ is the MSE loss function, the Lagrangian reads:
%{\setlength{\jot}{10pt}
%\bse
%\mathcal{L} (\boldsymbol{w}) = J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{\intercal} \boldsymbol{w}
%= \frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^{\intercal} (X \boldsymbol{w} - \boldsymbol{y}) + \frac{\lambda}{2m}
%\boldsymbol{w}^{\intercal} \boldsymbol{w}
%\ese}
%
%At this point we can redefine this Lagrangian as a new loss function of the form:
%\bse
%J (\boldsymbol{w}) = \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^{\intercal}
%(X \boldsymbol{w} - \boldsymbol{y}) + \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big ]
%\ese
%
%and then the problem is to minimize this loss function which is actually a regression problem. The corresponding
%regression is called ``Ridge regression'' (or ``Tikhonov regularization''), where the only difference with linear
%regression is that we have to add the extra term in the loss function to reduce overfitting. \v
%
%The coefficient $\lambda$ is the one that controls the regularization effect on the regression. In one extreme where
%$\lambda=0$ the regularization term vanishes, and the loss function ends up to the mean squared error loss function,
%Hence, the ridge regression turns to linear regression. In the other extreme where $\lambda \to \infty$ then the
%regularization term penaltizes all parameters in an extreme way, so the ridge regression, in order to minimize the
%loss, is forced to set all the parameters to 0. In the end we end up with $\boldsymbol{w}^{\intercal} \boldsymbol{x}
%= 0$. For all intermediate values of $\lambda$ we get different levels of regularization. It is actually our job to
%tune the model to the right $\lambda$ that does the job. \v
%
%Now that we have a loss function, we treat the problem in the similar way as we did before. For example, in the
%linear case of ridge regression we can solve the normal equation in the same way we solved it before:
%{\setlength{\jot}{10pt}
%\begin{align*}
%J(\boldsymbol{w}) &= \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^{\intercal} (X \boldsymbol{w}
%- \boldsymbol{y}) + \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big]\\
%&= \frac{1}{2m} \Big[ \Big( (X \boldsymbol{w})^{\intercal} - \boldsymbol{y}^{\intercal} \Big)
%\Big(X\boldsymbol{w} - \boldsymbol{y} \Big) + \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big]\\
%&= \frac{1}{2m} \Big[ (X \boldsymbol{w})^{\intercal} (X \boldsymbol{w}) - (X \boldsymbol{w})^{\intercal}
%\boldsymbol{y} - \boldsymbol{y}^{\intercal} (X \boldsymbol{w}) + \boldsymbol{y}^{\intercal} \boldsymbol{y}
%+ \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big] \\
%&= \frac{1}{2m} \Big[ (X \boldsymbol{w})^{\intercal} (X \boldsymbol{w}) - 2 (X \boldsymbol{w})^{\intercal}
%\boldsymbol{y} + \boldsymbol{y}^{\intercal} \boldsymbol{y} + \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big] \\
%&= \frac{1}{2m} \Big[ \boldsymbol{w}^{\intercal} X^{\intercal} X \boldsymbol{w} - 2 \boldsymbol{w}^{\intercal}
%X^{\intercal} \boldsymbol{y} + \boldsymbol{y}^{\intercal} \boldsymbol{y}
%+ \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big]
%\end{align*}}
%
%By setting the derivative to 0 we obtain:
%{\setlength{\jot}{10pt}
%\begin{align*}
%& \nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = 0 \Rightarrow \\
%& \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ \boldsymbol{w}^{\intercal} X^{\intercal} X \boldsymbol{w} - 2
%\boldsymbol{w}^{\intercal} X^{\intercal} \boldsymbol{y} + \boldsymbol{y}^{\intercal} \boldsymbol{y} + \lambda
%\boldsymbol{w}^{\intercal} \boldsymbol{w} \Big] \\
%& \frac{1}{2m} \Big[ 2 X^{\intercal} X \boldsymbol{w} - 2 X^{\intercal} \boldsymbol{y}
%+ 2\lambda \boldsymbol{w} \Big] = 0 \Rightarrow \\
%& \frac{1}{m} \Big[ X^{\intercal} X \boldsymbol{w} - X^{\intercal} \boldsymbol{y}
%+ \lambda \boldsymbol{w} \Big] = 0 \Rightarrow \\
%& X^{\intercal} X \boldsymbol{w} - X^{\intercal} \boldsymbol{y} + \lambda \boldsymbol{w} = 0 \Rightarrow \\
%& (X^{\intercal} X + \lambda I) \boldsymbol{w} = X^{\intercal} \boldsymbol{y} \Rightarrow \\
%& \underbrace{(X^{\intercal} X + \lambda I)^{-1} (X^{\intercal} X + \lambda I)}_{I} \boldsymbol{w}
%= (X^{\intercal} X + \lambda I)^{-1} X^{\intercal} \boldsymbol{y} \Rightarrow \\
%& \boldsymbol{w} = (X^{\intercal} X + \lambda I)^{-1} X^{\intercal} \boldsymbol{y}
%\end{align*}}
%
%\vspace{-10pt}
%
%The only difference with the normal equation of linear regression is the extra term $\lambda I$ coming from
%regularization. \v
%
%Gradient descent also works for ridge regression. For the derivative of $J$:
%\bse
%\nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = \frac{1}{m} \Big(X^{\intercal} X \boldsymbol{w}
%- X^{\intercal} \boldsymbol{y} + \lambda \boldsymbol{w} \Big) = \frac{1}{m} X^{\intercal}
%\Big( (X + \lambda I) \boldsymbol{w} - \boldsymbol{y} \Big)
%\ese
%
%Hence, the update rule reads:
%\bse
%\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal}
%\Big( (X + \lambda I) \boldsymbol{w} - \boldsymbol{y} \Big)
%\ese
%
%\v
%
%Of course, L2 regularization can be applied also for the case of logistic regression. More specifically, for cross
%entropy loss function of logistic regression the Lagrangian reads:
%\bse
%\mathcal{L} (\boldsymbol{w}) = J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{\intercal} \boldsymbol{w}
%= - \frac{1}{m} \Big( \boldsymbol{y}^{\intercal} \cdot \ln h(X) + (I -\boldsymbol{y})^{\intercal}
%\cdot \ln (I - h(X)) \Big) + \frac{\lambda}{2m} \boldsymbol{w}^{\intercal} \boldsymbol{w}
%\ese
%
%Similarly to the linear case, we redefine this Lagrangian as a new loss function of the form:
%\bse
%J (\boldsymbol{w}) = - \frac{1}{m} \Big[ \boldsymbol{y}^{\intercal} \cdot \ln h(X) + (I - \boldsymbol{y})^{\intercal}
%\cdot \ln (I - h(X)) - \frac{\lambda}{2} \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big ]
%\ese
%
%As we said back in logistic regression, normal equation does not apply here since there is no closed analytical
%solution, however gradient descent still applies where the rule simply reads:
%\bse
%\boldsymbol{w} \coloneqq \Big( 1 - \frac{\alpha \lambda}{m} \Big) \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal}
%\Big( \frac{1}{1+exp(-X \boldsymbol{w})}-\boldsymbol{y} \Big)
%\ese
%
%In both cases solving ridge regression will give us as a result a solution that slightly underfits the data, compared
%to linear or logistic regression. This underfitting will produce higher bias hence, due to bias-variance trade off, a
%reduced variance which will lead to the reduction of overfitting.
%
%\subsection{Lasso Regression (L1 Regularization)}
%
%In ridge regression we used the L2 norm of the vector $\boldsymbol{w}$. Another way of regularization is to use L1
%norm which is:
%\bse
%|| \boldsymbol{w} ||_1 \leq C
%\ese
%
%By repeating the same way of analysis as in ridge regression, we can define the following loss function for linear
%regression:
%\bse
%J (\boldsymbol{w}) = \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^{\intercal}
%(X \boldsymbol{w} - \boldsymbol{y}) + \lambda ||\boldsymbol{w}||_1 \Big ]
%\ese
%
%and for logistic regression:
%\bse
%J (\boldsymbol{w}) = - \frac{1}{m} \Big[ \boldsymbol{y}^{\intercal} \cdot \ln h(X) +
%(I - \boldsymbol{y})^{\intercal} \cdot \ln (I - h(X)) - \frac{\lambda}{2} ||\boldsymbol{w}||_1\Big ]
%\ese
%
%The corresponding regression is called ``Least Absolute Shrinkage and Selection Operator Regression'', usually
%simply called ``Lasso regression''. An important characteristic of Lasso regression is that it tends to eliminate
%the weights of the least important features (i.e.\ set them to zero). In other words, Lasso regression automatically
%performs feature selection and outputs a sparse model (i.e.\ with few nonzero feature weights). \v
%
%As before, we can use normal equation and gradient descent to solve Lasso regression.
%
%\subsection{Elastic Net}
%
%Elastic net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple
%mix of both Ridge and Lasso's regularization terms, and you can control the mix ratio between them with a parameter
%$r$. \v
%
%The elastic net loss function for linear regression reads:
%\bse
%J (\boldsymbol{w}) = \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^{\intercal}
%(X \boldsymbol{w} - \boldsymbol{y}) + r \lambda ||\boldsymbol{w}||_1
%+ (1-r) \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big ]
%\ese
%
%and for logistic regression:
%\bse
%J (\boldsymbol{w}) = - \frac{1}{m} \Big[ \boldsymbol{y}^{\intercal} \cdot \ln h(X) + (I - \boldsymbol{y})^{\intercal}
%\cdot \ln (I - h(X)) - r \frac{\lambda}{2} || \boldsymbol{w}||_1 - (1-r) \frac{\lambda}{2} \boldsymbol{w}^{\intercal}
%\boldsymbol{w} \Big ]
%\ese
%
%When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression. \v
%
%Since elastic net summarizes both ridge and lasso regression, it is a good point to analyse, when one should use
%plain linear (or logistic) regression (without any regularization), ridge, lasso, or elastic net? It is almost always
%preferable to have at least a little bit of regularization, so generally plain linear regression should be avoided.
%Ridge is a good default, but if one suspects that only a few features are useful, they should prefer lasso or elastic
%net because they tend to reduce the useless features' weights down to zero, as we have discussed. In general, elastic
%net is preferred over lasso because lasso may behave erratically when the number of features is greater than the
%number of training instances or when several features are strongly correlated.
%
%\subsection{Early Stopping}
%
%A very different way to regularize iterative learning algorithms such as gradient descent is to stop training as soon
%as the validation error reaches a minimum. This is called ``early stopping''. In general as the algorithm learns,
%its prediction error on the training set goes down, along with its prediction error on the validation set. After a
%while though, the validation error stops decreasing and starts to go back up. This indicates that the model has
%started to overfit the training data. Early stopping just stops training as soon as the validation error reaches the
%minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a ``beautiful free
%lunch''.
%
%\section{Classification Error Metrics}
%
%In classification problems where both input and output can be either 0 or 1, we can follow a different approach of
%error evaluation based on exact matches and mismatches between prediction and actual result. The usual case, since we
%are dealing with a binary output, is to define either 0 or 1 as the positive class and the remaining as the negative
%one. Which one is which depends on the nature of the problem. For now we will stick with the case were 0 represents
%the negative class and 1 the positive one. \v
%
%Given that both the actual class and the predicted class can be either positive or negative we end up with 4
%different, distinct situations. Let us define them formally.
%
%\bd[True Positive]
%\textbf{True positive} (TP) also called \textbf{hit}, is the case where the model predicts a positive result when the
%actual outcome is indeed positive.
%\ed
%
%\bd[True Negative]
%\textbf{True negative} (TN) also called \textbf{correct rejection}, is the case where the model predicts a negative
%result when the actual outcome is indeed negative.
%\ed
%
%\bd[False Positive]
%\textbf{False positive} (FP) also called \textbf{false alarm} or \textbf{type I error}, is the case where the model
%predicts a positive result when the actual outcome is negative.
%\ed
%
%\bd[False Negative]
%\textbf{False negative} (FN) also called \textbf{miss} or \textbf{type II error}, is the case where the model
%predicts a negative result when the actual outcome is positive.
%\ed
%
%Once the model is trained, we test it on the evaluation set and we measure the number of occurrences of each category.
%Then we gather them all together to the so called ``confusion matrix''.
%
%\bd[Confusion Matrix]
%\textbf{Confusion matrix} is a table that reports the number of true positives TP, true negatives TN, false positives
%FP and false negatives FN of a model.
%\ed
%
%\vspace{-5pt}
%
%\fig{confusion}{0.35}
%
%Once we have constructed the confusion matrix we can define the following error metrics.
%
%\bd[Accuracy]
%\textbf{Accuracy} (ACC) is the rate that shows overall how often the model was correct:
%\bse
%ACC = \frac{TP + TN}{TP + TN + FP + FN}
%\ese
%\ed
%
%\v
%
%\bd[Error Rate]
%\textbf{Error rate} (ERR) also called \textbf{misclassification}, is the rate that shows overall how often the model
%was incorrect:
%\bse
%ERR = \frac{FP + FN}{TP + TN + FP + FN}
%\ese
%\ed
%
%\v
%
%It is of course: $ACC + ERR = 1$. \v
%
%\bd[True Positive Rate]
%\textbf{True positive rate} (TPR) also called \textbf{sensitivity} or
%\textbf{recall} or \textbf{hit rate}, is the rate that shows how often the model predicts positive when the actual
%outcome is indeed positive:
%\bse
%TPR = \frac{TP}{TP + FN}
%\ese
%\ed
%
%\v
%
%\bd[False Negative Rate]
%\textbf{False negative rate} (FNR) also called \textbf{miss rate}, is the rate that shows how often the model
%predicts negative when the actual outcome is positive:
%\bse
%FNR = \frac{FN}{TP + FN}
%\ese
%\ed
%
%\v
%
%It is of course: $TPR + FNR = 1$. \v
%
%\bd[True Negative Rate]
%\textbf{True negative rate} (TNR) also called \textbf{specificity} or \textbf{selectivity}, is the rate that shows how
%often the model predicts negative when the actual outcome is indeed negative:
%\bse
%TNR = \frac{TN}{TN + FP}
%\ese
%\ed
%
%\v
%
%\bd[False Positive Rate]
%\textbf{False positive rate} (FPR) also called \textbf{fall-out rate}, is the rate that shows how often the model
%predicts positive when the actual outcome is negative:
%\bse
%FPR = \frac{FP}{TN + FP}
%\ese
%\ed
%
%\v
%
%It is of course: $TNR + FPR = 1$. \v
%
%\bd[Positive Predicted Value]
%\textbf{Positive predicted value} (PPV) also called \textbf{precision}, is the rate that shows how often the model is
%correct when it predicts positive:
%\bse
%PPV = \frac{TP}{TP + FP}
%\ese
%\ed
%
%\v
%
%\bd[False Discovery Rate]
%\textbf{False discovery rate} (FDR) is the rate that shows how often the model is wrong when it predicts positive:
%\bse
%FDR = \frac{FP}{TP + FP}
%\ese
%\ed
%
%\v
%
%It is of course: $PPV + FDR = 1$. \v
%
%\bd[Negative Predicted Value]
%\textbf{Negative predicted value} (NPV) is the rate that shows how often the model is correct when it predicts negative:
%\bse
%NPV = \frac{TN}{TN + FN}
%\ese
%\ed
%
%\v
%
%\bd[False Omission Rate]
%\textbf{False omission rate} (FOR) is the rate that shows how often the model is wrong when it predicts negative:
%\bse
%FOR = \frac{FN}{TN + FN}
%\ese
%\ed
%
%\v
%
%It is of course: $NPV + FOR = 1$. \v
%
%\bd[$F_{\beta}$ Score]
%\textbf{$F_{\beta}$ score} (FOR) is defined as the harmonic mean of positive predicted value PPV (aka precision) and
%true positive rate (aka recall) each weighted based on value of $\beta$:
%\bse
%F_{\beta} = \frac{(1 + \beta^2) \cdot PPV \cdot TPR}{\beta^2 \cdot PPV + TPR}
%= \frac{(1 + \beta^2) \cdot \text{precission} \cdot \text{recall}}{\beta^2 \cdot \text{precission} + \text{recall}}
%= \frac{(1 + \beta^2) \cdot TP}{(1 + \beta^2) \cdot TP + \beta^2 \cdot FN + FP}
%\ese
%\ed
%
%\v
%
%The coefficient $\beta$ is chosen such that recall is considered $\beta$ times as important as precision. Two
%commonly used values for $\beta$ are 2 and 0.5, corresponding to the $F_{2}$ where weighs recall higher than
%precision (by placing more emphasis on false negatives) and the $F_{0.5}$ measure, which weighs recall lower than
%precision (by attenuating the influence of false negatives). However, the most commonly used value for $\beta$ is 1,
%corresponding to the $F_{1}$ where precision and recall are weighted equally:
%\bse
%F_{1} = \frac{2 \cdot PPV \cdot TPR}{ PPV + TPR} = \frac{2 \cdot \text{precission}
%\cdot \text{recall}}{\text{precission} + \text{recall}} = \frac{2 \cdot TP}{2 \cdot TP + FN + FP}
%\ese
%
%It is clear now that there are a lot of different error metrics for classification. One good question is ``which ones
%should I use?''. What follows is a practical guide on which ones are the most important and how to use them.
%
%\subsection{Accuracy \& Precision/Recall Trade-Off}
%
%\textbf{Accuracy} is quire straight forward and easy to understand evaluation metric. It just says how accurate the
%classifier is. However, one has to be extra careful with using accuracy as an evaluation metric. More often than not
%the dataset is skewed which results to the positive label (the one we want to predict) appearing in just a small
%portion of the dataset (say 5\%). This means that an algorithm predicting always negative would be 95\% accurate.
%What accuracy lacks is capturing the importance of not finding the positive ones. \v
%
%What people usually pay attention in classification errors is the accuracy of the positive predictions, i.e.\ the
%\textbf{precision} of the classifier. In simple words precision tells us how many times the algorithm was correct
%when it predicted positive. A trivial way to have perfect precision is to make one single positive prediction and
%ensure it is correct. But this would not be very useful, since the classifier would ignore all but one positive
%instance. So precision is typically used along \textbf{recall}. Recall simply states what percentage of the positive
%labels in the dataset was predicted correctly from the algorithm. Precision and recall are usually the most useful
%metrics to evaluate a classifier. \v
%
%Depending on the nature of the problem sometimes one mostly cares about precision, and in other contexts really care
%about recall. For example, if one trains a classifier to detect videos that are safe for kids, they would probably
%prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than
%a classifier that has a much higher recall but lets a few really bad videos show up. On the other hand, suppose one
%trains a classifier to detect shoplifters in surveillance images: it is probably fine if their classifier has only
%30\% precision as long as it has 99\% recall (sure, the security guards will get a few false alerts, but almost all
%shoplifters will get caught). Unfortunately, one can't have it both ways: increasing precision reduces recall, and
%vice versa. This is called the ``precision/recall trade-off''. \v
%
%Subsequently \textbf{$F_1$ score} that combines precision and recall is also quite useful, in particular if one needs
%a simple way to compare two classifiers. The $F_1$ score is the harmonic mean of precision and recall and,
%unfortunately, it does not have a straight forward intuitive meaning. The way to interpret it is to keep in mind that
%the harmonic mean gives much more weight to low values. As a result, a classifier will only get a high $F_1$
%score if both recall and precision are high.
%
%\subsection{Receiver Operating Characteristic (ROC) Curve}
%
%Another very widely used technique in classification evaluation is the so called ``receiver operating
%characteristic curve'' (ROC) which is a graphical plot that illustrates the diagnostic ability of a binary classifier
%system as its discrimination threshold is varied. In other words, each point in the curve represents a different
%value of the discrimination threshold (from 0 to 1), hence, a completely different behaviour of the model which leads
%to a different confusion matrix and subsequently different error metrics for each value. The method was originally
%developed for operators of military radar receivers, which is why it is so named. \v
%
%The ROC curve is created by plotting the true positive rate (TPR or sensitivity or recall) against the false positive
%rate (FPR or 1 - specificity). It can also be thought of as a plot of the power as a function of the Type I Error of
%the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as
%estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. \v
%
%ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and
%prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way
%to cost/benefit analysis of diagnostic decision making. \v
%
%When using normalized units, the area under the curve (often referred to as simply the AUC) is equal to the
%probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative
%one (assuming ``positive'' ranks higher than ``negative'').
%
%\fig{roc}{0.4}
%
%The machine learning community most often uses the ROC AUC statistic for model comparison. In the figure for example,
%the blue line is a better classifier than the orange and the green, because it has a larger are under the curve.
%Subsequently the orange is better than the green. This practice however has been questioned because AUC estimates are
%quite noisy and suffer from other problems. Nonetheless, the coherence of AUC as a measure of aggregated
%classification performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a
%number of other performance metrics such as the Brier score. Another problem with ROC AUC is that reducing the ROC
%curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance
%points plotted and not the performance of an individual system, as well as ignoring the possibility of concavity repair.
%
%\subsection{Baselines}
%
%Evaluation metrics, by themselves, mean little. When evaluating a model, it's essential to know the baseline we're
%evaluating it against. The exact baselines should vary from one use case to another, but here are the five baselines
%that might be useful across use cases.
%
%\bd[Random Baseline]
%\textbf{Random baseline} is the baseline of a model which just predicts at random.
%\ed
%
%In random baseline, the predictions are generated at random following a specific distribution, which can be the
%uniform distribution or the task's label distribution.
%
%\bd[Simple Heuristic Baseline]
%\textbf{Simple heuristic baseline} is the baseline of predictions based on simple heuristics.
%\ed
%
%In simple heuristic baseline, no machine learning model is involved.
%
%\bd[Zero Rule Baseline]
%\textbf{Zero rule baseline} is the baseline of a model which always predicts the most common class.
%\ed
%
%The zero rule baseline is a special case of the simple heuristic baseline.
%
%\bd[Human Baseline]
%\textbf{Human baseline} is the baseline of predictions made by human experts.
%\ed
%
%In many cases, the goal of machine leaning is to automate what would have been otherwise done by humans, so it's
%useful to know how your model performs compared to human experts. Even if your system isn't meant to replace human
%experts and only to aid them in improving their productivity, it's still important to know in what scenarios this
%system would be useful to humans.
%
%\bd[Existing Solutions Baseline]
%\textbf{Existing solutions baseline} is the baseline of existing solution models.
%\ed
%
%In many cases, machine learning systems are designed to replace existing solutions, which might be business logic
%with a lot of if/else statements or third-party solutions. It's crucial to compare your new model to these existing
%solutions. Your machine learning model doesn't always have to be better than existing solutions to be useful. A model
%whose performance is a little bit inferior can still be useful if it's much easier or cheaper to use.
%
%\section{Support Vector Machine}
%
%Support vector machine (SVM) is another, more advanced, supervised learning model with associated learning algorithm
%that analyze data for classification and regression analysis. Developed at Bell Laboratories by Vladimir Vapnik SVMs
%are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed
%by Vapnik and Chervonenkis. \v
%
%Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds
%a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear
%classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM
%maps training examples to points in space so as to maximise the width of the gap between the two categories. New
%examples are then mapped into that same space and predicted to belong to a category based on which side of the gap
%they fall. \v
%
%Although SVM applies mainly in classification problems however there is also another model called support vector
%regression that applies the same ideas in regression problems. Here we will explore only SVM. \v
%
%To tell the SVM story, we'll need to first talk about margins and the idea of separating data with a large ``gap''.
%Next, we'll talk about the optimal margin classifier, which will lead us into a digression on Lagrange duality. We'll
%also see kernels, which give a way to apply SVMs efficiently in very high dimensional (such as infinite dimensional)
%feature spaces, and finally, we'll close off the story with the SMO algorithm, which gives an efficient
%implementation of SVMs. \v
%
%Consider logistic regression, where the probability $P(y = 1| \boldsymbol{x}; \boldsymbol{w})$ is modelled by:
%\bse
%h(\boldsymbol{w}^{\intercal} \boldsymbol{x}) = \frac{1}{1 + exp(-\boldsymbol{w}^{\intercal} \boldsymbol{x})}
%\ese
%
%We would then predict 1 on an input $\boldsymbol{x}$ if and only if $h \left(\boldsymbol{w}^{\intercal} \boldsymbol{x}\right)
%\geq 0.5$ or equivalently, if and only if $\boldsymbol{w}^{\intercal} \boldsymbol{x} \geq 0$. Consider a positive
%training example ($y = 1$). The larger $\boldsymbol{w}^{\intercal} \boldsymbol{x}$ is, the larger also is $h
%\left(\boldsymbol{w}^{\intercal} \boldsymbol{x}\right)$ a.k.a.\ the larger $P(y = 1| \boldsymbol{x}; \boldsymbol{w})$ is, and thus
%also the higher our degree of confidence that the label is 1. Thus, informally we can think of our prediction as
%being a very confident one that $y = 1$ if $\boldsymbol{w}^{\intercal} \boldsymbol{x} \gg 0$. \v
%
%Similarly, we think of logistic regression as making a very confident prediction of $y = 0$, if
%$\boldsymbol{w}^{\intercal} \boldsymbol{x} \ll 0$. Given a training set, again informally it seems that we'd have
%found a good fit to the training data if we can find $\boldsymbol{w}$ so that $\boldsymbol{w}^{\intercal}
%\boldsymbol{x}^{(i)} \gg 0$ whenever $y^(i) = 1$ and $\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} \ll 0$ whenever
%$y^(i) = 0$, since this would reflect a very confident (and correct) set of classifications for all the training
%examples. This seems to be a nice goal to aim for, and we'll soon formalize this idea using the notion of functional
%margins. \v
%
%For a different type of intuition, consider the following figure, in which the symbol ``X'' represent positive
%training examples, the symbol ``O'' denote negative training examples, a decision boundary (this is the line given by
%the equation $\boldsymbol{w}^{\intercal} \boldsymbol{x} = 0$ is also called the separating hyperplane) is also shown,
%and three points have also been labelled ``A'', ``B'' and ``C''.
%
%\fig{svm}{0.5}
%
%Notice that the point ``A'' is very far from the decision boundary. If we are asked to make a prediction for the value
%of $y$ at ``A'', it seems we should be quite confident that $y = 1$ there. Conversely, the point ``C'' is very close to
%the decision boundary, and while it's on the side of the decision boundary on which we would predict $y = 1$, it
%seems likely that just a small change to the decision boundary could easily have caused out prediction to be $y = 0$.
%Hence, we're much more confident about our prediction at ``A'' than at ``C''. The point ``B'' lies in-between these two
%cases, and more broadly, we see that if a point is far from the separating hyperplane, then we may be significantly
%more confident in our predictions. Again, informally we think it'd be nice if, given a training set, we manage to
%find a decision boundary that allows us to make all correct and confident (meaning far from the decision boundary)
%predictions on the training examples. We'll formalize this later using the notion of geometric margins. \v
%
%To make our discussion of SVMs easier, we'll first need to introduce a new notation for talking about classification.
%We will be considering a linear classifier for a binary classification problem with labels $y$ and features $x$. From
%now, we'll use $y \in \{ -1, 1 \}$ (instead of $ \{ 0, 1 \} $) to denote the class labels. Also, we will separate the
%$w_0$ component from $\boldsymbol{w}$ and from now on we will be denoting it $b$, and we will write our classifier as:
%\bse
%h_{\boldsymbol{w}, b} (\boldsymbol{x}) = g(\boldsymbol{w}^{\intercal} \boldsymbol{x} + b)
%\ese
%
%\v
%
%This $\boldsymbol{w}$, $b$ notation allows us to explicitly treat the intercept term $b$ separately from the other
%parameters. (We also drop the convention we had previously of letting $x_0 = 1$ be an extra coordinate in the input
%feature vector.) Note also that, from our definition of $g$ above, our classifier will directly predict either 1 or
%-1 without first going through the intermediate step of estimating the probability of $y$ being 1 (which was what
%logistic regression did).
%
%\bd[Functional Margin Of A Training Example]
%Given a training example $(x^{(i)}, y^{(i)})$ we define the \textbf{functional margin of $(\boldsymbol{w}$, $b$) with
%respect to the training example} as:
%\bse
%{\hat{\gamma}}^{(i)} = y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b)
%\ese
%\ed
%
%Note that if $y^{(i)}= 1,$ then for the functional margin to be large (i.e.\ for our prediction to be confident and
%correct), we need $\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b$ to be a large positive number. Conversely, if
%$y^{(i)}= -1,$ then for the functional margin to be large (i.e.\, for our prediction to be confident and correct),
%we need $\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b$ to be a large negative number. Moreover, if
%$\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b \neq 0$, then our prediction on this example is correct. Hence,
%a large functional margin represents a confident and a correct prediction. \v
%
%For a linear classifier with the choice of $g$ given above, there's one property of the functional margin that makes
%it not a very good measure of confidence, however. Given our choice of $g$, we note that if we replace
%$\boldsymbol{w}$ with $2\boldsymbol{w}$ and $b$ with $2b$, then since $g \left(\boldsymbol{w}^{\intercal} \boldsymbol{x} +
%b\right) = g\left(2\boldsymbol{w}^{\intercal} \boldsymbol{x} + 2b\right)$ this would not change $h_{\boldsymbol{w}, b}
%(\boldsymbol{x})$ at all meaning that $g$, and hence, also $h_{\boldsymbol{w}, b} (\boldsymbol{x})$, depends only on
%the sign, but not on the magnitude, of $\boldsymbol{w}^{\intercal} \boldsymbol{x} + b$. However, replacing the
%scaling by a factor also results in multiplying our functional margin by the same factor. Thus, it seems that by
%exploiting our freedom to scale $\boldsymbol{w}$ and $b$, we can make the functional margin arbitrarily large without
%really changing anything meaningful. Intuitively, it might therefore make sense to impose some sort of normalization
%condition. \v
%
%Given a training set we also define the functional margin of ($\boldsymbol{w}$, $b$) with respect to the set as follows.
%
%\bd[Functional Margin Of A Set] Given a training set $\{ x^{(i)}, y^{(i)} \}$ we define the \textbf{functional margin
%of $(\boldsymbol{w}$, $b$) with respect to the training set} as:
%\bse
%{\hat{\gamma}} = \min {\hat{\gamma}}^{(i)}
%\ese
%\ed
%
%Next, let's talk about geometric margins. Consider the picture below:
%
%\fig{svm2}{0.5}
%
%The decision boundary corresponding to ($\boldsymbol{w}$, $b$) is shown, along with the vector $\boldsymbol{w}$. Note
%that $\boldsymbol{w}$ is orthogonal to the separating hyperplane. Consider the point at $A$, which represents the
%input $\boldsymbol{x}^{(i)}$ label $y^{(i)} = 1$. Its distance to the decision boundary, $\gamma^{(i)}$ is given by
%the line segment $AB$. \v
%
%How can we find the value of $y^{(i)}$? Well, $\frac{\boldsymbol{w}}{||\boldsymbol{w}||}$ is a unit-length vector
%pointing in the same direction as $\boldsymbol{w}$. Since $A$ represents $\boldsymbol{x}^{(i)}$ we therefore find
%that the point $B$ is given by $\boldsymbol{x}^{(i)} - \gamma^{(i)} \frac{\boldsymbol{w}}{||\boldsymbol{w}||}$. But
%this point lies on the decision boundary, and all points on the decision boundary satisfy the equation
%$\boldsymbol{w}^{\intercal} \boldsymbol{x} + b = 0$. Hence:
%\bse
%\boldsymbol{w}^{\intercal} \Big(\boldsymbol{x}^{(i)} - \gamma^{(i)}\frac{\boldsymbol{w}}{||\boldsymbol{w}||} \Big) + b
%= 0
%\ese
%
%Solving for $\gamma^{(i)}$ yields:
%\bse
%\gamma^{(i)} = \frac{\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b}{||\boldsymbol{w}||}
%= \left(\frac{\boldsymbol{w}}{||\boldsymbol{w}||} \right)^{\intercal}\boldsymbol{x}^{(i)} + \frac{b}{||\boldsymbol{w}||}
%\ese
%
%\v
%
%This was worked out for the case of a positive training example at $A$ in the figure, where being on the positive
%side of the decision boundary is good. \v
%
%We are now ready to properly define the geometrical margin.
%
%\bd[Geometrical Margin Of A Training Example] Given a training example $(x^{(i)}, y^{(i)})$ we define the
%\textbf{geometrical margin of $(\boldsymbol{w}$, $b$) with respect to the training example} as:
%\bse
%\gamma^{(i)} = y^{(i)} \left( \left( \frac{\boldsymbol{w}}{||\boldsymbol{w}||} \right)^{\intercal} \boldsymbol{x}^{(i)}
%+ \frac{b}{||\boldsymbol{w}||} \right)
%\ese
%\ed
%
%Note that if $||\boldsymbol{w}|| = 1$, then the functional margin equals the geometric margin. This thus gives us a
%way of relating these two different notions of margin. Also, the geometric margin is invariant to rescaling of the
%parameters. This will in fact come in handy later. Specifically, because of this invariance to the scaling of the
%parameters, when trying to fit $\boldsymbol{w}$ and $b$ to training data, we can impose an arbitrary scaling
%constraint on $\boldsymbol{w}$ without changing anything important. \v
%
%Finally, given a training set we also define the geometric margin of (w, b) with respect to the set to be the
%smallest of the geometric margins on the individual training examples:
%
%\bd[Geometrical Margin Of A Set] Given a training set $\{ x^{(i)}, y^{(i)} \}$ we define the \textbf{geometrical
%margin of $(\boldsymbol{w}$, $b$) with respect to the training set} as:
%\bse
%\gamma = \min {\gamma}^{(i)}
%\ese
%\ed
%
%Given a training set, it seems from our previous discussion that a natural desideratum is to try to find a decision
%boundary that maximizes the (geometric) margin, since this would reflect a very confident set of predictions on the
%training set and a good ``fit'' to the training data. Specifically, this will result in a classifier that separates
%the positive and the negative training examples with a ``gap'' (geometric margin). \v
%
%For now, we will assume that we are given a training set that is linearly separable, i.e.\ that it is possible to
%separate the positive and negative examples using some separating hyperplane. How we we find the one that achieves
%the maximum geometric margin? We can pose the following optimization problem:
%\bse
%\max_{\gamma, \boldsymbol{w}, b} \gamma
%\ese
%
%subject to:
%\bse
%y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) \geq \gamma, \:\:\: i=1,2,\ldots,m
%\qquad \text{and} \qquad || \boldsymbol{w} || = 1
%\ese
%
%\v
%
%In other words, we want to maximize $\gamma$, subject to each training example having functional margin at least
%$\gamma$. The $|| \boldsymbol{w} || = 1$ constraint moreover ensures that the functional margin equals to the
%geometric margin, so we are also guaranteed that all the geometric margins are at least $\gamma$,. Thus, solving this
%problem will result in parameters with the largest possible geometric margin with respect to the training set. \v
%
%If we could solve the optimization problem above, we'd be done. But the $|| \boldsymbol{w} || = 1$ constraint is a
%nasty (non-convex) one, and this problem certainly isn't in any format that we can plug into standard optimization
%software to solve. So, lets try transforming the problem into a nicer one. Consider:
%\bse
%\max_{\gamma, \boldsymbol{w}, b} \frac{\hat{\gamma}}{|| \boldsymbol{w} ||}
%\ese
%
%subject to:
%\bse
%y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) \geq \hat{\gamma}, \:\:\: i=1,2,\ldots, m
%\ese
%
%\v
%
%Here, we're going to maximize $\frac{\hat{\gamma}}{|| \boldsymbol{w} ||}$ subject to the functional margins all being
%at least $\hat{\gamma}$. Since the geometric and functional margins are related by $\gamma = \frac{\hat{\gamma}}{||
%\boldsymbol{w} ||}$ this will give us the answer we want. Moreover, we've gotten rid of the constraint $||
%\boldsymbol{w} || = 1$ that we didn't like. The downside is that we now have a nasty (again, non-convex) objective
%$\frac{\hat{\gamma}}{|| \boldsymbol{w} ||}$ function, and, we still don't have any off-the-shelf software that can
%solve this form of an optimization problem. \v
%
%Lets keep going. Recall our earlier discussion that we can add an arbitrary scaling constraint on $\boldsymbol{w}$
%and $b$ without changing anything. This is the key idea we'll use now. We will introduce the scaling constraint that
%the functional margin of $\boldsymbol{w}$ and $b$ with respect to the training set must be 1:
%\bse
%\hat{\gamma} = 1
%\ese
%
%Since multiplying $\boldsymbol{w}$ and $b$ by some constant results in the functional margin being multiplied by that
%same constant, this is indeed a scaling constraint, and can be satisfied by rescaling $\boldsymbol{w}$ and $b$.
%Plugging this into our problem above, and noting that maximizing $\frac{\hat{\gamma}}{|| \boldsymbol{w} ||} =
%\frac{1}{|| \boldsymbol{w} ||} $ is the same thing as minimizing $|| \boldsymbol{w} ||^2$, we now have the following
%optimization problem:
%\bse
%\min_{\gamma, \boldsymbol{w}, b} \frac{1}{2} || \boldsymbol{w} ||^2
%\ese
%
%subject to:
%\bse
%y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) \geq 1, \:\:\: i=1,2,\ldots,m
%\ese
%
%\v
%
%We can rewrite the constraints as:
%\bse
%g_{i}(\boldsymbol{w}) = -y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) + 1 \leq 0, \:\:\: i=1,2,\ldots,m
%\ese
%
%According to Appendix~\ref{ch:constrained-optimization} we are dealing with an inequality constrained optimization
%problem (with no equality part) so we will use the KKT multipliers in order to solve it. \v
%
%Notice that that all KKT conditions are satisfied, since we do not have any equality constraints (Hence, $\mu_{i} = 0,
%\:\:\: \forall i$), and we have $\lambda_{i} > 0$ only for the training examples that have functional margin exactly
%equal to one, i.e.\ the ones corresponding to constraints that hold with equality $ g_{i}(\boldsymbol{w}) = 0$.
%Consider the figure below, in which a maximum margin separating hyperplane is shown by the solid line.
%
%\fig{svm3}{0.5}
%
%\vspace{-10pt}
%
%The points with the smallest margins are exactly the ones closest to the decision boundary. Here these are the three
%points: one negative and two positive examples, that lie on the dashed lines parallel to the decision boundary. Thus,
%only three of the $\lambda_{i}$ (the ones corresponding to these three training examples) will be non-zero at the
%optimal solution to our optimization problem. These three points are called ``support vectors''. The fact that the
%number of support vectors can be much smaller than the size the training set will be useful later. \v
%
%Moving on by applying what we developed in Appendix~\ref{ch:constrained-optimization} the Lagrangian of the
%inequality constrained optimization problem reads:
%\bse
%\mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) = \frac{1}{2} || \boldsymbol{w} ||^2 - \sum_{i} \lambda_{i} \left( y^{(i)}
%(\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) - 1 \right)
%\ese
%
%Note that there are only $\lambda_{i}$ but no $\mu_{i}$ Lagrange multipliers, since the problem has only inequality
%constraints. \v
%
%Now we move on trying to solve this optimization problem. Once again, according to
%Appendix~\ref{ch:constrained-optimization} the necessary conditions for optimization of $ \mathcal{L}$ turn to:
%\bse
%\nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) = 0 \:\:\: \text{and} \:\:\:
%\frac{\partial \mathcal{L}(\boldsymbol{w}, b, \lambda_{i})}{\partial b} = 0 \:\:\: \text{and} \:\:\:
%\frac{\partial \mathcal{L} (\boldsymbol{w}, b, \lambda_{i})}{\partial \lambda_{i}} = 0
%\ese
%
%Starting with the first one, quite straightforward we obtain:
%\bse
%\nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) = \boldsymbol{w} - \sum_{i} \lambda_{i} y^{(i)}
%\boldsymbol{x}^{(i)} = 0
%\ese
%
%which implies:
%\bse
%\boldsymbol{w} = \sum_{i} \lambda_{i} y^{(i)} \boldsymbol{x}^{(i)}
%\ese
%
%As for the derivative with respect to $b$, we obtain:
%\bse
%\frac{\partial \mathcal{L}(\boldsymbol{w}, b, \lambda_{i})}{\partial b} = \sum_{i} \lambda_{i} y^{(i)} = 0
%\ese
%
%By manipulating the initial Lagrangian a bit we get:
%{\setlength{\jot}{2pt}
%\begin{align*}
%\mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) & = \frac{1}{2} || \boldsymbol{w} ||^2 - \sum_{i} \lambda_{i}
%\left(y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) - 1\right) \\
%& = \frac{1}{2} || \boldsymbol{w} ||^2 - \sum_{i} \lambda_{i} y^{(i)} \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}
%- b \sum_{i} \lambda_{i} y^{(i)} + \sum_{i} \lambda_{i}
%\end{align*}}
%
%By substituting the results from the two first derivatives in the previous expression of the Lagrangian we end up to:
%{\setlength{\jot}{5pt}
%\begin{align*}
%\mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) & = \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
%\left( \boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} - \sum_{i} \lambda_{i} y^{(i)}
%\left( \sum_{j} \lambda_{j} y^{(j)} \left( \boldsymbol{x}^{(j)} \right)^{\intercal} \right) \boldsymbol{x}^{(i)}
%+ \sum_{i} \lambda_{i} \\
%& = \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
%\left( \boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} - \sum_{i} \sum_{j} \lambda_{i}
%\lambda_{j} y^{(i)} y^{(j)} \left(\boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} + \sum_{i}\lambda_{i} \\
%& = \sum_{i} \lambda_{i} - \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
%\left(\boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)}
%\end{align*}}
%
%Hence, now the Lagrangian reads:
%\bse
%\mathcal{L}(\lambda_{i}) = \sum_{i} \lambda_{i} - \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
%\left( \boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)}
%\ese
%
%where the dependencies on the parameters $\boldsymbol{w}$ and $b $ are gone and the Lagrangian is just a function of
%the KKT multipliers. So we ended up with the so called ``dual optimization problem'' which is:
%\bse
%\max_{\lambda_{i}} \left( \sum_{i} \lambda_{i} - \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
%\left( \boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} \right)
%\ese
%
%subject to:
%\bse
%\lambda_{i} \geq 0, \:\:\: i=1,2,\ldots, m \qquad \text{and} \qquad \sum_{i} \lambda_{i} y^{(i)} = 0
%\ese
%
%where as per usual we can turn the maximization to minimization by multiplying the Lagrangian with a minus sign so we
%end up with:
%\bse
%\min_{\lambda_{i}} \left(\frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
%\left(\boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} - \sum_{i} \lambda_{i} \right)
%\ese
%
%subject to:
%\bse
%\lambda_{i} \geq 0, \:\:\: i=1,2,\ldots, m \qquad \text{and} \qquad \sum_{i} \lambda_{i} y^{(i)} = 0
%\ese
%
%\v
%
%As usual we can rewrite everything in form of matrices. By introducing the matrix $Q$ and the KKT multipliers vector
%$\boldsymbol{\lambda}$:
%\bse
%Q = \begin{bmatrix}
%y^{(1)} y^{(1)} \left( \boldsymbol{x}^{(1)} \right)^{\intercal} \boldsymbol{x}^{(1)} & \ldots & y^{(1)} y^{(m)}
%\left( \boldsymbol{x}^{(1)} \right)^{\intercal} \boldsymbol{x}^{(m)} \\
%y^{(2)} y^{(2)} \left( \boldsymbol{x}^{(2)} \right)^{\intercal} \boldsymbol{x}^{(2)} & \ldots & y^{(2)} y^{(m)}
%\left( \boldsymbol{x}^{(2)} \right)^{\intercal}\boldsymbol{x}^{(m)} \\
%\vdots & \vdots & \ddots & \ldots \\
%y^{(m)} y^{(1)} \left( \boldsymbol{x}^{(m)} \right)^{\intercal} \boldsymbol{x}^{(1)} & \ldots & y^{m)} y^{(m)}
%\left( \boldsymbol{x}^{(m)} \right)^{\intercal} \boldsymbol{x}^{(m)}
%\end{bmatrix}, \qquad
%\boldsymbol{\lambda} = \begin{bmatrix} \lambda_{1} \\ \lambda_{2} \\ \vdots \\ \lambda_{m} \end{bmatrix}
%\ese
%
%\v
%
%we can rewrite the dual optimization problem as:
%\bse
%\min_{\boldsymbol{\lambda}} \left( \frac{1}{2} \boldsymbol{\lambda}^\intercal Q \boldsymbol{\lambda}
%- \boldsymbol{\lambda} \right)
%\ese
%
%subject to:
%\bse
%\boldsymbol{\lambda} \geq 0 \qquad \text{and} \qquad \boldsymbol{y}^\intercal \boldsymbol{\lambda} = 0
%\ese
%
%\v
%
%This final dual optimization problem is usually solved numerically through quadratic programming (as for example we
%use gradient descent for loss functions optimization problems). Once we solve it we obtain the KKT multipliers vector
%$\boldsymbol{\lambda}$ and subsequently all the individual KKT multipliers $\lambda_{i}$ from its components. We also
%notice that almost all $\lambda_{i}$'s are $0$ except from a few ones where $\lambda_{i} > 0$ coming from the support
%vectors. By making use of the KKT multipliers we find the parameter $\boldsymbol{w}$ as:
%\bse
%\boldsymbol{w} = \sum_{i} \lambda_{i} y^{(i)} \boldsymbol{x}^{(i)}
%\ese
%
%We once again notice that the only terms that survive are the ones with $\lambda_{i} > 0$ coming from the support
%vectors. All the other points do not contribute at all to the model. \v
%
%Once we have $\boldsymbol{w}$ we can compute $b$ from the fact that for support vectors their distance from the plane
%is equal to $1$, i.e.\ :
%\bse
%y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) = 1
%\ese
%
%hence:
%\bse
%b = \frac{1}{y^{(i)}} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}
%\ese
%
%\v
%
%for any $i$ that is a support vector. This last equation should produce the same parameter $b$ for all support
%vectors. This is a good way to check that the models works fine. \v
%
%By having found both $\boldsymbol{w}$ and $b$, we have found the separator $\boldsymbol{w}^{\intercal} \boldsymbol{x}
%+ b$ and we are in the position to predict to which class a new point belongs. Since $\boldsymbol{w}^{\intercal}
%\boldsymbol{x} + b$ is going to be either negative (if the point lies below the plane) or positive (if the point lies
%above the plane) we finally have for the prediction:
%\bse
%h(\boldsymbol{x}) = sign(\boldsymbol{w}^{\intercal} \boldsymbol{x} + b)
%\ese
%
%This model of classification is called ``hard margin support vector machine''. A careful reader would notice that
%in hard margin support vector machine we strictly impose that all instances must be off the street and on the right
%side, i.e.\ no violations are allowed (Hence, the naming). There are two main issues with hard margin classification.
%First, it only works if the data is linearly separable. Second, it is sensitive to outliers. \v
%
%However, in real life the data are almost never linearly separable and the hard margin support vector machine would
%not be able to find a solution. \v
%
%To avoid these issues, one should use a more flexible model. The objective is to find a good balance between keeping
%the street as large as possible and limiting the margin violations (i.e.\ instances that end up in the middle of
%the street or even on the wrong side). \v
%
%Fortunately the generalization of hard margin support vector machine to non-linear separable is easy. Without getting
%into details two of the ways we can achieve this are the following.
%\bit
%\item \textbf{Non-Linear Transformations \& Kernels} By switching from a space $X$ where the data are not linear
%separable to a space $Z$ where the data are linear separable through a transformation $\Phi$:
%\bse
%(\boldsymbol{x_1}, \boldsymbol{x_2}, \ldots, \boldsymbol{x_n}) \xrightarrow{\Phi}{}
%(\boldsymbol{z_1}, \boldsymbol{z_2}, \ldots, \boldsymbol{z_n})
%\ese
%
%Accordingly the dual optimization problem would change to:
%\bse
%\min_{\lambda_{i}} \left(\frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
%\left(\boldsymbol{z}^{(i)} \right)^{\intercal} \boldsymbol{z}^{(j)} - \sum_{i} \lambda_{i} \right)
%\ese
%
%subject to:
%\bse
%\lambda_{i} \geq 0, \:\:\: i=1,2,\ldots, m \qquad \text{and} \qquad \sum_{i} \lambda_{i} y^{(i)} = 0
%\ese
%
%and then we would follow the same procedure as before in order to obtain a solution. \v
%
%The biggest caveat of this method is the inner product $\left (\boldsymbol{z}^{(i)} \right)^{\intercal}
%\boldsymbol{z}^{(j)}$ that needs to be computed in $Z$ space and it's computationally expensive. One way to overcome
%this problem is by using the trick of kernels (Appendix~\ref{ch:kernels}) in which we substitute the inner product
%with a kernel:
%\bse
%\left(\boldsymbol{z}^{(i)} \right)^{\intercal} \boldsymbol{z}^{(j)} = K (\boldsymbol{x}^{(i)}, \boldsymbol{x}^{(j)})
%\ese
%
%Given the freedom of choosing a kernel the model turns quite flexible.
%
%\item \textbf{L1 Regularization \& Soft Margin SVM} While mapping data to a high dimensional feature space via $\Phi$
%does generally increase the likelihood that the data is separable, we can't guarantee that it always will be so.
%Also, in some cases it is not clear that finding a separating hyperplane is exactly what we'd want to do, since that
%might be susceptible to outliers. \v
%
%To make the algorithm work for non-linearly separable datasets as well as be less sensitive to outliers, we can
%simply impose the technique of L1 Regularization that we mentioned in the previous chapter. That way our model is
%more open to errors and it allows some wrongly labeled data. \v
%
%\fig{svm4}{0.6}
%
%In this case the final model is called ``soft margin support vector machine'' (in contrast with the SVM we
%developed in the beginning called ``hard margin support vector machine''), due to the fact that it allows the
%margin to be violated by outliers.
%\eit
%
%Now let's move on to a new subject.
%
%\section{$k$-Nearest Neighbors}
%
%In this section we will introduce yet another famous supervised learning algorithm called ``k-nearest neighbors
%algorithm'' ($k$-NN), which is a non-parametric\footnote{A model is called ``nonparametric'' not because it does not
%have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so
%the model structure is free to stick closely to the data. In contrast, a parametric model, such as a linear model,
%has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting
%(but increasing the risk of underfitting).} method used for classification and regression. In both cases, the
%input consists of the $k$ closest training examples in the feature space, since the algorithm assumes that similar
%things exist in close proximity (in other words, similar things are near to each other). The output depends on
%whether $k$-NN is used for classification or regression:
%\bit
%\item In $k$-NN classification, the output is a class membership. An object is classified by a plurality vote of its
%neighbors, with the object being assigned to the class most common among its $k$ nearest neighbors ($k$ is a positive
%integer, typically small). If $k = 1$, then the object is simply assigned to the class of that single nearest neighbor.
%\item In $k$-NN regression, the output is the property value for the object. This value is the average of the values
%of $k$ nearest neighbors.
%\eit
%
%One good question is ``how to measure the distance between two points''? There are many ways of calculating
%distance, and one way might be preferable depending on the problem we are solving. However, the straight-line
%distance (i.e.\ the Euclidean distance) is a popular and familiar choice. \v
%
%Another good questions is ``how many neighbors should I choose for the model''? In other words what should be the
%value of $k$? The best choice of $k$ depends upon the data. Generally, larger values of $k$ reduces effect of the
%noise on the classification, but make boundaries between classes less distinct. A good $k$ can be selected by various
%heuristic techniques. In binary classification problems, it is helpful to choose $k$ to be an odd number as this
%avoids tied votes. One popular way of choosing the empirically optimal $k$ in this setting is via bootstrap method.
%The special case where the class is predicted to be the class of the closest training sample (i.e.\ when $k = 1$) is
%called the ``nearest neighbor'' algorithm. \v
%
%\fig{knn}{0.40}
%
%$k$-NN has many advantages. First of all the algorithm is very simple and easy to implement. Also, there is no need
%to build a model, tune several parameters, or make additional assumptions. Finally the algorithm is versatile, i.e.\
%it can be used for classification, regression. However, of course, it carries and some disadvantages. One has to
%keep in mind that the algorithm gets significantly slower as the number of examples and/or predictors/independent
%variables increase and that the accuracy of $k$-NN algorithm can be severely degraded by the presence of noisy or
%irrelevant features, or if the feature scales are not consistent with their importance. \v
%
%A drawback of the basic ``majority voting'' classification occurs when the class distribution is skewed. That is,
%examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common
%among the $k$ nearest neighbors due to their large number. In order to overcome this problem (both for classification
%and regression), a useful technique can be to assign weights to the contributions of the neighbors, so that the
%nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme
%consists in giving each neighbor a weight of $\frac{1}{d}$, where $d$ is the distance to the neighbor.

\newpage

\section{Decision Tree}

Decision trees are the fundamental building blocks of a random forest (that we will see later in the notes), which is
among the most powerful machine learning algorithms available today. This is the main motivation why we need to study
decision trees.

\bd [Decision Tree]
A \textbf{decision tree} is a decision support recursive partitioning structure that uses a tree-like model of
decisions and their possible alternatives or outcomes.
\ed

Decision trees are versatile (can perform both classification and regression tasks), non-parametric (the number of
parameters is not determined prior to training), powerful, machine learning algorithms, capable of fitting complex
datasets. They go from observations about an item to conclusions about the item's target value. They are one way to
display an algorithm that only contains conditional control statements. Their flowchart-like structure helps in
decision-making since it mimics the human-level thinking. That is why decision trees are easy to understand and
interpret. \v

Let's begin with some basic definitions.

\bd[Decision Node]
A \textbf{decision node}, depicted by a square, represents a condition, or choice, to be made on a feature or attribute.
\ed

\bd[Root Decision Node]
The topmost decision node in a decision tree is known as the \textbf{root decision node}.
\ed

The root decision node represents the entire population (or sample) which then gets divided into two or more homogeneous
sets. This process of division is called ``splitting''.

\bd[Splitting]
\textbf{Splitting} is a process of dividing a decision node into two or more sub-nodes.
\ed

\bd [Branch]
A \textbf{branch} represents one of the alternatives of a decision node.
\ed

\bd[Parent Node]
A decision node which is divided into sub-nodes is called a \textbf{parent node}.
\ed

\bd[Child Node]
The sub-nodes of a parent node are called a \textbf{child nodes}.
\ed

As it makes sense, the root decision node does not have a parent node.

\bd[Terminal Node / Leaf]
A \textbf{terminal node}, or \textbf{leaf}, is a decision node with no children nodes, and it represents the decision,
or outcome, of a decision tree.
\ed

\fig{decisiontree}{0.35}

Decision trees regress/classify the examples in a top-down approach by sorting them down the tree, from the root to
some leaf node, with the leaf node providing the regression/classification to the example. Each node in the tree
acts as a test case for some attribute, and each branch descending from that node corresponds to one of the possible
answers to the test case. \v

The algorithm begins by selecting the best attribute using something called ``attribute selection measures'' to
split the records. Then it makes that attribute a decision node and breaks the dataset into smaller subsets. It then
starts the tree building by repeating this process recursively for each child node until all the tuples belong to
the same feature value, or there are no more remaining feature or instances.

\bd[Attribute Selection Measure]
An \textbf{attribute selection measure} is a heuristic for selecting the splitting that partition data into the best
possible manner.
\ed

Two of the most popular attribute selection measures are the ``information gain'' and the ``Gini impurity''. Both of
them measure the ``impurity'' of a node, so let's start by defining what impurity is.

\bd [Node Impurity]
\textbf{Node impurity} is a measure of the homogeneity of the labels at the node.
\ed

\bd [Pure Node]
A node is called \textbf{pure} when all of its data belongs to a single class.
\ed

\bd [Impure Node]
A node is called \textbf{impure} when not all of its data belongs to a single class.
\ed

\be
In this figure, node C requires less information to describe as all values are similar, while node B requires more
information to describe it and node A requires the maximum information. In impurity terms, C is a pure node, B is less
impure and A is more impure.

\fig{purity}{0.45}
\ee

Information gain is defined through ``information entropy'', so let's start by defining the latter.

\bd [Information Entropy]
Given a discrete random variable $X$, with possible outcomes $\{x_{1}, \ldots, x_{n} \}$ which occur with probabilities
$P(x_{1}), \ldots, P (x_{n})$, the \textbf{information entropy} $H$ of $X$ is defined as:
\bse
H(X) = -\sum _{i=1}^{n} {P(x_{i}) \log P(x_{i})}
\ese

\v

where the summation is over the variable's possible values.
\ed

Having defined information entropy we can now define information gain.

\bd [Information Gain]
\textbf{Information gain} $IG$ is the change in information entropy $H$ from a prior state $T$ to a state that takes
some information $a$ as given:
\bse
IG(T,a) = H(T) - H(T|a)
\ese
\ed

In a decision tree $H(T)$ is the information entropy of the parent node and $H(T|a)$ is the sum of the information
entropies of all children nodes. Informally speaking, information gain is a statistical property that measures how
well a given attribute separates the training examples according to their target classification. It is a synonym for
Kullback-Leibler divergence, that we introduced in the parametric inference chapter.

\be
In the figure below, we can see that a feature with low information gain splits the data relatively evenly and as a
result doesn't bring us any closer to a decision.

\fig{purity2}{0.48}

Whereas, an attribute with high information gain splits the data into groups with an uneven number of positives and
negatives and as a result, helps in separating the two from each other.

\fig{purity3}{0.48}

In other words we are always looking to maximize the information gain after any splitting.
\ee

Having defined information gain we can now move on to Gini impurity.

\bd [Gini Impurity \ Gini Index]
Given a discrete random variable $X$, with possible outcomes $\{x_{1}, x_{2}, \ldots, x_{n} \}$ which occur with
probabilities $ P(x_{1}), P(x_{2}), \ldots, P(x_{n})$, the \textbf{Gini impurity}, or \textbf{Gini index}, $I_{G}$ of
$X$ is defined as:
\bse
I_{G}(X) = 1 - \sum _{i=1}^{n} P^2(x_{i})
\ese

\v

where the summation is over the variable's possible values.
\ed

In simple words, Gini impurity measures the degree or probability of a particular variable being wrongly classified
when it is randomly chosen.  \v

Now that we have defined both information gain and Gini impurity, a good question is which one should one use as an
attribute selection measure. In general keep in mind that it only matters in 2\% of the cases whether you use Gini
impurity or information gain. And given that information gain uses information entropy (which uses logarithms), it
might be a little slower to compute. Hence, most of the algorithms use Gini impurity as an attribute selection
measure. \v

In any case, by using either information gain or Gini impurity one can start building (training) the tree. The
building (training) is usually done through a procedure called ``Classification and Regression Tree'' (CART)
algorithm. The algorithm works by first splitting the training set into two subsets using a single feature $k$ and a
threshold $t_k$, by searching for the pair ($k$, $t_k$) that produces the purest subsets (weighted by their size).
Once it has successfully split the training set in two, it splits the subsets using the same logic, then the
sub-subsets, and so on, recursively. Once the CART algorithm finished, the building (training) part is over and the
decision tree is ready. One can now feed a new instance to the tree and by asking the corresponding question of each
node they will end up in one of the leafs, hence, to a decision. \v

The loss function that the algorithm tries to minimize is the so called ``CART loss function''\footnote{For regression,
CART algorithm instead of trying to split the training set in a way that minimizes impurity, it tries to split the
training set in a way that minimizes the MSE\@.}.

\bd [CART Loss Function]
The \textbf{CART loss function for regression} is defined as:
\bse
J(k, t_{k}) = \frac{m_{\text{left}}}{m} \text{MSE}_{\text{left}} + \frac{m_{\text{right}}}{m}\text{MSE}_{\text{right}}
\ese

where $m$ is the total number of instances in both left and right subsets, $m_{\text{left}}$ and $m_{\text{right}}$ are
the number of instances in the left and right subset, and $\text{MSE}_{\text{left}}$ and $\text{MSE}_{\text{right}}$ are
the mean squared errors of the left and the right subsets given by:
\bse
\text{MSE}_{\text{node}} = \sum_{i \in \text{node}} ({\hat{y}}_\text{node} - y^{(i)})^2
\ese

where:
\bse
{\hat{y}}_\text{node} = \frac{1}{m_{\text{node}}} \sum_{i \in \text{node}} y^{(i)}
\ese

The \textbf{CART loss function for classification} is defined as:
\bse
J(k, t_{k}) = \frac{m_{\text{left}}}{m} G_{\text{left}} + \frac{m_{\text{right}}}{m} G_{\text{right}}
\ese

where $m$ is the total number of instances in both left and right subsets, $m_{\text{left}}$ and $m_{\text{right}}$ are
the number of instances in the left and right subset, and $G_{\text{left}}$ and $G_{\text{right}}$ are an attribute
selection measure (either information gain or Gini impurity).
\ed

CART algorithm is greedy, since it greedily searches for an optimum split at the top level, then repeats the process
at each subsequent level. It does not check whether the split will lead to the lowest possible impurity
several levels down. A greedy algorithm often produces a solution that's reasonably good but not guaranteed to be
optimal. Unfortunately, finding the optimal tree requires $O(\exp(m))$ time, making the problem intractable even for
small training sets. This is why we must settle for a ``reasonably good'' solution. \v

Decision trees are intuitive, and their decisions are easy to interpret. They provide nice, simple classification
rules that can even be applied manually if need be. Their main drawback is that they are very sensitive to small
variations in the training data, hence they overfit the data. ``Random forests'' can limit this instability by
averaging predictions over many trees, as we will see right next.

\section{Ensemble Learning}

Suppose one poses a complex question to thousands of random people, then aggregate their answers. In many cases they
will find that this aggregated answer is better than an expert's answer. This is called the ``wisdom of the crowd''.

\bd[Wisdom Of The Crowd]
The \textbf{wisdom of the crowd} is the collective opinion of a diverse and independent group of individuals rather
than that of a single expert, with the main idea being that large groups of people are collectively smarter than
individual experts when it comes to problem-solving, decision-making, innovating, and predicting.
\ed

Similarly, if one aggregates the predictions of a group of predictors, they will often get better predictions than
with the best individual predictor. In machine learning terminology a group of predictors is called an ``ensemble''.

\bd[Ensemble]
In machine learning, an \textbf{ensemble} is a finite collection of models.
\ed

\bd[Ensemble Learning]
\textbf{Ensemble learning} is a machine learning technique that uses an ensemble of learning algorithms to obtain better
predictive performance than could be obtained from any of the constituent learning algorithms alone.
\ed

An ensemble learning model is itself a supervised learning algorithm, because it can be trained and then used to
make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not
necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown
to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to
overfit the training data more than a single model would, but in practice, some ensembles tend to reduce problems
related to overfitting of the training data. \v

Empirically, ensembles tend to yield better results when there is a significant diversity among the models they
consist of. Many ensembles, therefore, seek to promote diversity among the models they combine. Although perhaps
non-intuitive, more random algorithms can be used to produce a stronger ensemble than very deliberate algorithms.
Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques
that attempt to dumb-down the models in order to promote diversity. \v

While the number of component models of an ensemble has a great impact on the accuracy of prediction, there is a
limited number of studies addressing this problem. More recently, a theoretical framework suggested that there is an
ideal number of component models for an ensemble such that having more or less than this number  would deteriorate
the accuracy. This is called ``the law of diminishing returns in ensemble construction''. Their theoretical
framework shows that using the same number of independent component models as class labels gives the highest
accuracy. \v

Let us now define the two categories of ensemble learning: ``hard'' and ``soft voting ensembles''.

\bd[Hard Voting Ensemble]
An ensemble that aggregates the predictions of a collection of trained models and predicts the class that gets the most
votes is called a \textbf{hard voting ensemble}.
\ed

\bd[Soft Voting Ensemble]
An ensemble that aggregates the predictions, among with their corresponding probabilities, of a collection of trained
models and predicts the class with the highest class probability, averaged over all the individual models is called a
\textbf{soft voting ensemble}.
\ed

\fig{ensemble}{0.38}

Somewhat surprisingly, those voting ensembles often achieve a higher accuracy than the best model in the ensemble.
In fact, even if each model is a weak learner (meaning it does only slightly better than random guessing), the
ensemble can still be strong learner (achieving high accuracy), provided there are a sufficient number of weak
learners and they are sufficiently diverse.

\subsection{Bootstraping}

Both hard and soft voting ensembles work best when the models are as independent of one another as possible. The
most straight forward to get diverse models is to train them using very different algorithms. This increases the
chance that they will make very different types of errors, improving the ensemble's accuracy. \v

However, the most succesful approach to get diverse models, is to use the same training algorithm for every model
and perform random sampling, i.e.\ train them on different random subsets of the training set. The most popular way
of doing sampling in machine learning is the so called ``bootstraping''\footnote{For the sake of completeness, the
opposite of bootstraping is a process called ``pasting'' in which one performs random sampling without replacement.}.

\bd[Bootstraping]
\textbf{Bootstraping} is the process of randomly sampling the original dataset with replacement and generating a new
dataset of the same size as the original.
\ed

\bd[Bootstrap Dataset]
The \textbf{bootstrap dataset} is the dataset obtained by bootstraping an original dataset.
\ed

Given that the bootstrap dataset must be the same size as the original dataset, this means that it can have
duplicate objects. It also means that there is another dataset consisted of the remaining entries that did not
make it to the bootstrap dataset, called the ``out-of-bag dataset''.

\bd[Out-Of-Bag Dataset]
The \textbf{out-of-bag dataset} is the dataset of the remaining entries which were not in the bootstrap dataset.
\ed

\fig{ensemble2}{0.27}

\subsubsection{Bagging}

A special form of bootstraping is the so called ``bagging''.

\bd[Bagging]
\textbf{Bagging} (from \textbf{b}ootstrap \textbf{agg}regat\textbf{ing}) is the process of performing bootstraping
multiple times, training a model with each bootstrap dataset, and aggegating the predictions of each model.
\ed

\fig{ensemble3}{0.29}

The most common implementation of bagging is the ``Random Forest'' algorithm.

\subsubsection{Boosting}

\bd[Boosting]
\textbf{Boosting} is the process of incrementally building an ensemble by training each new model instance to emphasize
the training instances that previous models mis-classified.
\ed

\fig{ensemble4}{0.1}

The most common implementation of boosting are ``AdaBoost'' (short for Adaptive  Boosting) and ``Gradient Boosting''
algorithms that we will see right after random forests.

\section{Random Forest}

\bd[Random Forest]
\textbf{Random forest} is a hard voting, bagging, ensemble learning model for regression and classification that works
by combining a multitude of decision trees in training time and outputting the class that is the most voted of the
classes (classification) or mean prediction (regression) of the individual trees.
\ed

In random forest model, one trains a group of decision trees, each on a different random subset of the same size as
the training set (bagging). To make predictions, one obtains the predictions of all the individual trees, then
predicts the class that gets the most votes (hard voting). Despite its simplicity, random forest model is one of the
most powerful machine learning algorithms available today. \v

When one is growing a decision tree in a random forest, on top of bagging, at each node, they only select a random
subset of the features to be considered for splitting. That way, random forests introduce extra randomness when
growing trees since instead of searching for the very best feature when splitting a node, they search for it among a
random subset of features. The algorithm results in greater tree diversity, which trades a higher bias for a lower
variance, generally yielding an overall better model. As a result, random forests correct for decision trees' habit
of overfitting to their training set. \v

It is possible to make trees even more random by also using random thresholds for each feature rather than searching
for the best possible thresholds, like regular decision trees do. A forest of such extremely random trees is called
an ``Extremely Randomized Trees''.

\bd[Extremely Randomized Trees]
\textbf{Extremely randomized trees} is a random forest algorithm using random thresholds for each feature rather than
searching for the best possible thresholds, like regular decision trees.
\ed

Once again, this technique trades more bias for a lower variance. It also makes Extra-Trees much faster to train
than regular random forests, because finding the best possible threshold for each feature at every node is one of
the most time-consuming tasks of growing a tree.

\section{AdaBoost \& Gradient Boosting}