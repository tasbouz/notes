%! suppress = EscapeUnderscore
%! suppress = EscapeUnderscore
%! suppress = EncloseWithLeftRight
Supervised learning is one of the four basic categories of machine learning, and it consists of a family of models
and techniques that we will introduce in this chapter. First let's start with a formal definition of supervised
learning.

\section{Basic Definitions}

\bd[Supervised Learning]
\textbf{Supervised learning} is the machine learning task of learning a function that maps an input to an output
based on examples of ``input - output'' pairs called a ``training set''.
\ed

\fig{supervisedmodel}{0.6}

Some more specific notation that we will be using throughout supervised learning:
\bit
\item Input variables, or attributes, or features: $x$.
\item The $i^{th}$ feature: $x_{i}$.
\item The $j^{th}$ training example: $x^{(j)}$.
\item The $i^{th}$ feature of the $j^{th}$ training example: $x_{i}^{(j)}$.
\item Output variables, or targets, or classes, or labels: $y$.
\item The $i^{th}$ target: $y^{(i)}$.
\item Total number of training examples: $m$.
\item Total number of features: $n$.
\eit

Now let's dive in the models and techniques of supervised learning, starting with one of the most basic ones called
``linear regression''.

\section{Linear Regression}

\bd[Linear Regression]
\textbf{Linear regression} is a linear approach to modelling the relationship between a dependent variable (target)
and one or more independent variables (features).
\ed

\bd[Simple / Multiple Linear Regression]
When there is only one independent variable (feature) then the model is called \textbf{simple linear regression}. For
more than one independent variables (features) the process is called \textbf{multiple linear regression}.
\ed

\bd[Univariate / Multivariate Linear Regression]
When only one dependent variable (target) is predicted then the model is called \textbf{univariate linear regression}.
For more than one correlated, dependent variables (targets) being predicted, the process is called
\textbf{multivariate linear regression}. \ed

In linear regression the hypothesis function $h$ is a linear combinations of the features:
\bse
h(x) = w_0 + w_{1} x_1 + \ldots + w_n x_n
\ese

where $w_0$ is called ``bias'' or ``intercept'' and the rest $w_i$'s are called ``weights''. We usually refer
to weights and bias as the ``parameters'' of the regression (or the model) and they are the ones that we try to
determine through the training examples by using a learning algorithm. Once we find them then $h$ is ready to predict
new inputs with unknown outcomes.

\fig{linearregression}{0.4}

\v

It is a usual procedure to define $x_0 = 1$, so the linear regression hypothesis function can be rewritten as:
\bse
h(x) = w_0 x_0 + w_1 x_1 + \ldots + w_n x_n = \sum_{i=0}^{n} w_i x_i
\ese

Moreover, by defining the feature vector $\boldsymbol{x}$ and parameter vector $\boldsymbol{w}$ as:
\bse
\boldsymbol{x} = \begin{bmatrix} x_{0} \\ x_{1} \\ \vdots \\ x_{n} \end{bmatrix}, \qquad
\boldsymbol{w} = \begin{bmatrix} w_{0} \\ w_{1} \\ \vdots \\ w_{n} \end{bmatrix}
\ese

we can rewrite the linear regression the hypothesis function in the very simple form of:
\bse
h(x) = \boldsymbol{w}^{\intercal} \boldsymbol{x}
\ese

Notice that linear regression does not ``allow'' any polynomial terms of second or higher degree, thus the naming.
However, what if our data is more complex than a straight line? Surprisingly, we can use a linear model to fit
nonlinear data. The way to do this is to add powers of each feature as new features, and then move on with linear
regression. This technique is called ``polynomial regression''. \v

Now that we have a hypothesis function, we need a rule in order to be able to find the parameters $\boldsymbol{w}$.
This rule can be obtained through the probabilistic interpretation of linear regression. \v

More precisely, after having obtained the parameters $\boldsymbol{w}$, the hypothesis will fit the data in the best
possible way but, since as we said we are dealing with probabilistic systems, we will still have some errors
$\epsilon$. In other words, for each training example the following formula will apply:

\bse
y^{(i)} = h(x^{(i)}) + \epsilon^{(i)} = \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + \epsilon^{(i)}
\ese

where $\boldsymbol{x}^{(i)}$ is the corresponding training example feature vector:
\bse
\boldsymbol{x}^{(i)} = \begin{bmatrix} x_{0}^{(i)} \\ x_{1}^{(i)} \\ \vdots \\ x_{n}^{(i)} \end{bmatrix}
\ese

At this point we will make one assumption which needs to be valid in order for the linear regression to be valid.
Namely, we assume that \textbf{the errors $ \epsilon^{(i)}$ are independent and identically distributed following a
normal distribution with mean 0 and variance $\sigma^2$}:
\bse
\epsilon^{(i)} \sim N(0, \sigma^2)
\ese

which means that the probability distribution of the errors is given by:
\bse
P( \epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi \sigma^2}} exp \Big( - \frac{(\epsilon^{(i)})^{2}}{2 \sigma^2} \Big)
\ese

From the assumption of the errors follows:
\begin{align*}
&y^{(i)} = \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + \epsilon^{(i)} \Rightarrow \\ &\epsilon^{(i)} =
y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}
\end{align*}

By substituting the error back to the probability:
{\setlength{\jot}{10pt}
\begin{align*}
&P( \epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi \sigma^2}}
\cdot exp \Big( - \frac{(\epsilon^{(i)})^2}{2 \sigma^2} \Big) \Rightarrow \\
& P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w}) = \frac{1}{\sqrt{2 \pi \sigma^2}}
\cdot exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big)
\end{align*}}

In other words we get that the conditional distribution of $y^{(i)}$ given $\boldsymbol{x}^{(i)}$ and
$\boldsymbol{w}$ is a normal distribution with mean $\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}$ and variance
$\sigma^2$:
\bse
y^{(i)} \sim N(\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}, \sigma^2)
\ese

\v

Given the probability distribution of $y^{(i)}$ as a function of the parameters, we can now use the principle of
maximum likelihood that we developed in parametric inference chapter, in order to find the rule that will give us the
best parameters $\boldsymbol{w}$. \v

For the likelihood we get:
\bse
\mathcal{L} (\boldsymbol{w} |y ) = P(y^{(1)}, y^{(2)}, \ldots, y^{(m)} | \boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)},
\ldots, \boldsymbol{x}^{(m)} ; \boldsymbol{w}) = \prod_{i=1}^{m} P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w})
\ese

where we used the fact that $\epsilon^{(i)}$ are independent. \v

By substituting the probability:
\bse
\mathcal{L} (\boldsymbol{w} |y ) = \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot exp \Big( - \frac{(y^{(i)} -
\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big)
\ese

Subsequently, for the log-likelihood:
{\setlength{\jot}{10pt}
\begin{align*}
l (\boldsymbol{w} |y ) &= \ln \mathcal{L} (\boldsymbol{w} |y ) \\
&= \ln \Big[ \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot
exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big) \Big] \\
&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot
exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big) \Big] \\
&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \Big] + \sum_{i=1}^{m}
\ln \Big[ exp \Big( - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big] \Big) \\
&= \sum_{i=1}^{m} \ln \Big[ \frac{1}{\sqrt{2 \pi \sigma^2}} \Big] +
\sum_{i=1}^{m} \Big[ - \frac{(y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}{2 \sigma^2} \Big]
\end{align*}}

According to the principle of maximum likelihood, the best parameters can be found by maximizing the log-likelihood.
The first term of the log-likelihood is just a constant term, so it does not contribute at all to the maximization,
and the same holds for the denominator of the second term. Hence:
{\setlength{\jot}{10pt}
\begin{align*}
\boldsymbol{w} &= \argmax_{\boldsymbol{w}} [ l (\boldsymbol{w} |y )] \\
&= \argmax_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} ( - (y^{(i)}
- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}) ^2) \Big] \\
&= \argmax_{\boldsymbol{w}} \Big[ - \sum_{i=1}^{m} (y^{(i)}
- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2) \Big]\\
&= \argmin_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} (y^{(i)}
- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2 \Big]
\end{align*}}

At this point we can formally define the following function.

\bd [Mean Squared Error Loss Function]
\textbf{Mean squared error loss function} (MSE) $J(\boldsymbol{w})$ is defined as:
\bse
J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2
\ese
\ed

\v

Hence, the principle of maximum likelihood translates to finding the parameters $\boldsymbol{w}$ that minimize the
MSE loss function. The intuition behind the minimization of the MSE loss function is straight forward since what we
are actually doing is minimizing the square of the errors between the prediction and the actual outcome (square
because only the magnitude of the error is important and not the sign). By minimizing as much as possible the errors
we will eventually get the best line that fits the data. \v

We can group together all training examples in one matrix and all training labels in one vector as: \v
\bse
X = \begin{bmatrix}
\left(\boldsymbol{x}^{(1)}\right)^{\intercal} \\
\left(\boldsymbol{x}^{(2)}\right)^{\intercal} \\
\vdots \\
\left(\boldsymbol{x}^{(m)}\right)^{\intercal}
\end{bmatrix} =
\begin{bmatrix}
x_{0}^{(1)} & x_{1}^{(1)} & \ldots & x_{n}^{(1)} \\
x_{0}^{(2)} & x_{1}^{(2)} & \ldots & x_{n}^{(2)} \\
\vdots & \vdots & \ddots & \ldots \\
x_{0}^{(m)} & x_{1}^{(m)} & \ldots & x_{n}^{(m)}
\end{bmatrix}, \qquad
\boldsymbol{y} = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix}
\ese

\v

By doing so then we can write the MSE loss function in the simple form of:
\bse
J(\boldsymbol{w}) = \frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^2 =
\frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^{\intercal} (X \boldsymbol{w} - \boldsymbol{y})
\ese

Beside the MSE loss function that is derived directly through the principal of maximum likelihood, in machine
learning is quite common to use some variations of MSE depending on the problem. Here we will introduce the most
basic of them.

\bd [Root Mean Squared Error Loss Function]
\textbf{Root mean squared error loss function} (RMSE) $J(\boldsymbol{w})$ is defined as:
\bse
J(\boldsymbol{w}) = \sqrt{MSE} = \sqrt{\frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} -
\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2}
\ese
\ed

RMSE is a frequently used measure of the differences between values predicted by a model and the values observed, and
it is probably the most easily interpreted statistic, since it has the same units as the data, so it is a better
measure of goodness of fit than a correlation coefficient. RMSE is simply the square root of the average of squared
errors, or in other words it is the average distance of a data point from the fitted line, measured along a vertical
line. The effect of each error on RMSE is proportional to the size of the squared error, thus larger errors have a
disproportionately large effect on RMSE. Consequently, RMSE is sensitive to outliers. \v

RMSE is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the
data. In general, a lower RMSE is better than a higher one. However, comparisons across different types of data would
be invalid because the measure is dependent on the scale of the numbers used.

\bd [Mean Bias Error]
\textbf{Mean bias error loss function} (MBE) $J(\boldsymbol{w})$ is defined as:
\bse
J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})
\ese
\ed

MBE captures the average bias in the prediction and is usually not used as a measure of the model error as high
individual errors in prediction can also produce a low MBE. MBE is primarily used to estimate the average bias in the
model and to decide if any steps need to be taken to correct the model bias. MBE can convey useful information, but
should be interpreted cautiously because positive and negative errors will cancel out.

\bd [Mean Absolute Error]
\textbf{Mean absolute error loss function} (MAE) $J(\boldsymbol{w})$ is defined as:
\bse
J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} | y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} |
\ese
\ed

MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It's
the average over the test sample of the absolute differences between prediction and actual observation where all
individual differences have equal weight. If the absolute value is not taken (the signs of the errors are not
removed), the average error becomes the MBE\@.

\bd [Mean Absolute Percentage Error]
\textbf{Mean absolute percentage error loss function} (MAPE) $J(\boldsymbol{w})$ is defined as:
\bse
J(\boldsymbol{w}) = \frac{1}{2m} \sum_{i=1}^{m} \Big{|} \frac{y^{(i)} -
\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}}{y^{(i)}} \Big{|}
\ese
\ed

MAPE (also known as mean absolute percentage deviation (MAPD)) is the mean of the absolute percentage errors of
forecasts. Error is defined as actual or observed value minus the forecasted value. Percentage errors are summed
without regard to sign to compute MAPE. This measure is easy to understand because it provides the error in terms of
percentages. Also, because absolute percentage errors are used, the problem of positive and negative errors canceling
each other out is avoided. Consequently, MAPE has managerial appeal and is a measure commonly used in forecasting.
The smaller the MAPE the better the forecast.

\section{Optimization Techniques}

Given the MSE loss function (or any other loss function), the goal of machine learning is to optimize it (usually
minimize it) in order to obtain the best parameters that fit the data. Optimizing loss functions is one of the
biggest parts of machine learning, and we can do so with the so called ``optimization techniques''.

\bd [Optimization Techniques]
\textbf{Optimization techniques} are techniques used for finding the optimum solution or unconstrained maxima or
minima of continuous and differentiable functions. These are analytical methods and make use of differential calculus
in locating the optimal solution.
\ed

\subsection{Normal Equation}

Probably the most straight forward optimization technique is the so called ``normal equation''. Since we are
looking a minimum for $J(\boldsymbol{w})$ the natural thing to do, is to simply calculate the derivative with respect
to the parameter vector and then set it to zero (as we did when we introduced the principle of maximum likelihood). \v

It is more handy to use the vector form of MSE loss function, so for the derivative we get:
{\setlength{\jot}{10pt}
\begin{align*}
\nabla_{\boldsymbol{w}} J(\boldsymbol{w})
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w} -
\boldsymbol{y})^{\intercal} (X \boldsymbol{w} - \boldsymbol{y}) \Big] \\
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ \Big( (X \boldsymbol{w})^{\intercal} -
\boldsymbol{y}^{\intercal} \Big) \Big( X \boldsymbol{w} - \boldsymbol{y} \Big) \Big] \\
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w})^{\intercal} (X \boldsymbol{w}) -
(X\boldsymbol{w})^{\intercal} \boldsymbol{y} - \boldsymbol{y}^{\intercal}(X \boldsymbol{w}) +
boldsymbol{y}^{\intercal} \boldsymbol{y} \Big] \\
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ (X \boldsymbol{w})^{\intercal} (X \boldsymbol{w}) -
2 (X\boldsymbol{w})^{\intercal} \boldsymbol{y} + \boldsymbol{y}^{\intercal}\boldsymbol{y} \Big] \\
&= \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ \boldsymbol{w}^{\intercal} X^{\intercal} X \boldsymbol{w} -
2\boldsymbol{w}^{\intercal} X^{\intercal} \boldsymbol{y} +\boldsymbol{y}^{\intercal} \boldsymbol{y} \Big] \\
&= \frac{1}{2m} \Big[ 2 X^{\intercal} X \boldsymbol{w} - 2 X^{\intercal}\boldsymbol{y} \Big] \\
&= \frac{1}{m} \Big[ X^{\intercal} X \boldsymbol{w} - X^{\intercal}\boldsymbol{y} \Big]
\end{align*}}

By setting the derivative to 0 we obtain:
{\setlength{\jot}{10pt}
\begin{align*}
& \nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = 0 \Rightarrow \\
& \frac{1}{m} \Big[ X^{\intercal} X \boldsymbol{w} - X^{\intercal} \boldsymbol{y} \Big] = 0 \Rightarrow \\
& X^{\intercal} X \boldsymbol{w} - X^{\intercal} \boldsymbol{y} = 0 \Rightarrow \\
& X^{\intercal} X \boldsymbol{w} = X^{\intercal} \boldsymbol{y} \Rightarrow \\
& \underbrace{(X^{\intercal} X)^{-1} (X^{\intercal} X)}_{I} \boldsymbol{w} =
(X^{\intercal} X)^{-1} X^{\intercal} \boldsymbol{y} \Rightarrow \\
& \boldsymbol{w} = (X^{\intercal} X)^{-1} X^{\intercal} \boldsymbol{y}
\end{align*}}

This final expression is called ``Normal Equation'', and it is an exact analytical solution that gives the
parameter vector. \v

Despite the fact that normal equation gives an exact analytical result, computing the seemingly harmless inverse of
an $((n+1) \times m) \times (m \times (n+1) ) = (n+1) \times (n+1)$ matrix is, with today's most efficient computer
science algorithm, of cubic time complexity ( in other words, if you double the number of features, you multiply the
computation time by roughly $2^3$). This means that as the dimensions of $X$ increase (mainly the number of features), 
the amount of operations required to compute the final result increases in a cubic trend. If $X$ was rather small,
then using the normal equation would be feasible. \v

In practise, for the vast majority of any industrial application with large datasets, the normal equation would take
extremely, sometimes nonsensically, long. This is the reason why normal equation is almost never used. Now let's move
on the the most standard optimization technique used today called ``gradient descent''.

\subsection{Gradient Descent}

Gradient descent, and all its improvements and alternatives, is the most used optimization technique in machine
learning and deep learning. It is a generic optimization algorithm capable of finding optimal solutions to a wide
range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a
cost function by starting with some random initial values for the parameters and calculating the value of the loss
function based on them, and finally updating them based on the following relation:

\bd[Gradient Descent Update Rule]
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \alpha \nabla_{\boldsymbol{w}} J(\boldsymbol{w})
\ese
\ed

Since the derivative is positive when $J$ is upwards slopping and negative when it is downwards slopping, the minus
sign makes sure that we always update the parameters towards the direction that minimizes $J$. Once the minimum is
reached then $J$ is at a global optimum so the derivative is 0 and further updates are not possible. Gradient descent
is over and the best parameters have been found.

\vspace{-9pt}

\fig{gradientdescent}{0.22}

The hyperparameter $\alpha$ is called ``learning rate'' and defines how big or small steps we take after each
iteration of gradient descent. If $\alpha$ is too large, we might fail to find the minimum due to oscillations around
it. If $\alpha$ is too small then gradient descent might take too much time to reach the minimum of $J$. Tuning
learning rate in a ``right'' value is a topic by itself, and it is quite heavily researched today.

\bd[Learning Rate]
The \textbf{learning rate} is a tuning parameter in an optimization algorithm that determines the step size at each
iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired
information overrides old information, it metaphorically represents the speed at which a machine learning model
``learns''.
\ed

\vspace{-12pt}

\fig{alpha}{0.7}

Of course, not all cost functions look like nice, regular bowls. There may be holes, ridges, plateaus, and all sorts
of irregular terrains, making convergence to the minimum difficult. In general gradient descent works best with
convex functions, where if you pick any two points on the curve of a convex function, the line segment joining them
never crosses the curve. This implies that there are no local minima, just one global minimum. They are also
continuous functions with slopes that never changes abruptly. Hence, in convex functions gradient descent is
guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not
too high). \v

Coming back to our case, let's find the update rule specifically for linear regression. Fortunately, the MSE cost
function of linear regression happens to be a convex function. The only thing missing is the derivative of MSE loss
function $J$. However, in the previous chapter with normal equation we showed that:
\bse
\nabla_{\boldsymbol{w}} J(\boldsymbol{w})
= \frac{1}{m} \Big(X^{\intercal} X\boldsymbol{w} - X^{\intercal} \boldsymbol{y} \Big)
= \frac{1}{m} X^{\intercal} \Big( X \boldsymbol{w}-\boldsymbol{y} \Big)
\ese

\v

Hence, the update rule reads:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal} \Big( X \boldsymbol{w} -\boldsymbol{y} \Big)
\ese

\v

As we already mentioned there are many improvements and modified algorithms based on gradient descent philosophy. We
are going to cover a lot of them in these notes. For now, let's start with 3 basics versions of gradient descent.
\bit
\item \textbf{Batch Gradient Descent} Batch gradient descent is actually the one we just saw. As we see in gradient
descent, the whole training set $X$ is used in order to make just one update of the parameters. We usually refer to
the whole training set as the ``batch''. For that reason, the usual terminology for what we have seen so far is
batch gradient descent, meaning that the whole batch is used in order to update the parameter. \v

Notice that in batch gradient descent the algorithm goes over the entire data once before updating the parameters
because this is the true gradient of the loss as derived earlier (sum of the gradients of the losses corresponding to
each data point). Since there are no approximations each step guarantees that the loss will decrease. However, it
carries a flipside. As the number of training examples grows the dimensions of $X$ grows and using the whole training
set for every iteration becomes computationally expensive. For really large dataset with a million of points in the
training data, in order to make just one update to the parameters the algorithm needs to make a million of calculations.
Obviously this is very slow.
\item \textbf{Mini-Batch Gradient Descent} In mini-batch gradient descent we divide the whole dataset to $b$ subsets
of $\frac{m}{b}$ training examples each, called ``mini-batches'', and we update the parameters using each of the
mini-batches in each iteration.

\item \textbf{Stochastic Gradient Descent} In stochastic gradient descent, which can be seen as an extreme case of
mini-batch gradient descent where $b=m$, we only use one training example per iteration to update the parameters. In
every iteration we are estimating the total gradient based on just one single data point that we pick randomly hence
the name ``stochastic''. One has to keep in mind that since this is an approximation there is no guarantee that each
step will decrease the loss function. \v

In fact, during stochastic gradient descent one observes many oscillations in the loss function due to the greediness
of the decisions. Each point is trying to push the parameters in a direction most favourable to it (without being
aware of how this affects other points). A parameter update which is locally favourable to one point may harm other
points (its almost as if the data points are competing with each other). This fact provides no guarantee that each
local greedy move reduces the global error. However, obviously, is much much faster that batch and mini-batch
gradient descent. \v

In a way, mini-batch gradient descent tries to strike a balance between the goodness of batch gradient descent and
speed of stochastic gradient descent.
\eit

In general, in most supervised learning models batch gradient descent works just fine so we don't need the
alternative techniques we just introduced. However, in more complicated models, such as deep learning models, these
techniques can be really useful. For this reason we will meet again these techniques in the chapter of deep learning.

\section{Feature Engineering}

There are many possible features to use in your model. The process of choosing what information to use and how to
extract this information into a format usable by your machine learning models is called ``feature engineering''.

\bd[Feature Engineering]
\textbf{Feature engineering} is the process of choosing what information (features) to use and how to extract this
information into a format usable by a machine learning model.
\ed

For complex tasks the number of features used can go up to millions while, for domain-specific tasks you might need
subject matter expertise to be able to come up with useful features. We usually refer to the set of all possible
features as the ``feature space''.

\bd[Feature Space]
The \textbf{feature space} is the $N$ dimensional space defined by the set of $N$ possible features, not including
the target.
\ed

In general, because of the importance and the ubiquity of feature engineering in machine learning projects, there
have been many techniques developed to streamline the process. In this section we will introduce some of the most
common techniques ones.

\subsection{Feature Selection}

\bd[Feature Selection]
\textbf{Feature selection} is the process of reducing the size of feature space by selecting a subset of relevant
features for use in model construction.
\ed

Feature selection techniques are used for several reasons like, simplifying models by reducing complexity to make
them easier to interpret, avoiding the curse of dimensionality, and enhancing generalization by reducing overfitting.
There are also production-related reasons like reducing resource requirements, speeding up predicting and training
times, and minimizing both training and inference costs. \v

There are some general guidelines that can be followed when selecting features, like relevance and independence.
Starting with relevance, we want to remove features that don't influence the outcome. In general it's a good practice
to explore and visualize the data in order to identify useless information. In this step we can drop the attributes
that provide no useful information for the task, drop variables that have a very low variation (i.e.\ not too much
information), and drop variables that have very low correlation with the target. \v

Moving on to independence, we want to remove features that are highly correlated with each other. The reason is that
highly correlated features provide redundant information and they can cause overfitting. In order to do so, we use
a set of different methods called ``filter methods'', ``wrapper methods'', and ``embedded methods''.

\bd[Filter Methods]
\textbf{Filter methods} are feature selection methods that use statistical techniques to evaluate the relationship
between each feature and the target variable. They are generally univariate and consider the feature independently,
or with regard to the dependent variable.
\ed

The most common filter methods are:
\bit
\item \textbf{Pearson Correlation}: Used for linear correlation between the features.
\item \textbf{Kendall Tau Rank Correlation}: Used for monotonic relationships and small sample sizes.
\item \textbf{Spearman's Rank Correlation Coefficient}: Used for monotonic relationships.
\item \textbf{Mutual Information}: Used for non-linear relationships.
\item \textbf{F-Test}: Used for calculating the F-statistic between categorical features and the target.
\item \textbf{Chi-Squared Test}: Used for calculating the chi-squared statistic between categorical features and the
target.
\eit

\bd[Wrapper Methods]
\textbf{Wrapper methods} are feature selection methods that use a machine learning algorithm to evaluate the
performance of the model with a given subset of features.
\ed

Wrapper methods are generally computationally expensive and consider the selection of a set of features as a search
problem. The most common wrapper methods are:
\bit
\item \textbf{Forward Selection}: Starts with an empty set of features and adds features one by one until the
termination criterion is reached.
\item \textbf{Backward Elimination}: Starts with the full set of features and removes features one by one until the
termination criterion is reached.
\item \textbf{Recursive Feature Elimination}: Starts with the full set of features and recursively removes features
one by one until the termination criterion is reached.
\eit

\bd[Embedded Methods]
\textbf{Embedded methods} are feature selection methods that use machine learning algorithms that have built-in
feature selection methods.
\ed

Embedded methods are generally computationally expensive and consider the selection of a set of features as part of
the learning process.

\subsubsection{Missing Values}

One of the first things you might notice when dealing with data in production is that some values are missing.
However, not all types of missing values are equal. There are three types of missing values.

\bd[Missing Not At Random (MNAR)]
\textbf{Missing Not At Random} (\textbf{MNAR}) is when the reason a value is missing is because of the nature of the
value itself.
\ed

\bd[Missing At Random (MAR)]
\textbf{Missing At Random} (\textbf{MAR}) is when the reason a value is missing due to another observed variable.
\ed

\bd[Missing Completely At Random (MCAR)]
\textbf{Missing Completely At Random} (\textbf{MCAR}) is when there's no pattern in when the value is missing.
\ed

No matter, which of the three types of missing values you have to deal with, when encountering missing values, you
have to take some actions, otherwise the model will fail to be trained. The two thing that you can do is to either
remove the missing values (deletion) or fill in the missing values with certain values (imputation).

\bd[Deletion]
\textbf{Deletion} is the technique of removing missing values completely.
\ed

Deletion works well when variables have a very high percentage of missing values. \v

One way to delete is column deletion: if a variable has too many missing values, just remove that variable. The
drawback of this approach is that you might remove important information and reduce the accuracy of your model. \v

Another way to delete is row deletion: if a sample has missing value(s), just remove that sample. This method can
work when the missing values are completely at random (MCAR) and the number of examples with missing values is small.
However, removing rows of data can also remove important information that your model needs to make predictions,
especially if the missing values are not at random (MNAR). On top of that, removing rows of data can create biases in
your model, especially if the missing values are at random (MAR). \v

Even though deletion is tempting because it's easy to do, deleting data can lead to losing important information and
introduce biases into your model. If you don't want to delete missing values, you will have to impute them, which
means ``fill them with certain values''.

\bd[Imputation]
\textbf{Imputation} is the technique of filling in missing values with certain values.
\ed

Deciding which ``certain values'' to use is the hard part. One common practice is to fill in missing values with
their defaults. Another common practice is to fill in missing values with a statistic such as the mean, median, or
mode. Both practices work well in many cases, but sometimes they can cause hair-pulling bugs. In general, you want to
avoid filling missing values with possible values. \v

Multiple techniques might be used at the same time or in sequence to handle missing values for a particular set of
data. Regardless of what techniques you use, one thing is certain: there is no perfect way to handle missing values.
With deletion, you risk losing important information or accentuating biases. With imputation, you risk injecting your
own bias into and adding noise to your data, or worse, data leakage.

\bd[Data Leakage]
\textbf{Data leakage} refers to the phenomenon when a form of the label leaks into the set of features used for making
predictions, and this same information is not available during inference.
\ed

Data leakage during imputation might occur if the mean or median is calculated using entire data instead of just the
train split. This type of leakage can be prevented by using only statistics from the train split to fill in missing
values in all the splits.

\subsection{Feature Scaling}

Machine learning algorithms don't perform well when the input numerical attributes have very different scales. Hence,
before inputting features into models, it's important to scale them to be similar ranges. This process is called
``feature scaling''.

\bd[Feature Scaling]
\textbf{Feature scaling} is the process of scaling the features to get all attributes to have the same scale.
\ed

This is one of the simplest things you can do that often results in a performance boost for your model. More
specificaly some of the benefits of feature scaling is that it helps neural nets converge faster, avoids null errors
during training and the model learns the right weights for each feature. On the other hand, neglecting to do so can
cause your model to make gibberish predictions. \v

There are two common ways to get all attributes to have the same scale: ``normalization'' and ``standardization''.

\bd[Normalization]
\textbf{Normalization} is the process where the values are shifted and rescaled so that they end up ranging from 0 to 1
following the formula:
\bse
x^\prime = \frac{x - \min(x)}{\max(x) - \min(x)}
\ese
\ed

Normalization is also called ``min-max scaling''. It is very sensitive to outliers in the data. If there are
outliers in the data, they will be mapped to a very small or large value, and the rest of the data will be mapped to
a very small interval.

\fig{normalization}{0.5}

\bd[Standardization]
\textbf{Standardization} is the process where the values are shifted and rescaled so that they end up having zero mean
and unit variance following the formula:
\bse
x^\prime = \frac{x - \bar{x}}{\sigma}
\ese
\ed

\fig{standardization}{0.5}

Unlike normalization, standardization does not bound values to a specific range, which may be a problem for some
algorithms. However, standardization is much less affected by outliers. \v

As it is obvious by now, scaling requires global statistics like min, max, mean, and variance of your data. One
common mistake is to use the entire training data to generate global statistics before splitting it into different
splits, leaking the mean and variance of the test samples into the training process, allowing a model to adjust its
predictions for the test samples. This information isn't available in production, so the model's performance will
likely degrade. This type of leakage is similar to the type of leakage caused by imputation, and in order to be
avoided, always split your data first before scaling, then use the statistics from the train split to scale all the
splits. Some even suggest that we split our data before any exploratory data analysis and data processing, so that we
don't accidentally gain information about the test split. \v

Last but not least, another technique usually used in feature scaling is the so called ``discretization''.

\bd[Discretization]
\textbf{Discretization} is the process of turning a continuous feature into a discrete feature.
\ed

\fig{cvsd.png}{0.5}

With discretization instead of having to learn an infinite number of possible incomes, our model can focus on
learning only a certain amount of categories (buckets), which is a much easier task to learn. This technique is
supposed to be more helpful with limited training data.

\subsection{Feature Crossing}

\bd[Feature Crossing]
\textbf{Feature crossing} is the technique to combine two or more features to generate new features.
\ed

Feature crossing technique is useful to model the nonlinear relationships between features hence, it's essential for
models that can't learn or are bad at learning nonlinear relationships, such as linear regression (that we just saw),
logistic regression (that we will see right after), and tree-based models (that we will see in the end of this chapter).
Feature crossing is less important in neural networks (that we will see in next chapters) due to their natures of being
able to learn nonlinear relationships. \v

A caveat of feature crossing is that it can make your feature space blow up. Another caveat is that because feature
crossing increases the number of features models use, it can make models overfit to the training data.

\subsection{Feature Importance \& Generalization}

Generally, adding more features leads to better model performance. Usually the list of features used for a model in
production only grows over time. However, more features doesn't always mean better model performance. Having too many
features can be bad both during training and serving your model for many reasons. \v

First of all, the more features you have, the more opportunities there are for data leakage. Secondly, too many
features can cause overfitting. From a technical perspective, too many features can increase memory required to serve
a model, which, in turn, might require you to use a more expensive machine/instance to serve your model. Also, too
many features can increase inference latency when doing predictions. Last but not least, useless features become
technical debts. Whenever your data pipeline changes, all the affected features need to be adjusted accordingly. \v

In general, there are two factors you might want to consider when evaluating whether a feature is good for a model:
``feature importance'' and ``feature generalization''.

\bd[Feature Importance]
\textbf{Feature importance} is the measure of how much information a feature provides to a model.
\ed

\bd[Feature Generalization]
\textbf{Feature generalization} is the measure of how well a model performs on data it hasn't seen before.
\ed

\section{Logistic Regression}

\bd[Logistic Regression]
\textbf{Logistic regression} (or classification) is a statistical model that is used for the classification of a
discrete dependent variable (target) to a specific label (class) from a set of labels (classes).
\ed

\bd[Binary / Multinomial Classification]
\textbf{Binary} classification is the problem of classifying instances into one of two classes. Classifying instances
into three or more classes is called \textbf{multinomial} (or multiclass) classification.
\ed

\bd[Multilabel Classification]
\textbf{Multilabel} classification is a variant of the classification problem where multiple labels may be assigned
to each instance. Multilabel classification is a generalization of multinomial classification, since there is no
constraint on how many of the classes the instance can be assigned to. \ed

\v

\fig{classification}{0.55}

For now we will focus on binary classification, and later we will make a comment on multinomial classification. \v

In binary classification, since the output is binary and can take only the values 0 or 1, the hypothesis function of
the linear regression is not a valid approximator for logistic regression since it produces a continuous set of
outputs. So our first step is to find a suitable hypothesis function $h$ for logistic regression. The hypothesis
function that we actually use in logistic regression is the logistic function (part of a broader family called
sigmoid functions that we will introduce in deep learning chapter) and it is given by:
\bse
h(\boldsymbol{x}) = \frac{1}{1 + exp(- \boldsymbol{w}^{\intercal} \boldsymbol{x})}
\ese

\v

From now on, in order to save space, we will write $h(\boldsymbol{x})$ instead of the actual expression for the
logistic function.

\vspace{-5pt}

\fig{sigmoid}{0.18}

\v

Hence, in logistic regression, the hypothesis function computes a weighted sum of the input features (plus a bias
term), but instead of outputting the result directly like the linear regression hypothesis function does, it outputs
the logistic of this result. Notice that $h(\boldsymbol{x}) < 0.5$ when $\boldsymbol{w}^{\intercal} \boldsymbol{x} <
0$, and $h(\boldsymbol{x}) \geq 0.5$ when $\boldsymbol{w}^{\intercal} \boldsymbol{x} \geq 0$, so a logistic
regression model predicts $1$ if $\boldsymbol{w}^{\intercal} \boldsymbol{x}$ is positive and 0 if it is negative.
(The argument of the logistic function is often called ``logit''). \v

It is worth mentioning that one can alter the discrimination threshold of 0.5 to any value between 0 and 1. This is
quite usual in logistic regression models when obtaining the correct class is more important than a possible
misclassification. \v

Now let's try to give a meaning to the hypothesis function. As we can see the logistic function produces results in
the interval $[0,1]$. It is quite close to what we need, but not exactly so, since we do not need all the values
between 0 and 1. For this reason we will interpret the hypothesis function of logistic regression as a probability
measure of the target to belong to class 1. The closest to 0 the hypothesis function, the more unlikely for the
target to belong in class 1 (Hence, it belongs to class 0) and the closest to 1 the more likely to belong to the class
1. Hence, by defining a threshold (say at 0.5) the idea is that for $h (\boldsymbol{x}) <0.5$ the algorithm will
predict 0 and for $h(\boldsymbol{x}) \geq 0.5$ the algorithm will predict 1. Based on this intuition, we can write:
\bse
h(\boldsymbol{x}) = P(y=1 | \boldsymbol{x}; \boldsymbol{w})
\ese

Of course, since the output must be either 0 or 1 we get:
\begin{align*}
& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) + P(y=1 | \boldsymbol{x}; \boldsymbol{w}) = 1 \Rightarrow \\
& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) = 1 - P(y=1 | \boldsymbol{x}; \boldsymbol{w}) \Rightarrow \\
& P(y=0 | \boldsymbol{x}; \boldsymbol{w}) = 1 - h(\boldsymbol{x})
\end{align*}

We can combine these two probabilities in one in the following way:
\bse
P(y | \boldsymbol{x}; \boldsymbol{w}) = h(\boldsymbol{x})^y \cdot (1 - h (\boldsymbol{x}))^{(1-y)}
\ese

So in logistic regression the output follows a Bernoulli distribution with parameter $h(\boldsymbol{x})$. Now that we
have a probability distribution, similarly to the linear regression, we can use the principle of the maximum
likelihood in order to obtain the best parameters that maximize the likelihood. Thus, we will obtain the loss
function for the logistic regression case. \v

By making again the assumption that we are dealing with independent and identically distributed random variables, for
the likelihood is:
\bse
\mathcal{L} (\boldsymbol{w} |y ) = P(y^{(1)}, y^{(2)}, \ldots, y^{(m)} | \boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)},
\ldots, \boldsymbol{x}^{(m)} ; \boldsymbol{w}) = \prod_{i=1}^{m} P(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{w})
\ese

By substituting the probability:
\bse
\mathcal{L} (\boldsymbol{w} |y )
= \prod_{i=1}^{m} h(\boldsymbol{x}^{(i)})^{y^{(i)}} \cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})}
\ese

Subsequently for the log-likelihood:
{\setlength{\jot}{10pt}
\begin{align*}
l (\boldsymbol{w} |y ) &= \ln \mathcal{L} (\boldsymbol{w} |y ) \\
&= \ln \Big[ \prod_{i=1}^{m} h(\boldsymbol{x}^{(i)})^{y^{(i)}}
\cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big] \\
&= \sum_{i=1}^{m} \ln \Big[ h(\boldsymbol{x}^{(i)})^{y^{(i)}}
\cdot (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big] \\
&= \sum_{i=1}^{m} \Big[ \ln \Big( h(\boldsymbol{x}^{(i)})^{y^{(i)}}\Big)
+ \ln \Big( (1 - h(\boldsymbol{x}^{(i)}))^{(1 - {y^{(i)}})} \Big) \Big] \\
&= \sum_{i=1}^{m} \Big[ y^{(i)} \cdot \ln h(\boldsymbol{x}^{(i)})
+ (1 - {y^{(i)}}) \cdot \ln (1 - h (\boldsymbol{x}^{(i)})) \Big]
\end{align*}}

Once again, according to the principle of maximum likelihood, the best parameters can be found by maximizing the
log-likelihood. Hence:
{\setlength{\jot}{10pt}
\begin{align*}
\boldsymbol{w} &= \argmax_{\boldsymbol{w}} [l (\boldsymbol{w} |y )] \\
&= \argmax_{\boldsymbol{w}} \Big[ \sum_{i=1}^{m} \Big( (y^{(i)}
\cdot \ln h(\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \Big] \\
&= \argmin_{\boldsymbol{w}} \Big[ - \sum_{i=1}^{m} \Big( y^{(i)}
\cdot \ln h(\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \Big]
\end{align*}}

where in the last step we did the usual trick of multiplying the whole expression with a minus sign and switching the
optimization problem from maximizing the quantity to minimizing it. \v

At this point we can formally define the following logistic regression loss function.

\bd [Cross Entropy Loss Function]
\textbf{Cross entropy loss function} (also called log loss) is defined as:
\bse
J(\boldsymbol{w}) = - \frac{1}{m} \sum_{i=1}^{m} \Big( y^{(i)} \cdot \ln h (\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}})
\cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
\ese
\ed

The cross entropy is the loss function of the logistic regression in the similar way where MSE loss function is the
loss function for linear regression. The name ``cross entropy'' comes from the definition of cross entropy which is
the average amount of information needed to identify an event between two probability distributions $p$ and $q$ over
the same underlying set of events. \v

Notice that the only possible values of $y^{(i)}$ is 0 or 1. This means that in any case, one of the terms $y^{(i)}$
or $(1-y^{(i)})$ in $J$ will vanish and the other one will be equal to 1. So in the end the only thing that is
actually part of the loss is the logarithm of the hypothesis function, which given that the hypothesis function is a
logistic function which is always between 0 and 1, the logarithm is always negative. With the overall negative sign
the loss turns positive and this is what we want to minimize. \v

We are not going to write a vectorized form for the cross entropy loss function for two reasons. First of all, not
all matrices have a logarithm and those matrices that do have a logarithm may have more than one logarithm. So one
has to be careful when uses the vectorized form of logistic regression because it carries logarithms of matrices.
Secondly, derivatives of logarithms of non square matrices sometimes are not defined. Since we need to calculate the
derivative of $J$ we might get problems. For this reason we will use the non vectorized form for the calculations,
however we will express the final result in a vectorized form. \v

As in the linear case, the principle of maximum likelihood leads to the minimization of the cross entropy loss
function in order to obtain the best parameters. We will examine the same techniques that we developed for the
gradient descent in the linear case, i.e.\ normal equation and gradient descent.

\subsection{Normal Equation}

As we already mentioned, since we want to minimize a function the straight forward way of doing that is to calculate
the derivative and then set it to 0. However, in the case of logistic regression the normal equation does not apply
since there is no closed analytical solution of $\nabla_{\boldsymbol{w}} J (\boldsymbol{w})=0$. The only way for
solving the optimization problem is through gradient descent.

\subsection{Gradient Descent}

Gradient descent works fine in logistic regression and, as with the MSE cost function in linear regression, it also
happens to be a convex function. We will follow exactly the same steps as in linear regression:
\begingroup
\allowdisplaybreaks
{\setlength{\jot}{10pt}
\begin{align*}
\nabla_{\boldsymbol{w}} J(\boldsymbol{w})
&= - \frac{1}{m} \nabla_{\boldsymbol{w}} \Big[\sum_{i=1}^{m} \Big( y^{(i)} \cdot \ln h(\boldsymbol{x}^{(i)})
+ (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)}))\Big) \Big] \\
&= - \frac{1}{m} \sum_{i=1}^{m} \Big( y^{(i)} \cdot \nabla_{\boldsymbol{w}} \ln h(\boldsymbol{x}^{(i)})
+ (1 - {y^{(i)}}) \cdot \nabla_{\boldsymbol{w}} \ln (1 - h(\boldsymbol{x}^{(i)})) \Big) \\
&= - \frac{1}{m} \sum_{i=1}^{m} \Big( y^{(i)} \cdot \frac{\nabla_{\boldsymbol{w}}
h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})} - (1 - {y^{(i)}}) \cdot \frac{\nabla_{\boldsymbol{w}}
h(\boldsymbol{x}^{(i)})}{1 - h(\boldsymbol{x}^{(i)})} \Big)\\
&= - \frac{1}{m} \sum_{i=1}^{m} \Big( \frac{y^{(i)}}{h(\boldsymbol{x}^{(i)})}
- \frac{1 - {y^{(i)}}}{1 - h(\boldsymbol{x}^{(i)})} \Big) \cdot \nabla_{\boldsymbol{w}} h(\boldsymbol{x}^{(i)}) \\
&= - \frac{1}{m} \sum_{i=1}^{m} \Big( \frac{y^{(i)} \cdot (1 - h(\boldsymbol{x}^{(i)}) - (1 - {y^{(i)}})
\cdot h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)}) \cdot (1- h(\boldsymbol{x}^{(i)}))} \Big)
\cdot \nabla_{\boldsymbol{w}} \Big[ \frac{1}{1 + exp(- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})} \Big] \\
&= - \frac{1}{m} \sum_{i=1}^{m} \Big( \frac{y^{(i)} - y^{(i)} \cdot h(\boldsymbol{x}^{(i)})
- h(\boldsymbol{x}^{(i)}) + {y^{(i)}} \cdot h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})
\cdot (1 - h(\boldsymbol{x}^{(i)}))} \Big) \cdot \Big( \frac{(-1)
\cdot exp(- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})
\cdot (-\boldsymbol{x}^{(i)})}{(1 + exp(-\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}))^2} \Big) \\
&= - \frac{1}{m} \sum_{i=1}^{m} \Big( \frac{y^{(i)} - h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})
\cdot (1 - h(\boldsymbol{x}^{(i)}))} \Big) \cdot \Big( h(\boldsymbol{x}^{(i)})^2
\cdot \frac{1 - h(\boldsymbol{x}^{(i)})}{h(\boldsymbol{x}^{(i)})} \cdot \boldsymbol{x}^{(i)} \Big) \\
&= \frac{1}{m} \sum_{i=1}^{m} \frac{h(\boldsymbol{x}^{(i)}) - y^{(i)})}{h(\boldsymbol{x}^{(i)})
\cdot (1 - h (\boldsymbol{x}^{(i)}))} \cdot h(\boldsymbol{x}^{(i)})
\cdot (1- h(\boldsymbol{x}^{(i)}))\cdot\boldsymbol{x}^{(i)} \\
&= \frac{1}{m} \sum_{i=1}^{m} (h(\boldsymbol{x}^{(i)}) - y^{(i)}) \cdot \boldsymbol{x}^{(i)}
\end{align*}}
\endgroup

Hence, the update rule reads:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} \sum_{i=1}^{m}
\Big( \frac{1}{1 + exp(- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})} - y^{(i)} \Big) \cdot \boldsymbol{x}^{(i)}
\ese

\v

Or in vectorized form:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal}
\Big( \frac{1}{1+exp(-X \boldsymbol{w})}-\boldsymbol{y} \Big)
\ese

\v

At this point, notice that gradient descent can be generalized into one model for both linear and logistic regression
since the update rule for both of them can be be written in one coherent way as:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal} \Big( h(X)-\boldsymbol{y} \Big)
\ese

where one has to use either MSE loss function or cross entropy loss function depending on the regression problem.

\subsection{Multinomial Classification}

Multinomial (or multiclass) classification is the problem of classifying instances into one of three or more classes.
(Multinomial classification should not be confused with multilabel classification, where multiple labels are to be
predicted for each instance.) There are many techniques to deal with this problem which can be categorized into the
following three categories:
\bit
\item \textbf{Transformation To Binary} Which can be sub-categorized into the following sub-categories:
\bit
\item \textbf{One VS Rest (OvR), or One VS All (OvA), or One Against All(OAA)} OvR involves training a single
classifier per class, with the samples of that class as positive samples and all other samples as negatives. This
strategy requires the base classifiers to produce a real-valued confidence score for its decision, rather than just a
class label due to the fact that discrete class labels alone can lead to ambiguities, where multiple classes are
predicted for a single sample. Although this strategy is popular, it is a heuristic that suffers from several problems.
\item \textbf{One VS One (OvO)} In OvO one trains $\frac{K (K - 1)}{2}$ binary classifiers for a $K$-way multinomial
problem, where each receives the samples of a pair of classes from the original training set, and must learn to
distinguish these two classes. At prediction time, a voting scheme is applied: all $\frac{K (K - 1)}{2}$ classifiers
are applied to an unseen sample and the class that got the highest number of ``$+1$'' predictions gets predicted by the
combined classifier. Like OvR, OvO suffers from ambiguities in that some regions of its input space may receive the
same number of votes.
\eit
\item \textbf{Extension From Binary} Extension from binary techniques extend the existing binary classifier to solve
multinomial classification problems. Several algorithms have been developed based on neural networks, decision trees,
k-nearest neighbors, naive Bayes, support vector machines and extreme learning machines to address multi-class
classification problems.
\item \textbf{Hierarchical Classification} Hierarchical classification tackles the multinomial classification problem
by dividing the output space into a tree. Each parent node is divided into multiple child nodes and the process is
continued until each child node represents only one class. Several methods have been proposed based on hierarchical
classification.
\eit

As a final comment, it is worth mentioning that logistic regression itself can be generalized to support multiple
classes directly, without having to train and combine multiple binary classifiers. The final result is called
``softmax regression''. However, since softmax regression is used heavily in deep learning we will develop its
corresponding theory in the deep learning chapter.

\section{Generalized Linear Model}

As we showed, in linear regression the target follows a normal distribution while in logistic regression the target
follows a Bernoulli distribution. We can generalize both regressions in one coherent model called ``generalized
linear model'' in which the target is allowed to follow a broad family of probability distributions.

\bd[Generalized Linear Model]
\textbf{Generalized linear model} (GLM) is a model that allows the dependent variable to follow an exponential family
of probability distributions of the form:
\bse
P(y | \eta) = b(y) \cdot exp(\eta^{\intercal} T(y) - a(\eta))
\ese
\ed

By picking specific values for $b$, $\eta$, $T$ and $a$ we end up with different distributions (including linear and
logistic regression). Then we simply assume independence and apply the principle of maximum likelihood to obtain a
loss function, in order to minimize it and find the best parameters.

\section{Errors}

Since $h$ is an estimator of $f$, the theory we developed in the chapter of parametric inference for estimators also
holds for $h$. In other words, for the hypothesis function, which acts as an estimator for $f$, we can define
quantities such as MSE (not the loss function but they coincide), sampling deviation, bias and variance. We can then
use these quantities in order to evaluate how well a machine learning model performs. Let us see now the definitions
of these quantities adjusted for the case of machine learning where the estimator is $h$.

\subsection{Point-Wise, Overall, In-Sample \& Out-Of-Sample Error}

Starting from the corresponding MSE (again not the loss function), in machine learning case we define the following
quantities:

\bd[Point-Wise Error]
We define the \textbf{point-wise error} $e$ as a function of the real target function $f$ and the hypothesis function
$h$ at point $x$:
\bse
e = e( f(x), h(x) )
\ese
\ed

For example, for linear regression we could use $e( f(x), h(x) ) = (f(x) - h (x))^2$ while for logistic regression $e
( f(x), h(x) ) = [f(x) \neq h(x)]$. Given point-wise error we can generalize to overall error.

\bd[Overall Error]
We define the \textbf{overall error} $E$ as the average over all point-wise errors $e( f(x), h(x) )$ at every point $x$.
\ed

We distinguish between two kind of overall errors: the in-sample error and the out-of-sample error.

\bd[In-Sample Error]
We define the \textbf{in-sample error} $E_{in}$ as the average of point-wise errors of the dataset that the model was
trained:
\bse
E_{in} = \frac{1}{m} \sum_{i=1}^m e( f(x^{(i)}), h(x^{(i)}) )
\ese
\ed

In other words in-sample error shows how well the model performs on the data used to build it.

\bd[Out-Of-Sample Error]
We define the \textbf{out-of-sample error} $E_{out}$ as a the expected value of point-wise errors of new data:
\bse
E_{out} = E_{x}[e( f(x), h(x) )]
\ese
\ed

In other word out-of-sample error show how well the model generalizes to predictions for data it has not seen before.
It makes sense that in order for $h$ to work well out of sample, so it can predict, it must be $E_{out} \approx 0$.

\subsection{Bias \& Variance}

Back in parametric inference chapter, we introduced the bias $B$ of an estimator $\hat{\theta}$, as the difference
between the expected value of the estimator and the actual true parameter we want to estimate, $B =
E[\hat{\theta}_{n}] - \theta$. Coming to our case where our estimator is $\hat{\theta} = h(x)$ and the true parameter
is the target function $\theta = f(x)$, for the bias we get:

\bd[Bias]
We define the \textbf{bias} $B$ of the hypothesis function $h$ as the quantity:
\bse
B = E[h(x)] - f(x)
\ese
\ed

With the expected value we assume that we train the model many times with different data each time, and we take the
expected value of the functions that the model spits each time. The bias error is an error from erroneous assumptions
in the learning algorithm. When we are dealing with high bias, formally we can say that the hypothesis set $H=\{h\}$
was not big enough in order to contain function that can approximate well the target function $f$. So our best
approximation for $f$ is still a bad one that cannot fit the data well.

\vspace{-5pt}

\fig{bias}{0.4}

\vspace{-5pt}

High bias can cause an algorithm to miss the relevant relations between features and target outputs and fail to
capture the underlying structure of the dataset. We call this a case of \textbf{underfitting}, since the model fails
to fit the given dataset well.

\vspace{-10pt}

\fig{underfitting}{0.8}

\vspace{-10pt}

Underfitting is one of the two main problems that a machine learning model can have and it leads to a high in-sample
error $E_{in}$ which subsequently leads to a high out-of-sample error $E_{out}$. Hence, even that the problem is
coming from the in-sample error it leads to not being able to generalize for out-of-sample data. \v

In parametric inference chapter, we also defined the variance of an estimator $\hat{\theta}_{n}$ as the expected
value of the square difference of the estimator from the expected value of the estimator: $Var = E[(\hat{\theta}_{n}
- E[\hat{\theta}_{n}])^2]$. Coming back to machine learning for the variance we get:

\bd[Variance]
We define the \textbf{variance} $B$ of the hypothesis function $h$ as the quantity:
\bse
Var = E_{x}[(h(x) - E_{x}[h(x)])^2]
\ese
\ed

Again, with the variance we assume that we train the model many times with different data each time, and we take the
variance (i.e.\ the spread) of the functions that the model spits each time. The variance is an error from
sensitivity to small fluctuations in the training set.

\vspace{-5pt}

\fig{variance}{0.45}

\vspace{-5pt}

When we are dealing with high variance, informally we can say that the hypothesis set $H=\{h\}$ is very big so in
order to compensate the spread of dataset the model finds a function that fits the particular data very well but
fails to generalize to new data. We call this a case of \textbf{overfitting}, since the model fails to generalize to
new data.

\vspace{-10pt}

\fig{overfitting}{0.83}

\vspace{-10pt}

Overfitting leads to a very low in-sample $E_{in} \approx 0$, since it does a very good job on fitting the data.
However, it fails to generalize, hence, to predict new data, which leads to a very high out-of-sample error $E_{out}$. \v

Back in parametric inference we also showed that the mean squared error can be decomposed to bias and variance, and
of course the same holds in our case since for the out of sample error of linear regression we can show:
\begingroup
\allowdisplaybreaks
{\setlength{\jot}{5pt}
\begin{align*}
E_{out} &= E_{x} \Big[ e(f(x),h(x)) \Big] \\
&= E_{x} \Big[ \Big( f(x) - h(x) \Big)^2 \Big] \\
&= E_{x} \Big [ \Big( f(x) - h(x) + E_{x}[h(x)] - E_{x}[h(x)] \Big) ^2 \Big] \\
&= E_{x} \Big[ \Big( \Big( f(x) - E_{x}[h(x)] \Big) + \Big( E_{x}[h(x)] - h(x) \Big) \Big)^2 \Big] \\
&= E_{x} \Big[ \Big( f(x) - E_{x}[h(x)] \Big)^{2} + 2 \Big( f(x) - E_{x}[h(x)] \Big)
\Big( E_{x}[h(x)] - h(x) \Big) + \Big( E_{x}[h(x)] - h(x) \Big)^2 \Big] \\
&= E_{x} \Big[ \Big( f(x) - E_{x}[h(x)] \Big)^{2} \Big] + E_{x} \Big[ 2 \Big( f(x) - E_{x}[h(x)] \Big)
\Big(E_{x}[h(x)] - h(x) \Big) \Big] + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
&= \Big( f(x) - E_{x}[h(x)] \Big)^{2} + 2 \Big( f(x) - E_{x}[h(x)] \Big)
\Big( E_{x} \Big[ E_{x}[h(x)] - h(x) \Big] \Big) + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2}\Big] \\
&= \Big( f(x) - E_{x}[h(x)] \Big)^{2} + 2 \Big( f(x) - E_{x}[h(x)] \Big) \Big( E_{x}[h(x)] - E_{x}[h(x)] \Big)
+ E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
&= \Big( f(x) - E_{x}[h(x)] \Big)^{2} + E_{x} \Big[ \Big( E_{x}[h(x)] - h(x) \Big)^{2} \Big] \\
&= B^2 + Var
\end{align*}}
\endgroup

Hence, the out-of-sample error is actually a combination of bias and variance. So in order to have a model that
generalizes well, we have to keep both of them low. However, since they are of opposite nature, the more we reduce
bias the more variance increases and vice versa. This is the so called \textbf{bias-variance trade off}. The goal in
machine learning is to balance this trade off so the model fits the data well and doesn't fail to generalize.

\fig{overunderfitting}{0.55}

Let's sum up. In the graph above we see that in the case of high bias (underfitting) we have restricted our model to
linear predictors, however the data do not follow a linear trend, hence, the hypothesis set is too small and the model
cannot find a good curve to fit the data. On the other hand, in the case of high variance (overfitting) the
hypothesis set is so big allowing complex predictors so the model managed to find a high degree polynomial that fit
the data really good, however it will fail to generalize since it depends a lot on the initial values of the data and
it is sensitive to fluctuations of them. Finally, in the last graph we have a good balance of bias and variance and
the model found a good curve. One has to keep in mind that increasing a model's complexity will typically increase
its variance and reduce its bias. Conversely, reducing a model's complexity increases its bias and reduces its
variance. This is why it is called a ``trade-off''. \v

It is worth mentioning that we have neglected the so-called ``irreducible error'' for the equations. This part is
due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g
fix the data sources, such as broken sensors, or detect and remove outliers).

\section{Evaluation}

Evaluation is about how good a model generalizes to new data, trying to answer the question ``how do we know that our
machine learning model is any good?''. After applying the learning algorithm to the data and having obtained a
hypothesis $h$, the machine learning model is ready to make new predictions. However, before that, we have to evaluate
the model by analysing the errors that we introduced in the previous chapter.\footnote{Ideally, the evaluation
methods should be the same during both development and production. But in many cases, the ideal is impossible because
during development, you have ground truth labels, but in production, you don't.} \v

The starting part is the in-sample and out-of-sample errors that we defined previously:
\bse
E_{in} = \frac{1}{m} \sum_{i=1}^m e( f(x^{(i)}), h(x^{(i)}) ) \qquad \text{and} \qquad E_{out} = E_{x}[e( f(x), h(x) )]
\ese

In general, for the error function $e$ we use the corresponding loss function $J$ that we used to train the model
(although some times we can use variations of it), since it is a function of the target and hypothesis functions as
$e$, and it is a really good measure of error:
\bse
E_{in} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )
\qquad \text{and} \qquad E_{out} = E_{x}[J( f(x), h(x) )]
\ese

where here the notation $J^{{(i)}}$ means the error coming from the $i^{th}$ training example. Hence, now $E_{in}$ is
calculated with the data that we trained the model, so it's a very good measure of how well the model performs in the
data that it was trained on. The problem comes from $E_{out}$ since we don't know how to compute this expected value.
Unsurprising we will perform the usual trick of substituting the expected value with the average so:
\bse
E_{in} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) ) \qquad \text{and} \qquad
E_{out} = \frac{1}{m} \sum_{i=1}^m J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )
\ese

Of course, since we estimate the expected value with an average that brings an error to the estimation of the
out-of-sample error. However, for our purposes we assume that this error is neglectful, and from now on we will treat
the estimated out-of-sample error as the actual out-of-sample error. In general we have to keep in mind though that
out-of-sample error carries an error. \v

The question that arises is what data are we going to use for $E_{out}$. Using the same data that we trained the
model is a really bad idea since, first of all, we will simply get $E_{out} = E_{in}$ and secondly the model already
knows the correct answers of the data since we used them to train it, and the evaluation will be biased. \v

In order to overcome this problem, we split the dataset (before training the model) into two parts: training set and
test set. Then we use the first to train the model and obtain $E_{in}$ and the latter to evaluate its performance and
obtain $E_{out}$. Since the model is trained with the train set, it has never seen the test set so the estimation of
the out-of-sample error with the evaluation set will be unbiased. \v

One of the things to consider is the proportions of splitting the dataset into training and test sets. This again is
an area of heavy research, but in general in machine learning we usually split them in a proportion of ``80\% -
20\%''. In other areas of machine learning like deep learning where we usually have a very large amount of data we use
splitting rules of ``99\% - 1\%''. But we will address this issue in details in deep learning chapter. \v

Hence, before training we split the dataset as:
\bit
\item Training Dataset: $\{x_{\text{train}}^{(i)}, y_{\text{train}}^{(i)}\}, \:\:\: i = 1,2,\ldots,m_{\text{train}}
\qquad$ (usually about 80\% of initial dataset). \v
\item Test Dataset: $\{x_{\text{test}}^{(i)}, y_{\text{test}}^{(i)}\}, \:\:\: i = 1,2,\ldots,m_{\text{test}} \qquad$
(usually about 20\% of initial dataset). \v
\eit

And subsequently the errors become:
\bse
E_{in} = \frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} J^{{(i)}}(f(x^{(i)}), h(x^{(i)}) ) \qquad \text{and}
\qquad E_{out} = \frac{1}{m_{\text{test}}} \sum_{i=1}^{m_{\text{test}}} J^{{(i)}}( f(x^{(i)}), h(x^{(i)}) )
\ese

\v

From now on we will be referring to the first expression as $J_{\text{train}} = E_{in}$ and to the second one as
$J_{\text{test}} = E_{out}$. \v

So for example for linear regression we would have:
\bse
J_{\text{train}} = \frac{1}{2m_{train}} \sum_{i=1}^{m_{train}} (y^{(i)}
- \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2 \qquad \text{and} \qquad J_{\text{test}}
= \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}} (y^{(i)} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)})^2
\ese

\v

While for logistic regression we would have:
\bse
J_{\text{train}} = - \frac{1}{m_{train}} \sum_{i=1}^{m_{train}} \Big( y^{(i)}
\cdot \ln h(\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
\ese

\v

and:
\bse
J_{\text{test}} = - \frac{1}{m_{test}} \sum_{i=1}^{m_{test}} \Big( y^{(i)}
\cdot \ln h(\boldsymbol{x}^{(i)}) + (1 - {y^{(i)}}) \cdot \ln (1 - h(\boldsymbol{x}^{(i)})) \Big)
\ese

\v

Coming to a more applied side of machine learning, it is worth mentioning that it is very important to not use the
test dataset until the very end when the final model is ready. The test set is actually something we use in order to
provide it as a performance measure together with the model and not something to use for gaining insights or
improving the model. For the later, what people usually do is to further split the train set to a ``new'' (even
smaller) train set, and a so called ``evaluation set'' (using again a proportion of ``80\% - 20\%'') and then the
use the new smaller train set to train the model and the evaluation set in order to improve the model (e.g.\ tune the
parameters, find best features, etc). There are many best practices on how to use the evaluation set (e.g.\ K-fold
cross validation, grid search, etc) however these techniques are of an applied nature and they get a bit out of topic
for these notes. Long story short, once you are confident about your final model, measure its performance on the test
set to estimate the generalization error. Don't tweak your model after measuring the generalization error since you
would just start overfitting the test set. \v

Coming back to the theoretical aspect of machine learning, now that we have $J_{\text{train}} $ and $J_{\text{eval}}
$ we know how well the models performs in and out of sample. However, we can also use them in order to find if the
model suffers from underfitting or overfitting. There are two ways that we can do so. \v

The first way, is by gradually increasing the complexity of the model (higher polynomial degrees so bigger hypothesis
set), training the model for each complexity level and calculate both $J_{train}$ and $J_{eval}$ for each model. Then
by plotting out the different values for different levels of complexity we usually end up with the following graph:

\vspace{-10pt}

\fig{eval}{0.4}

\vspace{-10pt}

In the high bias area both $J_{train}$ and $J_{eval}$ are high. This means that the model does not fit the training
data well hence, it fails to generalize. This is the case of underfitting. In the high variance area, $J_{train}$ is
low but $J_{eval}$ is high. This means that the model fits the training data well but fails to generalize to unseen
data. This is the case of overfitting. Hence, by using the graph we can diagnose both cases!\v

The second way to diagnose the problem of the model, is through the so called ``learning curves''.

\bd[Learning Curve]
A \textbf{learning curve} is a graphical representation of how an increase in learning comes from greater experience;
or how the more someone performs a task, the better they get at it.
\ed

Informally, a learning curve is the relation between error (as expressed in loss function) and training examples $m$.
By plotting this relation for both $J_{train}$ and $J_{eval}$ we end up with two learning curves and by their
relative position we can diagnose if our model suffers from high bias or high variance. \v

More specifically, when the learning curves of $J_{train}$ and $J_{eval}$ do not have a large gap between them as $m$
increases we are usually dealing we a case of high bias and underfitting.

\vspace{10pt}

\fig{lchighbias}{0.6}

\vspace{10pt}

On the other hand when there is a large gap between the two curves we are dealing with the case of high variance and
overfitting.

\vspace{10pt}

\fig{lchighvariance}{0.5}

\vspace{10pt}

Once we detect the problem then we have to make some changes in order to fix them! Here are some of the techniques
that we follow:
\bit
\item \textbf{For high bias (underfitting)}:
\bit
\item Increase model complexity.
\item Increase number of features.
\eit
\item \textbf{For high variance (overfitting)}:
\bit
\item Decrease model complexity.
\item Decrease number of features.
\item Find more training examples.
\item Regularization (next section).
\eit
\eit

\section{Regularization}

As we saw in the previous section, when we allow a very broad hypothesis set with many higher order terms the model
might find a hypothesis function $h$ that gives a 0 in-sample error but fails to generalize. This is due to high
variance, i.e.\ large dependence on the very specific dataset used for training, and we call this a case of
overfitting. A way to deal with overfitting is a collection of techniques that undergo with the name
``regularization''.

\bd[Regularization]
\textbf{Regularization} is the process of adding information in order to solve an ill-posed problem and to prevent
overfitting.
\ed

There are many different regularization techniques. We will begin with four main ones called ``Ridge regression''
(or ``Tikhonov regression'' or ``L2 regularization''), ``Lasso regression'' (or ``L1 regularization''),
``elastic net'', and ``early stopping''.

\subsection{Ridge Regression / Tikhonov Regularization (L2 Regularization)}

The reason of overfitting is that the parameters $\boldsymbol{w}$ are free to get any value. With regularization we
penalize the parameters by imposing an extra constraint on $\boldsymbol{w}$ of the form:
\bse
\boldsymbol{w}^{\intercal} \boldsymbol{w} \leq C
\ese

where $C$ is a constant defined by us and it controls the effect of regularization. It is called ``L2
regularization'' because the quantity $\boldsymbol{w}^{\intercal} \boldsymbol{w}$ is actually the squared L2 norm of
the vector $\boldsymbol{w}$:
\bse
||\boldsymbol{w}||^2_2 = \boldsymbol{w}^{\intercal} \boldsymbol{w}
\ese

Hence, now the optimization problem becomes to minimize the loss function $J (\boldsymbol{w})$ subject to the above
mentioned constraint. According to Appendix~\ref{ch:constrained-optimization} in order to do so we define the
Lagrangian:
\bse
\mathcal{L} (\boldsymbol{w}) = J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{\intercal} \boldsymbol{w}
\ese

where $\lambda$ is the Lagrange multiplier, and then we solve the equation:
\bse
\nabla_{\boldsymbol{w}} \mathcal{L} (\boldsymbol{w}) = 0
\ese

For example, for linear regression where $J(\boldsymbol{w})$ is the MSE loss function, the Lagrangian reads:
{\setlength{\jot}{10pt}
\bse
\mathcal{L} (\boldsymbol{w}) = J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{\intercal} \boldsymbol{w}
= \frac{1}{2m} (X \boldsymbol{w} - \boldsymbol{y})^{\intercal} (X \boldsymbol{w} - \boldsymbol{y}) + \frac{\lambda}{2m}
\boldsymbol{w}^{\intercal} \boldsymbol{w}
\ese}

At this point we can redefine this Lagrangian as a new loss function of the form:
\bse
J (\boldsymbol{w}) = \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^{\intercal}
(X \boldsymbol{w} - \boldsymbol{y}) + \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big ]
\ese

and then the problem is to minimize this loss function which is actually a regression problem. The corresponding
regression is called ``Ridge regression'' (or ``Tikhonov regularization''), where the only difference with linear
regression is that we have to add the extra term in the loss function to reduce overfitting. \v

The coefficient $\lambda$ is the one that controls the regularization effect on the regression. In one extreme where
$\lambda=0$ the regularization term vanishes, and the loss function ends up to the mean squared error loss function,
Hence, the ridge regression turns to linear regression. In the other extreme where $\lambda \to \infty$ then the
regularization term penaltizes all parameters in an extreme way, so the ridge regression, in order to minimize the
loss, is forced to set all the parameters to 0. In the end we end up with $\boldsymbol{w}^{\intercal} \boldsymbol{x}
= 0$. For all intermediate values of $\lambda$ we get different levels of regularization. It is actually our job to
tune the model to the right $\lambda$ that does the job. \v

Now that we have a loss function, we treat the problem in the similar way as we did before. For example, in the
linear case of ridge regression we can solve the normal equation in the same way we solved it before:
{\setlength{\jot}{10pt}
\begin{align*}
J(\boldsymbol{w}) &= \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^{\intercal} (X \boldsymbol{w}
- \boldsymbol{y}) + \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big]\\
&= \frac{1}{2m} \Big[ \Big( (X \boldsymbol{w})^{\intercal} - \boldsymbol{y}^{\intercal} \Big)
\Big(X\boldsymbol{w} - \boldsymbol{y} \Big) + \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big]\\
&= \frac{1}{2m} \Big[ (X \boldsymbol{w})^{\intercal} (X \boldsymbol{w}) - (X \boldsymbol{w})^{\intercal}
\boldsymbol{y} - \boldsymbol{y}^{\intercal} (X \boldsymbol{w}) + \boldsymbol{y}^{\intercal} \boldsymbol{y}
+ \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big] \\
&= \frac{1}{2m} \Big[ (X \boldsymbol{w})^{\intercal} (X \boldsymbol{w}) - 2 (X \boldsymbol{w})^{\intercal}
\boldsymbol{y} + \boldsymbol{y}^{\intercal} \boldsymbol{y} + \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big] \\
&= \frac{1}{2m} \Big[ \boldsymbol{w}^{\intercal} X^{\intercal} X \boldsymbol{w} - 2 \boldsymbol{w}^{\intercal}
X^{\intercal} \boldsymbol{y} + \boldsymbol{y}^{\intercal} \boldsymbol{y}
+ \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big]
\end{align*}}

By setting the derivative to 0 we obtain:
{\setlength{\jot}{10pt}
\begin{align*}
& \nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = 0 \Rightarrow \\
& \frac{1}{2m} \nabla_{\boldsymbol{w}} \Big[ \boldsymbol{w}^{\intercal} X^{\intercal} X \boldsymbol{w} - 2
\boldsymbol{w}^{\intercal} X^{\intercal} \boldsymbol{y} + \boldsymbol{y}^{\intercal} \boldsymbol{y} + \lambda
\boldsymbol{w}^{\intercal} \boldsymbol{w} \Big] \\
& \frac{1}{2m} \Big[ 2 X^{\intercal} X \boldsymbol{w} - 2 X^{\intercal} \boldsymbol{y}
+ 2\lambda \boldsymbol{w} \Big] = 0 \Rightarrow \\
& \frac{1}{m} \Big[ X^{\intercal} X \boldsymbol{w} - X^{\intercal} \boldsymbol{y}
+ \lambda \boldsymbol{w} \Big] = 0 \Rightarrow \\
& X^{\intercal} X \boldsymbol{w} - X^{\intercal} \boldsymbol{y} + \lambda \boldsymbol{w} = 0 \Rightarrow \\
& (X^{\intercal} X + \lambda I) \boldsymbol{w} = X^{\intercal} \boldsymbol{y} \Rightarrow \\
& \underbrace{(X^{\intercal} X + \lambda I)^{-1} (X^{\intercal} X + \lambda I)}_{I} \boldsymbol{w}
= (X^{\intercal} X + \lambda I)^{-1} X^{\intercal} \boldsymbol{y} \Rightarrow \\
& \boldsymbol{w} = (X^{\intercal} X + \lambda I)^{-1} X^{\intercal} \boldsymbol{y}
\end{align*}}

\vspace{-10pt}

The only difference with the normal equation of linear regression is the extra term $\lambda I$ coming from
regularization. \v

Gradient descent also works for ridge regression. For the derivative of $J$:
\bse
\nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = \frac{1}{m} \Big(X^{\intercal} X \boldsymbol{w}
- X^{\intercal} \boldsymbol{y} + \lambda \boldsymbol{w} \Big) = \frac{1}{m} X^{\intercal}
\Big( (X + \lambda I) \boldsymbol{w} - \boldsymbol{y} \Big)
\ese

Hence, the update rule reads:
\bse
\boldsymbol{w} \coloneqq \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal}
\Big( (X + \lambda I) \boldsymbol{w} - \boldsymbol{y} \Big)
\ese

\v

Of course, L2 regularization can be applied also for the case of logistic regression. More specifically, for cross
entropy loss function of logistic regression the Lagrangian reads:
\bse
\mathcal{L} (\boldsymbol{w}) = J(\boldsymbol{w}) + \frac{\lambda}{2m} \boldsymbol{w}^{\intercal} \boldsymbol{w}
= - \frac{1}{m} \Big( \boldsymbol{y}^{\intercal} \cdot \ln h(X) + (I -\boldsymbol{y})^{\intercal}
\cdot \ln (I - h(X)) \Big) + \frac{\lambda}{2m} \boldsymbol{w}^{\intercal} \boldsymbol{w}
\ese

Similarly to the linear case, we redefine this Lagrangian as a new loss function of the form:
\bse
J (\boldsymbol{w}) = - \frac{1}{m} \Big[ \boldsymbol{y}^{\intercal} \cdot \ln h(X) + (I - \boldsymbol{y})^{\intercal}
\cdot \ln (I - h(X)) - \frac{\lambda}{2} \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big ]
\ese

As we said back in logistic regression, normal equation does not apply here since there is no closed analytical
solution, however gradient descent still applies where the rule simply reads:
\bse
\boldsymbol{w} \coloneqq \Big( 1 - \frac{\alpha \lambda}{m} \Big) \boldsymbol{w} - \frac{\alpha}{m} X^{\intercal}
\Big( \frac{1}{1+exp(-X \boldsymbol{w})}-\boldsymbol{y} \Big)
\ese

In both cases solving ridge regression will give us as a result a solution that slightly underfits the data, compared
to linear or logistic regression. This underfitting will produce higher bias hence, due to bias-variance trade off, a
reduced variance which will lead to the reduction of overfitting.

\subsection{Lasso Regression (L1 Regularization)}

In ridge regression we used the L2 norm of the vector $\boldsymbol{w}$. Another way of regularization is to use L1
norm which is:
\bse
|| \boldsymbol{w} ||_1 \leq C
\ese

By repeating the same way of analysis as in ridge regression, we can define the following loss function for linear
regression:
\bse
J (\boldsymbol{w}) = \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^{\intercal}
(X \boldsymbol{w} - \boldsymbol{y}) + \lambda ||\boldsymbol{w}||_1 \Big ]
\ese

and for logistic regression:
\bse
J (\boldsymbol{w}) = - \frac{1}{m} \Big[ \boldsymbol{y}^{\intercal} \cdot \ln h(X) +
(I - \boldsymbol{y})^{\intercal} \cdot \ln (I - h(X)) - \frac{\lambda}{2} ||\boldsymbol{w}||_1\Big ]
\ese

The corresponding regression is called ``Least Absolute Shrinkage and Selection Operator Regression'', usually
simply called ``Lasso regression''. An important characteristic of Lasso regression is that it tends to eliminate
the weights of the least important features (i.e.\ set them to zero). In other words, Lasso regression automatically
performs feature selection and outputs a sparse model (i.e.\ with few nonzero feature weights). \v

As before, we can use normal equation and gradient descent to solve Lasso regression.

\subsection{Elastic Net}

Elastic net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple
mix of both Ridge and Lasso's regularization terms, and you can control the mix ratio between them with a parameter
$r$. \v

The elastic net loss function for linear regression reads:
\bse
J (\boldsymbol{w}) = \frac{1}{2m} \Big[ (X \boldsymbol{w} - \boldsymbol{y})^{\intercal}
(X \boldsymbol{w} - \boldsymbol{y}) + r \lambda ||\boldsymbol{w}||_1
+ (1-r) \lambda \boldsymbol{w}^{\intercal} \boldsymbol{w} \Big ]
\ese

and for logistic regression:
\bse
J (\boldsymbol{w}) = - \frac{1}{m} \Big[ \boldsymbol{y}^{\intercal} \cdot \ln h(X) + (I - \boldsymbol{y})^{\intercal}
\cdot \ln (I - h(X)) - r \frac{\lambda}{2} || \boldsymbol{w}||_1 - (1-r) \frac{\lambda}{2} \boldsymbol{w}^{\intercal}
\boldsymbol{w} \Big ]
\ese

When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression. \v

Since elastic net summarizes both ridge and lasso regression, it is a good point to analyse, when one should use
plain linear (or logistic) regression (without any regularization), ridge, lasso, or elastic net? It is almost always
preferable to have at least a little bit of regularization, so generally plain linear regression should be avoided.
Ridge is a good default, but if one suspects that only a few features are useful, they should prefer lasso or elastic
net because they tend to reduce the useless features' weights down to zero, as we have discussed. In general, elastic
net is preferred over lasso because lasso may behave erratically when the number of features is greater than the
number of training instances or when several features are strongly correlated.

\subsection{Early Stopping}

A very different way to regularize iterative learning algorithms such as gradient descent is to stop training as soon
as the validation error reaches a minimum. This is called ``early stopping''. In general as the algorithm learns,
its prediction error on the training set goes down, along with its prediction error on the validation set. After a
while though, the validation error stops decreasing and starts to go back up. This indicates that the model has
started to overfit the training data. Early stopping just stops training as soon as the validation error reaches the
minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a ``beautiful free
lunch''.

\section{Classification Error Metrics}

In classification problems where both input and output can be either 0 or 1, we can follow a different approach of
error evaluation based on exact matches and mismatches between prediction and actual result. The usual case, since we
are dealing with a binary output, is to define either 0 or 1 as the positive class and the remaining as the negative
one. Which one is which depends on the nature of the problem. For now we will stick with the case were 0 represents
the negative class and 1 the positive one. \v

Given that both the actual class and the predicted class can be either positive or negative we end up with 4
different, distinct situations. Let us define them formally.

\bd[True Positive]
\textbf{True positive} (TP) also called \textbf{hit}, is the case where the model predicts a positive result when the
actual outcome is indeed positive.
\ed

\bd[True Negative]
\textbf{True negative} (TN) also called \textbf{correct rejection}, is the case where the model predicts a negative
result when the actual outcome is indeed negative.
\ed

\bd[False Positive]
\textbf{False positive} (FP) also called \textbf{false alarm} or \textbf{type I error}, is the case where the model
predicts a positive result when the actual outcome is negative.
\ed

\bd[False Negative]
\textbf{False negative} (FN) also called \textbf{miss} or \textbf{type II error}, is the case where the model
predicts a negative result when the actual outcome is positive.
\ed

Once the model is trained, we test it on the evaluation set and we measure the number of occurrences of each category. 
Then we gather them all together to the so called ``confusion matrix''.

\bd[Confusion Matrix]
\textbf{Confusion matrix} is a table that reports the number of true positives TP, true negatives TN, false positives
FP and false negatives FN of a model.
\ed

\vspace{-5pt}

\fig{confusion}{0.35}

Once we have constructed the confusion matrix we can define the following error metrics.

\bd[Accuracy]
\textbf{Accuracy} (ACC) is the rate that shows overall how often the model was correct:
\bse
ACC = \frac{TP + TN}{TP + TN + FP + FN}
\ese
\ed

\v

\bd[Error Rate]
\textbf{Error rate} (ERR) also called \textbf{misclassification}, is the rate that shows overall how often the model
was incorrect:
\bse
ERR = \frac{FP + FN}{TP + TN + FP + FN}
\ese
\ed

\v

It is of course: $ACC + ERR = 1$. \v

\bd[True Positive Rate]
\textbf{True positive rate} (TPR) also called \textbf{sensitivity} or
\textbf{recall} or \textbf{hit rate}, is the rate that shows how often the model predicts positive when the actual
outcome is indeed positive:
\bse
TPR = \frac{TP}{TP + FN}
\ese
\ed

\v

\bd[False Negative Rate]
\textbf{False negative rate} (FNR) also called \textbf{miss rate}, is the rate that shows how often the model
predicts negative when the actual outcome is positive:
\bse
FNR = \frac{FN}{TP + FN}
\ese
\ed

\v

It is of course: $TPR + FNR = 1$. \v

\bd[True Negative Rate]
\textbf{True negative rate} (TNR) also called \textbf{specificity} or \textbf{selectivity}, is the rate that shows how
often the model predicts negative when the actual outcome is indeed negative:
\bse
TNR = \frac{TN}{TN + FP}
\ese
\ed

\v

\bd[False Positive Rate]
\textbf{False positive rate} (FPR) also called \textbf{fall-out rate}, is the rate that shows how often the model
predicts positive when the actual outcome is negative:
\bse
FPR = \frac{FP}{TN + FP}
\ese
\ed

\v

It is of course: $TNR + FPR = 1$. \v

\bd[Positive Predicted Value]
\textbf{Positive predicted value} (PPV) also called \textbf{precision}, is the rate that shows how often the model is
correct when it predicts positive:
\bse
PPV = \frac{TP}{TP + FP}
\ese
\ed

\v

\bd[False Discovery Rate]
\textbf{False discovery rate} (FDR) is the rate that shows how often the model is wrong when it predicts positive:
\bse
FDR = \frac{FP}{TP + FP}
\ese
\ed

\v

It is of course: $PPV + FDR = 1$. \v

\bd[Negative Predicted Value]
\textbf{Negative predicted value} (NPV) is the rate that shows how often the model is correct when it predicts negative:
\bse
NPV = \frac{TN}{TN + FN}
\ese
\ed

\v

\bd[False Omission Rate]
\textbf{False omission rate} (FOR) is the rate that shows how often the model is wrong when it predicts negative:
\bse
FOR = \frac{FN}{TN + FN}
\ese
\ed

\v

It is of course: $NPV + FOR = 1$. \v

\bd[$F_{\beta}$ Score]
\textbf{$F_{\beta}$ score} (FOR) is defined as the harmonic mean of positive predicted value PPV (aka precision) and
true positive rate (aka recall) each weighted based on value of $\beta$:
\bse
F_{\beta} = \frac{(1 + \beta^2) \cdot PPV \cdot TPR}{\beta^2 \cdot PPV + TPR}
= \frac{(1 + \beta^2) \cdot \text{precission} \cdot \text{recall}}{\beta^2 \cdot \text{precission} + \text{recall}}
= \frac{(1 + \beta^2) \cdot TP}{(1 + \beta^2) \cdot TP + \beta^2 \cdot FN + FP}
\ese
\ed

\v

The coefficient $\beta$ is chosen such that recall is considered $\beta$ times as important as precision. Two
commonly used values for $\beta$ are 2 and 0.5, corresponding to the $F_{2}$ where weighs recall higher than
precision (by placing more emphasis on false negatives) and the $F_{0.5}$ measure, which weighs recall lower than
precision (by attenuating the influence of false negatives). However, the most commonly used value for $\beta$ is 1,
corresponding to the $F_{1}$ where precision and recall are weighted equally:
\bse
F_{1} = \frac{2 \cdot PPV \cdot TPR}{ PPV + TPR} = \frac{2 \cdot \text{precission}
\cdot \text{recall}}{\text{precission} + \text{recall}} = \frac{2 \cdot TP}{2 \cdot TP + FN + FP}
\ese

It is clear now that there are a lot of different error metrics for classification. One good question is ``which ones
should I use?''. What follows is a practical guide on which ones are the most important and how to use them.

\subsection{Accuracy \& Precision/Recall Trade-Off}

\textbf{Accuracy} is quire straight forward and easy to understand evaluation metric. It just says how accurate the
classifier is. However, one has to be extra careful with using accuracy as an evaluation metric. More often than not
the dataset is skewed which results to the positive label (the one we want to predict) appearing in just a small
portion of the dataset (say 5\%). This means that an algorithm predicting always negative would be 95\% accurate.
What accuracy lacks is capturing the importance of not finding the positive ones. \v

What people usually pay attention in classification errors is the accuracy of the positive predictions, i.e.\ the
\textbf{precision} of the classifier. In simple words precision tells us how many times the algorithm was correct
when it predicted positive. A trivial way to have perfect precision is to make one single positive prediction and
ensure it is correct. But this would not be very useful, since the classifier would ignore all but one positive
instance. So precision is typically used along \textbf{recall}. Recall simply states what percentage of the positive
labels in the dataset was predicted correctly from the algorithm. Precision and recall are usually the most useful
metrics to evaluate a classifier. \v

Depending on the nature of the problem sometimes one mostly cares about precision, and in other contexts really care
about recall. For example, if one trains a classifier to detect videos that are safe for kids, they would probably
prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than
a classifier that has a much higher recall but lets a few really bad videos show up. On the other hand, suppose one
trains a classifier to detect shoplifters in surveillance images: it is probably fine if their classifier has only
30\% precision as long as it has 99\% recall (sure, the security guards will get a few false alerts, but almost all
shoplifters will get caught). Unfortunately, one can't have it both ways: increasing precision reduces recall, and
vice versa. This is called the ``precision/recall trade-off''. \v

Subsequently \textbf{$F_1$ score} that combines precision and recall is also quite useful, in particular if one needs
a simple way to compare two classifiers. The $F_1$ score is the harmonic mean of precision and recall and,
unfortunately, it does not have a straight forward intuitive meaning. The way to interpret it is to keep in mind that
the harmonic mean gives much more weight to low values. As a result, a classifier will only get a high $F_1$
score if both recall and precision are high.

\subsection{Receiver Operating Characteristic (ROC) Curve}

Another very widely used technique in classification evaluation is the so called ``receiver operating
characteristic curve'' (ROC) which is a graphical plot that illustrates the diagnostic ability of a binary classifier
system as its discrimination threshold is varied. In other words, each point in the curve represents a different
value of the discrimination threshold (from 0 to 1), hence, a completely different behaviour of the model which leads
to a different confusion matrix and subsequently different error metrics for each value. The method was originally
developed for operators of military radar receivers, which is why it is so named. \v

The ROC curve is created by plotting the true positive rate (TPR or sensitivity or recall) against the false positive
rate (FPR or 1 - specificity). It can also be thought of as a plot of the power as a function of the Type I Error of
the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as
estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. \v

ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and
prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way
to cost/benefit analysis of diagnostic decision making. \v

When using normalized units, the area under the curve (often referred to as simply the AUC) is equal to the
probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative
one (assuming ``positive'' ranks higher than ``negative'').

\fig{roc}{0.4}

The machine learning community most often uses the ROC AUC statistic for model comparison. In the figure for example,
the blue line is a better classifier than the orange and the green, because it has a larger are under the curve.
Subsequently the orange is better than the green. This practice however has been questioned because AUC estimates are
quite noisy and suffer from other problems. Nonetheless, the coherence of AUC as a measure of aggregated
classification performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a
number of other performance metrics such as the Brier score. Another problem with ROC AUC is that reducing the ROC
curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance
points plotted and not the performance of an individual system, as well as ignoring the possibility of concavity repair.

\subsection{Baselines}

Evaluation metrics, by themselves, mean little. When evaluating a model, it's essential to know the baseline we're
evaluating it against. The exact baselines should vary from one use case to another, but here are the five baselines
that might be useful across use cases.

\bd[Random Baseline]
\textbf{Random baseline} is the baseline of a model which just predicts at random.
\ed

In random baseline, the predictions are generated at random following a specific distribution, which can be the
uniform distribution or the task's label distribution.

\bd[Simple Heuristic Baseline]
\textbf{Simple heuristic baseline} is the baseline of predictions based on simple heuristics.
\ed

In simple heuristic baseline, no machine learning model is involved.

\bd[Zero Rule Baseline]
\textbf{Zero rule baseline} is the baseline of a model which always predicts the most common class.
\ed

The zero rule baseline is a special case of the simple heuristic baseline.

\bd[Human Baseline]
\textbf{Human baseline} is the baseline of predictions made by human experts.
\ed

In many cases, the goal of machine leaning is to automate what would have been otherwise done by humans, so it's
useful to know how your model performs compared to human experts. Even if your system isn't meant to replace human
experts and only to aid them in improving their productivity, it's still important to know in what scenarios this
system would be useful to humans.

\bd[Existing Solutions Baseline]
\textbf{Existing solutions baseline} is the baseline of existing solution models.
\ed

In many cases, machine learning systems are designed to replace existing solutions, which might be business logic
with a lot of if/else statements or third-party solutions. It's crucial to compare your new model to these existing
solutions. Your machine learning model doesn't always have to be better than existing solutions to be useful. A model
whose performance is a little bit inferior can still be useful if it's much easier or cheaper to use.

\section{Support Vector Machine}

Support vector machine (SVM) is another, more advanced, supervised learning model with associated learning algorithm
that analyze data for classification and regression analysis. Developed at Bell Laboratories by Vladimir Vapnik SVMs
are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed
by Vapnik and Chervonenkis. \v

Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds
a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear
classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM
maps training examples to points in space so as to maximise the width of the gap between the two categories. New
examples are then mapped into that same space and predicted to belong to a category based on which side of the gap
they fall. \v

Although SVM applies mainly in classification problems however there is also another model called support vector
regression that applies the same ideas in regression problems. Here we will explore only SVM. \v

To tell the SVM story, we'll need to first talk about margins and the idea of separating data with a large ``gap''.
Next, we'll talk about the optimal margin classifier, which will lead us into a digression on Lagrange duality. We'll
also see kernels, which give a way to apply SVMs efficiently in very high dimensional (such as infinite dimensional)
feature spaces, and finally, we'll close off the story with the SMO algorithm, which gives an efficient
implementation of SVMs. \v

Consider logistic regression, where the probability $P(y = 1| \boldsymbol{x}; \boldsymbol{w})$ is modelled by:
\bse
h(\boldsymbol{w}^{\intercal} \boldsymbol{x}) = \frac{1}{1 + exp(-\boldsymbol{w}^{\intercal} \boldsymbol{x})}
\ese

We would then predict 1 on an input $\boldsymbol{x}$ if and only if $h \left(\boldsymbol{w}^{\intercal} \boldsymbol{x}\right)
\geq 0.5$ or equivalently, if and only if $\boldsymbol{w}^{\intercal} \boldsymbol{x} \geq 0$. Consider a positive
training example ($y = 1$). The larger $\boldsymbol{w}^{\intercal} \boldsymbol{x}$ is, the larger also is $h
\left(\boldsymbol{w}^{\intercal} \boldsymbol{x}\right)$ a.k.a.\ the larger $P(y = 1| \boldsymbol{x}; \boldsymbol{w})$ is, and thus
also the higher our degree of confidence that the label is 1. Thus, informally we can think of our prediction as
being a very confident one that $y = 1$ if $\boldsymbol{w}^{\intercal} \boldsymbol{x} \gg 0$. \v

Similarly, we think of logistic regression as making a very confident prediction of $y = 0$, if
$\boldsymbol{w}^{\intercal} \boldsymbol{x} \ll 0$. Given a training set, again informally it seems that we'd have
found a good fit to the training data if we can find $\boldsymbol{w}$ so that $\boldsymbol{w}^{\intercal}
\boldsymbol{x}^{(i)} \gg 0$ whenever $y^(i) = 1$ and $\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} \ll 0$ whenever
$y^(i) = 0$, since this would reflect a very confident (and correct) set of classifications for all the training
examples. This seems to be a nice goal to aim for, and we'll soon formalize this idea using the notion of functional
margins. \v

For a different type of intuition, consider the following figure, in which the symbol ``X'' represent positive
training examples, the symbol ``O'' denote negative training examples, a decision boundary (this is the line given by
the equation $\boldsymbol{w}^{\intercal} \boldsymbol{x} = 0$ is also called the separating hyperplane) is also shown,
and three points have also been labelled ``A'', ``B'' and ``C''.

\fig{svm}{0.5}

Notice that the point ``A'' is very far from the decision boundary. If we are asked to make a prediction for the value
of $y$ at ``A'', it seems we should be quite confident that $y = 1$ there. Conversely, the point ``C'' is very close to
the decision boundary, and while it's on the side of the decision boundary on which we would predict $y = 1$, it
seems likely that just a small change to the decision boundary could easily have caused out prediction to be $y = 0$.
Hence, we're much more confident about our prediction at ``A'' than at ``C''. The point ``B'' lies in-between these two
cases, and more broadly, we see that if a point is far from the separating hyperplane, then we may be significantly
more confident in our predictions. Again, informally we think it'd be nice if, given a training set, we manage to
find a decision boundary that allows us to make all correct and confident (meaning far from the decision boundary)
predictions on the training examples. We'll formalize this later using the notion of geometric margins. \v

To make our discussion of SVMs easier, we'll first need to introduce a new notation for talking about classification.
We will be considering a linear classifier for a binary classification problem with labels $y$ and features $x$. From
now, we'll use $y \in \{ -1, 1 \}$ (instead of $ \{ 0, 1 \} $) to denote the class labels. Also, we will separate the
$w_0$ component from $\boldsymbol{w}$ and from now on we will be denoting it $b$, and we will write our classifier as:
\bse
h_{\boldsymbol{w}, b} (\boldsymbol{x}) = g(\boldsymbol{w}^{\intercal} \boldsymbol{x} + b)
\ese

\v

This $\boldsymbol{w}$, $b$ notation allows us to explicitly treat the intercept term $b$ separately from the other
parameters. (We also drop the convention we had previously of letting $x_0 = 1$ be an extra coordinate in the input
feature vector.) Note also that, from our definition of $g$ above, our classifier will directly predict either 1 or
-1 without first going through the intermediate step of estimating the probability of $y$ being 1 (which was what
logistic regression did).

\bd[Functional Margin Of A Training Example]
Given a training example $(x^{(i)}, y^{(i)})$ we define the \textbf{functional margin of $(\boldsymbol{w}$, $b$) with
respect to the training example} as:
\bse
{\hat{\gamma}}^{(i)} = y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b)
\ese
\ed

Note that if $y^{(i)}= 1,$ then for the functional margin to be large (i.e.\ for our prediction to be confident and
correct), we need $\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b$ to be a large positive number. Conversely, if
$y^{(i)}= -1,$ then for the functional margin to be large (i.e.\, for our prediction to be confident and correct),
we need $\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b$ to be a large negative number. Moreover, if
$\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b \neq 0$, then our prediction on this example is correct. Hence,
a large functional margin represents a confident and a correct prediction. \v

For a linear classifier with the choice of $g$ given above, there's one property of the functional margin that makes
it not a very good measure of confidence, however. Given our choice of $g$, we note that if we replace
$\boldsymbol{w}$ with $2\boldsymbol{w}$ and $b$ with $2b$, then since $g \left(\boldsymbol{w}^{\intercal} \boldsymbol{x} +
b\right) = g\left(2\boldsymbol{w}^{\intercal} \boldsymbol{x} + 2b\right)$ this would not change $h_{\boldsymbol{w}, b}
(\boldsymbol{x})$ at all meaning that $g$, and hence, also $h_{\boldsymbol{w}, b} (\boldsymbol{x})$, depends only on
the sign, but not on the magnitude, of $\boldsymbol{w}^{\intercal} \boldsymbol{x} + b$. However, replacing the
scaling by a factor also results in multiplying our functional margin by the same factor. Thus, it seems that by
exploiting our freedom to scale $\boldsymbol{w}$ and $b$, we can make the functional margin arbitrarily large without
really changing anything meaningful. Intuitively, it might therefore make sense to impose some sort of normalization
condition. \v

Given a training set we also define the functional margin of ($\boldsymbol{w}$, $b$) with respect to the set as follows.

\bd[Functional Margin Of A Set] Given a training set $\{ x^{(i)}, y^{(i)} \}$ we define the \textbf{functional margin
of $(\boldsymbol{w}$, $b$) with respect to the training set} as:
\bse
{\hat{\gamma}} = \min {\hat{\gamma}}^{(i)}
\ese
\ed

Next, let's talk about geometric margins. Consider the picture below:

\fig{svm2}{0.5}

The decision boundary corresponding to ($\boldsymbol{w}$, $b$) is shown, along with the vector $\boldsymbol{w}$. Note
that $\boldsymbol{w}$ is orthogonal to the separating hyperplane. Consider the point at $A$, which represents the
input $\boldsymbol{x}^{(i)}$ label $y^{(i)} = 1$. Its distance to the decision boundary, $\gamma^{(i)}$ is given by
the line segment $AB$. \v

How can we find the value of $y^{(i)}$? Well, $\frac{\boldsymbol{w}}{||\boldsymbol{w}||}$ is a unit-length vector
pointing in the same direction as $\boldsymbol{w}$. Since $A$ represents $\boldsymbol{x}^{(i)}$ we therefore find
that the point $B$ is given by $\boldsymbol{x}^{(i)} - \gamma^{(i)} \frac{\boldsymbol{w}}{||\boldsymbol{w}||}$. But
this point lies on the decision boundary, and all points on the decision boundary satisfy the equation
$\boldsymbol{w}^{\intercal} \boldsymbol{x} + b = 0$. Hence:
\bse
\boldsymbol{w}^{\intercal} \Big(\boldsymbol{x}^{(i)} - \gamma^{(i)}\frac{\boldsymbol{w}}{||\boldsymbol{w}||} \Big) + b
= 0
\ese

Solving for $\gamma^{(i)}$ yields:
\bse
\gamma^{(i)} = \frac{\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b}{||\boldsymbol{w}||}
= \left(\frac{\boldsymbol{w}}{||\boldsymbol{w}||} \right)^{\intercal}\boldsymbol{x}^{(i)} + \frac{b}{||\boldsymbol{w}||}
\ese

\v

This was worked out for the case of a positive training example at $A$ in the figure, where being on the positive
side of the decision boundary is good. \v

We are now ready to properly define the geometrical margin.

\bd[Geometrical Margin Of A Training Example] Given a training example $(x^{(i)}, y^{(i)})$ we define the
\textbf{geometrical margin of $(\boldsymbol{w}$, $b$) with respect to the training example} as:
\bse
\gamma^{(i)} = y^{(i)} \left( \left( \frac{\boldsymbol{w}}{||\boldsymbol{w}||} \right)^{\intercal} \boldsymbol{x}^{(i)}
+ \frac{b}{||\boldsymbol{w}||} \right)
\ese
\ed

Note that if $||\boldsymbol{w}|| = 1$, then the functional margin equals the geometric margin. This thus gives us a
way of relating these two different notions of margin. Also, the geometric margin is invariant to rescaling of the
parameters. This will in fact come in handy later. Specifically, because of this invariance to the scaling of the
parameters, when trying to fit $\boldsymbol{w}$ and $b$ to training data, we can impose an arbitrary scaling
constraint on $\boldsymbol{w}$ without changing anything important. \v

Finally, given a training set we also define the geometric margin of (w, b) with respect to the set to be the
smallest of the geometric margins on the individual training examples:

\bd[Geometrical Margin Of A Set] Given a training set $\{ x^{(i)}, y^{(i)} \}$ we define the \textbf{geometrical
margin of $(\boldsymbol{w}$, $b$) with respect to the training set} as:
\bse
\gamma = \min {\gamma}^{(i)}
\ese
\ed

Given a training set, it seems from our previous discussion that a natural desideratum is to try to find a decision
boundary that maximizes the (geometric) margin, since this would reflect a very confident set of predictions on the
training set and a good ``fit'' to the training data. Specifically, this will result in a classifier that separates
the positive and the negative training examples with a ``gap'' (geometric margin). \v

For now, we will assume that we are given a training set that is linearly separable, i.e.\ that it is possible to
separate the positive and negative examples using some separating hyperplane. How we we find the one that achieves
the maximum geometric margin? We can pose the following optimization problem:
\bse
\max_{\gamma, \boldsymbol{w}, b} \gamma
\ese

subject to:
\bse
y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) \geq \gamma, \:\:\: i=1,2,\ldots,m
\qquad \text{and} \qquad || \boldsymbol{w} || = 1
\ese

\v

In other words, we want to maximize $\gamma$, subject to each training example having functional margin at least
$\gamma$. The $|| \boldsymbol{w} || = 1$ constraint moreover ensures that the functional margin equals to the
geometric margin, so we are also guaranteed that all the geometric margins are at least $\gamma$,. Thus, solving this
problem will result in parameters with the largest possible geometric margin with respect to the training set. \v

If we could solve the optimization problem above, we'd be done. But the $|| \boldsymbol{w} || = 1$ constraint is a
nasty (non-convex) one, and this problem certainly isn't in any format that we can plug into standard optimization
software to solve. So, lets try transforming the problem into a nicer one. Consider:
\bse
\max_{\gamma, \boldsymbol{w}, b} \frac{\hat{\gamma}}{|| \boldsymbol{w} ||}
\ese

subject to:
\bse
y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) \geq \hat{\gamma}, \:\:\: i=1,2,\ldots, m
\ese

\v

Here, we're going to maximize $\frac{\hat{\gamma}}{|| \boldsymbol{w} ||}$ subject to the functional margins all being
at least $\hat{\gamma}$. Since the geometric and functional margins are related by $\gamma = \frac{\hat{\gamma}}{||
\boldsymbol{w} ||}$ this will give us the answer we want. Moreover, we've gotten rid of the constraint $||
\boldsymbol{w} || = 1$ that we didn't like. The downside is that we now have a nasty (again, non-convex) objective
$\frac{\hat{\gamma}}{|| \boldsymbol{w} ||}$ function, and, we still don't have any off-the-shelf software that can
solve this form of an optimization problem. \v

Lets keep going. Recall our earlier discussion that we can add an arbitrary scaling constraint on $\boldsymbol{w}$
and $b$ without changing anything. This is the key idea we'll use now. We will introduce the scaling constraint that
the functional margin of $\boldsymbol{w}$ and $b$ with respect to the training set must be 1:
\bse
\hat{\gamma} = 1
\ese

Since multiplying $\boldsymbol{w}$ and $b$ by some constant results in the functional margin being multiplied by that
same constant, this is indeed a scaling constraint, and can be satisfied by rescaling $\boldsymbol{w}$ and $b$.
Plugging this into our problem above, and noting that maximizing $\frac{\hat{\gamma}}{|| \boldsymbol{w} ||} =
\frac{1}{|| \boldsymbol{w} ||} $ is the same thing as minimizing $|| \boldsymbol{w} ||^2$, we now have the following
optimization problem:
\bse
\min_{\gamma, \boldsymbol{w}, b} \frac{1}{2} || \boldsymbol{w} ||^2
\ese

subject to:
\bse
y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) \geq 1, \:\:\: i=1,2,\ldots,m
\ese

\v

We can rewrite the constraints as:
\bse
g_{i}(\boldsymbol{w}) = -y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) + 1 \leq 0, \:\:\: i=1,2,\ldots,m
\ese

According to Appendix~\ref{ch:constrained-optimization} we are dealing with an inequality constrained optimization
problem (with no equality part) so we will use the KKT multipliers in order to solve it. \v

Notice that that all KKT conditions are satisfied, since we do not have any equality constraints (Hence, $\mu_{i} = 0,
\:\:\: \forall i$), and we have $\lambda_{i} > 0$ only for the training examples that have functional margin exactly
equal to one, i.e.\ the ones corresponding to constraints that hold with equality $ g_{i}(\boldsymbol{w}) = 0$.
Consider the figure below, in which a maximum margin separating hyperplane is shown by the solid line.

\fig{svm3}{0.5}

\vspace{-10pt}

The points with the smallest margins are exactly the ones closest to the decision boundary. Here these are the three
points: one negative and two positive examples, that lie on the dashed lines parallel to the decision boundary. Thus,
only three of the $\lambda_{i}$ (the ones corresponding to these three training examples) will be non-zero at the
optimal solution to our optimization problem. These three points are called ``support vectors''. The fact that the
number of support vectors can be much smaller than the size the training set will be useful later. \v

Moving on by applying what we developed in Appendix~\ref{ch:constrained-optimization} the Lagrangian of the
inequality constrained optimization problem reads:
\bse
\mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) = \frac{1}{2} || \boldsymbol{w} ||^2 - \sum_{i} \lambda_{i} \left( y^{(i)}
(\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) - 1 \right)
\ese

Note that there are only $\lambda_{i}$ but no $\mu_{i}$ Lagrange multipliers, since the problem has only inequality
constraints. \v

Now we move on trying to solve this optimization problem. Once again, according to
Appendix~\ref{ch:constrained-optimization} the necessary conditions for optimization of $ \mathcal{L}$ turn to:
\bse
\nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) = 0 \:\:\: \text{and} \:\:\:
\frac{\partial \mathcal{L}(\boldsymbol{w}, b, \lambda_{i})}{\partial b} = 0 \:\:\: \text{and} \:\:\:
\frac{\partial \mathcal{L} (\boldsymbol{w}, b, \lambda_{i})}{\partial \lambda_{i}} = 0
\ese

Starting with the first one, quite straightforward we obtain:
\bse
\nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) = \boldsymbol{w} - \sum_{i} \lambda_{i} y^{(i)}
\boldsymbol{x}^{(i)} = 0
\ese

which implies:
\bse
\boldsymbol{w} = \sum_{i} \lambda_{i} y^{(i)} \boldsymbol{x}^{(i)}
\ese

As for the derivative with respect to $b$, we obtain:
\bse
\frac{\partial \mathcal{L}(\boldsymbol{w}, b, \lambda_{i})}{\partial b} = \sum_{i} \lambda_{i} y^{(i)} = 0
\ese

By manipulating the initial Lagrangian a bit we get:
{\setlength{\jot}{2pt}
\begin{align*}
\mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) & = \frac{1}{2} || \boldsymbol{w} ||^2 - \sum_{i} \lambda_{i}
\left(y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) - 1\right) \\
& = \frac{1}{2} || \boldsymbol{w} ||^2 - \sum_{i} \lambda_{i} y^{(i)} \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}
- b \sum_{i} \lambda_{i} y^{(i)} + \sum_{i} \lambda_{i}
\end{align*}}

By substituting the results from the two first derivatives in the previous expression of the Lagrangian we end up to:
{\setlength{\jot}{5pt}
\begin{align*}
\mathcal{L}(\boldsymbol{w}, b, \lambda_{i}) & = \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
\left( \boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} - \sum_{i} \lambda_{i} y^{(i)}
\left( \sum_{j} \lambda_{j} y^{(j)} \left( \boldsymbol{x}^{(j)} \right)^{\intercal} \right) \boldsymbol{x}^{(i)}
+ \sum_{i} \lambda_{i} \\
& = \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
\left( \boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} - \sum_{i} \sum_{j} \lambda_{i}
\lambda_{j} y^{(i)} y^{(j)} \left(\boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} + \sum_{i}\lambda_{i} \\
& = \sum_{i} \lambda_{i} - \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
\left(\boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)}
\end{align*}}

Hence, now the Lagrangian reads:
\bse
\mathcal{L}(\lambda_{i}) = \sum_{i} \lambda_{i} - \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
\left( \boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)}
\ese

where the dependencies on the parameters $\boldsymbol{w}$ and $b $ are gone and the Lagrangian is just a function of
the KKT multipliers. So we ended up with the so called ``dual optimization problem'' which is:
\bse
\max_{\lambda_{i}} \left( \sum_{i} \lambda_{i} - \frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
\left( \boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} \right)
\ese

subject to:
\bse
\lambda_{i} \geq 0, \:\:\: i=1,2,\ldots, m \qquad \text{and} \qquad \sum_{i} \lambda_{i} y^{(i)} = 0
\ese

where as per usual we can turn the maximization to minimization by multiplying the Lagrangian with a minus sign so we
end up with:
\bse
\min_{\lambda_{i}} \left(\frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
\left(\boldsymbol{x}^{(i)} \right)^{\intercal} \boldsymbol{x}^{(j)} - \sum_{i} \lambda_{i} \right)
\ese

subject to:
\bse
\lambda_{i} \geq 0, \:\:\: i=1,2,\ldots, m \qquad \text{and} \qquad \sum_{i} \lambda_{i} y^{(i)} = 0
\ese

\v

As usual we can rewrite everything in form of matrices. By introducing the matrix $Q$ and the KKT multipliers vector
$\boldsymbol{\lambda}$:
\bse
Q = \begin{bmatrix}
y^{(1)} y^{(1)} \left( \boldsymbol{x}^{(1)} \right)^{\intercal} \boldsymbol{x}^{(1)} & \ldots & y^{(1)} y^{(m)}
\left( \boldsymbol{x}^{(1)} \right)^{\intercal} \boldsymbol{x}^{(m)} \\
y^{(2)} y^{(2)} \left( \boldsymbol{x}^{(2)} \right)^{\intercal} \boldsymbol{x}^{(2)} & \ldots & y^{(2)} y^{(m)}
\left( \boldsymbol{x}^{(2)} \right)^{\intercal}\boldsymbol{x}^{(m)} \\
\vdots & \vdots & \ddots & \ldots \\
y^{(m)} y^{(1)} \left( \boldsymbol{x}^{(m)} \right)^{\intercal} \boldsymbol{x}^{(1)} & \ldots & y^{m)} y^{(m)}
\left( \boldsymbol{x}^{(m)} \right)^{\intercal} \boldsymbol{x}^{(m)}
\end{bmatrix}, \qquad
\boldsymbol{\lambda} = \begin{bmatrix} \lambda_{1} \\ \lambda_{2} \\ \vdots \\ \lambda_{m} \end{bmatrix}
\ese

\v

we can rewrite the dual optimization problem as:
\bse
\min_{\boldsymbol{\lambda}} \left( \frac{1}{2} \boldsymbol{\lambda}^\intercal Q \boldsymbol{\lambda}
- \boldsymbol{\lambda} \right)
\ese

subject to:
\bse
\boldsymbol{\lambda} \geq 0 \qquad \text{and} \qquad \boldsymbol{y}^\intercal \boldsymbol{\lambda} = 0
\ese

\v

This final dual optimization problem is usually solved numerically through quadratic programming (as for example we
use gradient descent for loss functions optimization problems). Once we solve it we obtain the KKT multipliers vector
$\boldsymbol{\lambda}$ and subsequently all the individual KKT multipliers $\lambda_{i}$ from its components. We also
notice that almost all $\lambda_{i}$'s are $0$ except from a few ones where $\lambda_{i} > 0$ coming from the support
vectors. By making use of the KKT multipliers we find the parameter $\boldsymbol{w}$ as:
\bse
\boldsymbol{w} = \sum_{i} \lambda_{i} y^{(i)} \boldsymbol{x}^{(i)}
\ese

We once again notice that the only terms that survive are the ones with $\lambda_{i} > 0$ coming from the support
vectors. All the other points do not contribute at all to the model. \v

Once we have $\boldsymbol{w}$ we can compute $b$ from the fact that for support vectors their distance from the plane
is equal to $1$, i.e.\ :
\bse
y^{(i)} (\boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)} + b) = 1
\ese

hence:
\bse
b = \frac{1}{y^{(i)}} - \boldsymbol{w}^{\intercal} \boldsymbol{x}^{(i)}
\ese

\v

for any $i$ that is a support vector. This last equation should produce the same parameter $b$ for all support
vectors. This is a good way to check that the models works fine. \v

By having found both $\boldsymbol{w}$ and $b$, we have found the separator $\boldsymbol{w}^{\intercal} \boldsymbol{x}
+ b$ and we are in the position to predict to which class a new point belongs. Since $\boldsymbol{w}^{\intercal}
\boldsymbol{x} + b$ is going to be either negative (if the point lies below the plane) or positive (if the point lies
above the plane) we finally have for the prediction:
\bse
h(\boldsymbol{x}) = sign(\boldsymbol{w}^{\intercal} \boldsymbol{x} + b)
\ese

This model of classification is called ``hard margin support vector machine''. A careful reader would notice that
in hard margin support vector machine we strictly impose that all instances must be off the street and on the right
side, i.e.\ no violations are allowed (Hence, the naming). There are two main issues with hard margin classification.
First, it only works if the data is linearly separable. Second, it is sensitive to outliers. \v

However, in real life the data are almost never linearly separable and the hard margin support vector machine would
not be able to find a solution. \v

To avoid these issues, one should use a more flexible model. The objective is to find a good balance between keeping
the street as large as possible and limiting the margin violations (i.e.\ instances that end up in the middle of
the street or even on the wrong side). \v

Fortunately the generalization of hard margin support vector machine to non-linear separable is easy. Without getting
into details two of the ways we can achieve this are the following.
\bit
\item \textbf{Non-Linear Transformations \& Kernels} By switching from a space $X$ where the data are not linear
separable to a space $Z$ where the data are linear separable through a transformation $\Phi$:
\bse
(\boldsymbol{x_1}, \boldsymbol{x_2}, \ldots, \boldsymbol{x_n}) \xrightarrow{\Phi}{}
(\boldsymbol{z_1}, \boldsymbol{z_2}, \ldots, \boldsymbol{z_n})
\ese

Accordingly the dual optimization problem would change to:
\bse
\min_{\lambda_{i}} \left(\frac{1}{2} \sum_{i} \sum_{j} \lambda_{i} \lambda_{j} y^{(i)} y^{(j)}
\left(\boldsymbol{z}^{(i)} \right)^{\intercal} \boldsymbol{z}^{(j)} - \sum_{i} \lambda_{i} \right)
\ese

subject to:
\bse
\lambda_{i} \geq 0, \:\:\: i=1,2,\ldots, m \qquad \text{and} \qquad \sum_{i} \lambda_{i} y^{(i)} = 0
\ese

and then we would follow the same procedure as before in order to obtain a solution. \v

The biggest caveat of this method is the inner product $\left (\boldsymbol{z}^{(i)} \right)^{\intercal}
\boldsymbol{z}^{(j)}$ that needs to be computed in $Z$ space and it's computationally expensive. One way to overcome
this problem is by using the trick of kernels (Appendix~\ref{ch:kernels}) in which we substitute the inner product
with a kernel:
\bse
\left(\boldsymbol{z}^{(i)} \right)^{\intercal} \boldsymbol{z}^{(j)} = K (\boldsymbol{x}^{(i)}, \boldsymbol{x}^{(j)})
\ese

Given the freedom of choosing a kernel the model turns quite flexible.

\item \textbf{L1 Regularization \& Soft Margin SVM} While mapping data to a high dimensional feature space via $\Phi$
does generally increase the likelihood that the data is separable, we can't guarantee that it always will be so.
Also, in some cases it is not clear that finding a separating hyperplane is exactly what we'd want to do, since that
might be susceptible to outliers. \v

To make the algorithm work for non-linearly separable datasets as well as be less sensitive to outliers, we can
simply impose the technique of L1 Regularization that we mentioned in the previous chapter. That way our model is
more open to errors and it allows some wrongly labeled data. \v

\fig{svm4}{0.6}

In this case the final model is called ``soft margin support vector machine'' (in contrast with the SVM we
developed in the beginning called ``hard margin support vector machine''), due to the fact that it allows the
margin to be violated by outliers.
\eit

Now let's move on to a new subject.

\section{$k$-Nearest Neighbors}

In this section we will introduce yet another famous supervised learning algorithm called ``k-nearest neighbors
algorithm'' ($k$-NN), which is a non-parametric\footnote{A model is called ``nonparametric'' not because it does not
have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so
the model structure is free to stick closely to the data. In contrast, a parametric model, such as a linear model,
has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting
(but increasing the risk of underfitting).} method used for classification and regression. In both cases, the
input consists of the $k$ closest training examples in the feature space, since the algorithm assumes that similar
things exist in close proximity (in other words, similar things are near to each other). The output depends on
whether $k$-NN is used for classification or regression:
\bit
\item In $k$-NN classification, the output is a class membership. An object is classified by a plurality vote of its
neighbors, with the object being assigned to the class most common among its $k$ nearest neighbors ($k$ is a positive
integer, typically small). If $k = 1$, then the object is simply assigned to the class of that single nearest neighbor.
\item In $k$-NN regression, the output is the property value for the object. This value is the average of the values
of $k$ nearest neighbors.
\eit

One good question is ``how to measure the distance between two points''? There are many ways of calculating
distance, and one way might be preferable depending on the problem we are solving. However, the straight-line
distance (i.e.\ the Euclidean distance) is a popular and familiar choice. \v

Another good questions is ``how many neighbors should I choose for the model''? In other words what should be the
value of $k$? The best choice of $k$ depends upon the data. Generally, larger values of $k$ reduces effect of the
noise on the classification, but make boundaries between classes less distinct. A good $k$ can be selected by various
heuristic techniques. In binary classification problems, it is helpful to choose $k$ to be an odd number as this
avoids tied votes. One popular way of choosing the empirically optimal $k$ in this setting is via bootstrap method.
The special case where the class is predicted to be the class of the closest training sample (i.e.\ when $k = 1$) is
called the ``nearest neighbor'' algorithm. \v

\fig{knn}{0.55}

$k$-NN has many advantages. First of all the algorithm is very simple and easy to implement. Also, there is no need
to build a model, tune several parameters, or make additional assumptions. Finally the algorithm is versatile, i.e.\
it can be used for classification, regression. However, of course, it carries and some disadvantages. One has to
keep in mind that the algorithm gets significantly slower as the number of examples and/or predictors/independent
variables increase and that the accuracy of $k$-NN algorithm can be severely degraded by the presence of noisy or
irrelevant features, or if the feature scales are not consistent with their importance. \v

A drawback of the basic ``majority voting'' classification occurs when the class distribution is skewed. That is,
examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common
among the $k$ nearest neighbors due to their large number. In order to overcome this problem (both for classification
and regression), a useful technique can be to assign weights to the contributions of the neighbors, so that the
nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme
consists in giving each neighbor a weight of $\frac{1}{d}$, where $d$ is the distance to the neighbor. \v

One final, general comment is in order. Up to this point, all the algorithms we developed were model-based algorithms
since in each of them we built a model based on the data. $k$-NN is the first instance-based algorithm we introduce,
where the function is only approximated locally and all computation is deferred until function evaluation. In other
words, the neighbors are taken from a set of objects for which the class is known. This can be thought of as the
training set for the algorithm, though no explicit training step is required. This has as a consequence for the
$k$-NN algorithm to be sensitive to the local structure of the data. Since this algorithm relies on distance for
classification, normalizing the training data can improve its accuracy dramatically.

\section{Decision Trees \& Random Forests}

Like SVM and $k$-NN, decision trees are versatile, non-parametric machine learning algorithms that can perform both
classification and regression tasks. They go from observations about an item (represented in the branches) to
conclusions about the item's target value (represented in the leaves). They are powerful algorithms, capable of
fitting complex datasets. Decision trees are also the fundamental components of random forests (that we will see
right after), which are among the most powerful machine learning algorithms available today.

\subsection{Decision Trees}

\bd [Decision Tree]
A \textbf{decision tree} is a decision support tool that uses a flowchart-like tree-like structure of decisions and
their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display
an algorithm that only contains conditional control statements.
\ed

The flowchart-like structure of a decision tree helps in decision making. It's a visualization like a flowchart
diagram which easily mimics the human-level thinking. That is why decision trees are easy to understand and interpret. \v

Now let's give some basic definitions.

\bd [Node]
A \textbf{node} of a decision tree represents a feature.
\ed

\bd [Root Node]
The topmost node in a decision tree is known as the \textbf{root node}, or top decision node, of the decision tree.
It represents the entire population or sample and this further gets divided into two or more homogeneous sets.
\ed

The root node learns to partition based on the attribute value. It partitions the tree in a recursive manner called
``recursive partitioning''.

\bd [Splitting]
\textbf{Splitting} is a process of dividing a node into two or more sub-nodes.
\ed

\bd [Pruning]
\textbf{Pruning} is the process when one reduces the size of decision trees by removing nodes (opposite of splitting).
\ed

\bd [Decision Node]
When a sub-node splits into further sub-nodes, then it is called a\textbf{decision node}.
\ed

\bd [Parent / Child Node]
A node which is divided into sub-nodes is called a \textbf{parent node} of sub-nodes whereas sub-nodes are the
\textbf{child} of a parent node.
\ed

\bd [Leaf]
A \textbf{leaf}, or terminal node, is a node with no children (no further split) and it represents the outcome of a
decision rule.
\ed

\bd [Branch]
A \textbf{branch}, or subtree, is a subsection of the decision tree and it represents a decision rule.
\ed

\vspace{-10pt}

\fig{decisiontree}{0.6}

Decision trees classify (or regress) the examples by sorting them down the tree, from the root to some leaf node,
with the leaf node providing the classification (or regression) to the example. This approach is called a
``top-down'' approach. Each node in the tree acts as a test case for some attribute, and each edge descending from
that node corresponds to one of the possible answers to the test case. \v

The algorithm begins by selecting the best attribute using something called ``attribute selection measures'' to
split the records. Then it makes that attribute a decision node and breaks the dataset into smaller subsets. Is
starts tree building by repeating this process recursively for each child until one of the condition will match:
\bit
\item All the tuples belong to the same feature value.
\item There are no more remaining feature.
\item There are no more instances.
\eit

Attribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best
possible manner. It is also known as ``splitting rules'' because it helps us to determine breakpoints for tuples on
a given node. Attribute selection measure provides a rank to each feature by explaining the given dataset. Best score
attribute will be selected as a splitting attribute. In the case of a continuous-valued attribute, split points for
branches also need to define. \v

Two of the most popular selection measures are: ``information gain'', and ``Gini impurity'' and we will now
explain what they are. Both information gain and Gini impurity measure the impurity of a node. So let's start by
defining what impurity is.

\bd [Node Impurity]
\textbf{Node impurity} is a measure of the homogeneity of the labels at the node.
\ed

\bd [Pure / Impure Node]
A node is called \textbf{pure} when all of its data belongs to a single class. Subsequently a node is called
\textbf{impure} when not all of its data belongs to a single class. A node is 100\% impure when a node is split
evenly 50/50.
\ed

\be
As an example, in the figure below C requires less information to describe as all values are similar. On the other
hand, B requires more information to describe it and A requires the maximum information. In other words, we can say
that C is a pure node, B is less impure and A is more impure.

\fig{purity}{0.5}
\ee

As a final step we need to introduce the concept of information entropy.

\bd [Information Entropy]
In information theory, the \textbf{information entropy}, or simply entropy, of a random variable is the average level
of information inherent in the variable's possible outcomes. Given a discrete random variable $X$, with possible
outcomes $\{x_{1}, \ldots, x_{n} \}$ which occur with probability $ P(x_{1}), \ldots, P (x_{n})$, the entropy of $X$
is formally defined as:
\bse
H(X) = -\sum _{i=1}^{n} {P(x_{i}) \log P(x_{i})}
\ese

\v

where the summation is over the variable's possible values.
\ed

Having defined entropy we can now define information gain.

\bd [Information Gain]
\textbf{Information gain} $IG$ is the change in information entropy $H$ from a prior state $T$ to a state that takes
some information $a$ as given:
\bse
IG(T,a) =H(T) - H(T|a)
\ese

where $H(T)$ is the entropy of the parent node and $H(T|a)$ is the sum of the entropies of all children nodes.
\ed

Informally speaking, information gain is a statistical property that measures how well a given attribute separates
the training examples according to their target classification. It is a synonym for ``Kullback-Leibler
divergence'', that we introduced in the parametric inference chapter,, which shows the amount of information gained
about a random variable or signal from observing another random variable. However, in the context of decision trees,
the term is sometimes used synonymously with mutual information, which is the conditional expected value of the
Kullback-Leibler divergence of the univariate probability distribution of one variable from the conditional
distribution of this variable given the other one. \v

In the figure below, we can see that a feature with low information gain splits the data relatively evenly and as a
result doesn't bring us any closer to a decision.

\fig{purity2}{0.5}

Whereas, an attribute with high information gain splits the data into groups with an uneven number of positives and
negatives and as a result, helps in separating the two from each other.

\fig{purity3}{0.5}

In other words we are always looking to maximize the information gain after any splitting. Having defined information
gain we can now define Gini impurity.

\bd [Gini Impurity]
\textbf{Gini impurity}, or Gini index, $I_{G}$ measures the degree or probability of a particular variable being
wrongly classified when it is randomly chosen. Given a discrete random variable $X$, with possible outcomes $\{x_{1},
x_{2}, \ldots, x_{n} \}$ which occur with probability $ P(x_{1}), P(x_{2}), \ldots, P(x_{n})$, the Gini impurity of
$X$ is formally defined as:
\bse
I_{G}(X) = 1 - \sum _{i=1}^{n} P^2(x_{i})
\ese

\v

where the summation is over the variable's possible values.
\ed

Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was
randomly labeled according to the distribution of labels in the subset. \v

Now that we have defined both information gain and Gini impurity, a good question is ``which one should one use''? In
general keep in mind that it only matters in 2\% of the cases whether you use Gini impurity or information gain. And
given that information gain uses information entropy (which uses logarithms), it might be a little slower to compute.
Hence, most of the algorithms use Gini impurity as a selection measure. \v

In any case, by using either information gain or Gini impurity one can start building (training) the tree. The
building (training) is usually done through a procedure called ``Classification and Regression Tree'' (CART) algorithm.
The algorithm works by first splitting the training set into two subsets using a single feature $k$ and a threshold
$t_k$, by searching for the pair ($k$, $t_k$) that produces the purest subsets (weighted by their size). The loss
function that the algorithm tries to minimize is the so called ``CART loss function''. There is one CART loss function
for classification and one for regression. Let's start with the classification one.

\bd [CART Loss Function (For Classification)]
We define the \textbf{CART loss function} for classification as:
\bse
J(k, t_{k}) = \frac{m_{\text{left}}}{m} G_{\text{left}} + \frac{m_{\text{right}}}{m} G_{\text{right}}
\ese

\v

where $m$ is the total number of instances in both left and right subsets, $m_{\text{left}}$ and
$m_{\text{right}}$ are the number of instances in the left and right subset, and $G_{\text{left}}$ and
$G_{\text{right}}$ measure the impurity of the left and the right subset ($G$ could be either information gain or
Gini impurity).
\ed

For regression, CART algorithm instead of trying to split the training set in a way that minimizes impurity, it now
tries to split the training set in a way that minimizes the MSE. Hence, the corresponding CART loss function for
regression looks like the following.

\bd [CART Loss Function (For Regression)]
We define the \textbf{CART loss function} for regression as:
\bse
J(k, t_{k}) = \frac{m_{\text{left}}}{m} \text{MSE}_{\text{left}}
+ \frac{m_{\text{right}}}{m}\text{MSE}_{\text{right}}
\ese

where $m$ is the total number of instances in both left and right subsets, $m_{\text{left}}$ and
$m_{\text{right}}$ are the number of instances in the left and right subset, and $\text{MSE}_{\text{left}}$ and
$\text{MSE}_{\text{right}}$ are the mean squared errors of the left and the right subsets given by:
\bse
\text{MSE}_{\text{node}} = \sum_{i \in \text{node}} ({\hat{y}}_\text{node} - y^{(i)})^2
\ese

where:
\bse
{\hat{y}}_\text{node} = \frac{1}{m_{\text{node}}} \sum_{i \in \text{node}} y^{(i)}
\ese
\ed

In both cases (classification and regression), once the CART algorithm has successfully split the training set in
two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. As we can see, the
CART algorithm is a greedy algorithm. It greedily searches for an optimum split at the top level, then repeats the
process at each subsequent level. It does not check whether or not the split will lead to the lowest possible
impurity several levels down. A greedy algorithm often produces a solution that's reasonably good but not guaranteed
to be optimal. Unfortunately, finding the optimal tree requires $O(\exp(m)) $ time, making the problem intractable
even for small training sets. This is why we must settle for a ``reasonably good'' solution. \v

Once the CART algorithm finished, the building (training) part is over and the decision tree is ready. One can now
feed a new instance to the tree and by asking the corresponding question of each node they will end up in one of the
leafs, hence, to a decision. In order to have a better understanding of the procedure (although it is really simple)
we will provide two small examples (one for classification and one for regression). \v

Let's start with classification. One starts at the root node (depth 0, at the top). This node asks whether the
flower's petal length is smaller than 2.45 cm. If it is, then they move down to the root's left child node (depth 1,
left). In this case, it is a leaf node (i.e.\ it does not have any child nodes), so it does not ask any questions but
simply look at the predicted class for that node, and the decision tree predicts that the flower is a ``setosa''. Now
suppose one finds another flower, and this time the petal length is greater than 2.45 cm. They must move down to the
root's right child node (depth 1, right), which is not a leaf node, so the node asks another question: ``is the petal
width smaller than 1.75 cm''? If it is, then the flower is most likely a ``versicolor'' (depth 2, left). If not, it is
likely a ``virginica'' (depth 2, right).

\fig{iris}{0.4}

\v

The regression decision tree example looks very similar to the classification one with the main difference that
instead of predicting a class in each node, it predicts a value. Suppose you want to make a prediction for a new
instance with $x_1 = 0.6$. You traverse the tree starting at the root, and you eventually reach the leaf node that
predicts $value=0.111$. This prediction is the average target value of the 110 training instances associated with
this leaf node, and it results in a mean squared error equal to 0.015 over these 110 instances.

\v

\fig{iris2}{0.4}

\v

Decision trees are intuitive, and their decisions are easy to interpret. Such models are often called ``white box''
models. In contrast, as we will see, random forests or neural networks are generally considered ``black box'' models.
They make great predictions, and you can easily check the calculations that they performed to make these predictions,
nevertheless, it is usually hard to explain in simple terms why the predictions were made. Conversely, decision trees
provide nice, simple classification rules that can even be applied manually if need be. The main drawback of decision
trees is that they are very sensitive to small variations in the training data. Random forests can limit this
instability by averaging predictions over many trees, as we will see in a while.

\subsection{Ensemble Learning}

Suppose one poses a complex question to thousands of random people, then aggregate their answers. In many cases they
will find that this aggregated answer is better than an expert's answer. This is called the ``wisdom of the crowd''.
Similarly, if one aggregates the predictions of a group of predictors, they will often get better predictions than
with the best individual predictor. A group of predictors is called an ``ensemble'', thus, this technique is called
``ensemble learning'', and an ensemble learning algorithm is called an ``ensemble method''. \v

As an example of an ensemble method, one can train a group of decision tree classifiers, each on a different random
subset of the training set. To make predictions, one obtain the predictions of all the individual trees, then predict
the class that gets the most votes. Such an ensemble of decision trees is called a ``random forest'', and despite its
simplicity, this is one of the most powerful machine learning algorithms available today. More on random forests in
the next section. \v

An ensemble method is itself a supervised learning algorithm, because it can be trained and then used to make
predictions. The trained ensemble method, therefore, represents a single hypothesis. This hypothesis, however, is not
necessarily contained within the hypothesis space of the models from which it is built. Thus, ensemble methods can be
shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to
overfit the training data more than a single model would, but in practice, some ensemble methods tend to reduce
problems related to overfitting of the training data. \v

Empirically, ensembles tend to yield better results when there is a significant diversity among the models. Many
ensemble methods, therefore, seek to promote diversity among the models they combine. Although perhaps non-intuitive,
more random algorithms can be used to produce a stronger ensemble than very deliberate algorithms. Using a variety of
strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to
dumb-down the models in order to promote diversity. \v

While the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a
limited number of studies addressing this problem. A priori determining of ensemble size and the volume and velocity
of big data streams make this even more crucial for online ensemble classifiers. Mostly statistical tests were used
for determining the proper number of components. More recently, a theoretical framework suggested that there is an
ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers
would deteriorate the accuracy. It is called ``the law of diminishing returns in ensemble construction''. Their
theoretical framework shows that using the same number of independent component classifiers as class labels gives the
highest accuracy. \v

Let us now define the two most broad categories of ensemble learning: hard and soft voting classifiers.

\bd[Hard Voting Classifier]
An algorithm that aggregates the predictions of a collection of trained classifiers and predicts the class that gets
the most votes is called a \textbf{hard voting classifier}.
\ed

\bd[Soft Voting Classifier]
An algorithm that aggregates the predictions, among with their corresponding probabilities, of a collection of
trained classifiers and predicts the class with the highest class probability, averaged over all the individual
classifiers is called a \textbf{soft voting classifier}.
\ed

\v

\fig{ensemble}{0.55}

\v

Somewhat surprisingly, those voting classifiers often achieve a higher accuracy than the best classifier in the
ensemble. In fact, even if each classifier is a weak learner (meaning it does only slightly better than random
guessing), the ensembles can still be strong learners (achieving high accuracy), provided there are a sufficient
number of weak learners and they are sufficiently diverse. Coming to this last comment, both hard and soft voting
classifiers work best when the predictors are as independent from one another as possible. One way to get diverse
classifiers is to train them using very different algorithms. This increases the chance that they will make very
different types of errors, improving the ensemble's accuracy. \v

Another approach, in order to get diverse classifiers, is to use the same training algorithm for every predictor and
train them on different random subsets of the training set. When sampling is performed with replacement, this method
is called ``bagging'' (short for bootstrap aggregating), while when sampling is performed without replacement, it is
called ``pasting''. In other words, both bagging and pasting allow training instances to be sampled several times
across multiple predictors, but only bagging allows training instances to be sampled several times for the same
predictor (i.e.\ with bagging, some instances may be sampled several times for any given predictor, while others may
not be sampled at all). In general, bootstrapping introduces a bit more diversity in the subsets that each predictor
is trained on, so bagging ends up with a slightly higher bias than pasting, but the extra diversity also means that
the predictors end up being less correlated, so the ensemble's variance is reduced. Overall, bagging often results in
better models, which explains why it is generally preferred.

\vspace{7pt}

\fig{ensemble2}{0.45}

\v

Another ensemble method called ``boosting'', involves incrementally building an ensemble by training each new model
instance to emphasize the training instances that previous models mis-classified. In some cases, boosting has been
shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data. By
far, the most common implementation of boosting is ``AdaBoost'' (short for Adaptive Boosting) and ``Gradient Boosting''.
\v

In AdaBoost, a new predictor corrects its predecessor by paying a bit more attention to the training instances that
the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. For example,
when training an AdaBoost classifier, the algorithm first trains a base classifier (such as a decision tree) and uses
it to make predictions on the training set. The algorithm then increases the relative weight of misclassified
training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the
training set, updates the instance weights, and so on.

\vspace{7pt}

\fig{adaboost}{0.4}

\v

Just like AdaBoost, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its
predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method
tries to fit the new predictor to the residual errors made by the previous predictor. \v

One final famous ensemble method is ``stacking'', which involves training a learning algorithm to combine the
predictions of several other learning algorithms. Stacking is based on a simple idea: instead of using trivial
functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don't we train a
model to perform this aggregation? First, all of the other algorithms are trained using the available data, then a
combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as
additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the
ensemble techniques described in this article, although, in practice, a logistic regression model is often used as
the combiner. \v

Stacking typically yields performance better than any single one of the trained models. It has been successfully used
on both supervised learning tasks and unsupervised learning. It has also been used to estimate bagging's error rate.

\subsection{Random Forests}

Random forests are an ensemble learning method for classification, regression and other tasks that operate by
constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes
(classification) or mean prediction (regression) of the individual trees. \v

Random forests usually use the method of bagging for developing. So, when one is growing a tree in a random forest,
at each node only a random subset of the features is considered for splitting. Hence, random forests introduce extra
randomness when growing trees since instead of searching for the very best feature when splitting a node, they search
for the best feature among a random subset of features. The algorithm results in greater tree diversity, which trades
a higher bias for a lower variance, generally yielding an overall better model. As a result, random forests correct
for decision trees' habit of overfitting to their training set. \v

It is possible to make trees even more random by also using random thresholds for each feature rather than searching
for the best possible thresholds, like regular decision trees do. A forest of such extremely random trees is called
an ``Extremely Randomized Trees'' or ``Extra-Trees''. Once again, this technique trades more bias for a lower variance.
It also makes Extra-Trees much faster to train than regular random forests, because finding the best possible
threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. \v

Yet another great quality of random forests is that they make it easy to measure the relative importance of each
feature. As a result, random forests are very handy to get a quick understanding of what features actually matter, in
particular in feature selection. \v

Random forests are frequently used as ``blackbox'' models in businesses, as they generate reasonable predictions
across a wide range of data while requiring little configuration.