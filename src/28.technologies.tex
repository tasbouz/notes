%! suppress = Quote
%! suppress = EscapeUnderscore
%! suppress = EscapeHashOutsideCommand
%! suppress = EscapeUnderscore
%! suppress = EscapeHashOutsideCommand
%! suppress = Quote
In this chapter, I will go through some of the technologies I find interesting and use on a daily basis. For each one
of them, I will provide a short history note and then I will explain the way to use them.

\section{Serialization}

\bd[Serialization]
\textbf{Serialization} is the process of translating a data structure or object state into a format that can be stored
(for example, in a file or memory data buffer) or transmitted (for example, over a computer network) and reconstructed
later (possibly in a different computer environment).
\ed

When the resulting series of bits is reread according to the serialization format, it can be used to create a
semantically identical clone of the original object.

\bd[Deserialization]
The opposite operation, extracting a data structure from a series of bytes, is \textbf{deserialization}.
\ed

For serialization to be useful, architecture independence must be maintained. For example, for maximal use of
distribution, a computer running on a different hardware architecture should be able to reliably reconstruct a
serialized data stream, regardless of endianness. This means that the simpler and faster procedure of directly
copying the memory layout of the data structure cannot work reliably for all architectures. \v

Serializing the data structure in an architecture$-$independent format means preventing the problems of byte
ordering, memory layout, or simply different ways of representing data structures in different programming languages.

\bd[Human$-$Readable]
A \textbf{human$-$readable} format is any encoding of data or information that can be naturally read by humans.
\ed

In this section we will be focusing on human$-$readable serialization. In other words, serialization formats that can
be read by humans. Although there are many different human$-$readable serialization formats, we will go through the 4
most used ones:
\bit
\item CSV
\item JSON
\item XML
\item YAML
\eit

\subsection{CSV}

\bd[Comma Separated Value (CSV)]
A \textbf{Comma Separated Value} (\textbf{CSV}) file is a delimited text file that uses a comma to separate values.
\ed

Each line of a CSV file is a data record. Each record consists of one or more fields, separated by commas. The use of
the comma as a field separator is the source of the name for this file format. A CSV file typically stores tabular
data (numbers and text) in plain text, in which case each line will have the same number of fields. \v

The CSV file format is not fully standardized. Separating fields with commas is the foundation, but commas in the
data or embedded line breaks have to be handled specially. Some implementations disallow such content while others
surround the field with quotation marks, which yet again creates the need for escaping if quotation marks are present
in the data. \v

The term CSV also denotes several closely$-$related delimiter$-$separated formats that use other field delimiters
such as semicolons. These include tab$-$separated values and space$-$separated values. A delimiter guaranteed not to
be part of the data greatly simplifies parsing. These alternative delimiter$-$separated files are often given a
\code{.csv} extension despite the use of a non$-$comma field separator. This loose terminology can cause problems in
data exchange. Many applications that accept CSV files have options to select the delimiter character and the
quotation character. \v

CSV is widely supported by consumer, business, and scientific applications. Among its most common uses is moving
tabular data between programs that natively operate on incompatible (often proprietary or undocumented) formats. \v

Typical rules other CSV specifications and implementations are as follows:
\bit
\item CSV is a delimited data format that has fields/columns separated by the comma character and records/rows
terminated by newlines.
\item A CSV file does not require a specific character encoding, byte order, or line terminator format.
\item A record ends at a line terminator. However, line terminators can be embedded as data within fields, so software
must recognize quoted line$-$separators in order to correctly assemble an entire record from perhaps multiple lines.
\item All records should have the same number of fields, in the same order.
\item Data within fields is interpreted as a sequence of characters, not as a sequence of bits or bytes.
\item Adjacent fields must be separated by a single comma. However, CSV formats vary greatly in this choice of separator
character. In particular, in locales where the comma is used as a decimal separator, a semicolon, tab, or other
character is used instead.
\item Any field may be quoted (that is, enclosed within double$-$quote characters), while some fields must be quoted, as
specified in the following rules:
\bit
\item Fields with embedded commas or double$-$quote characters must be quoted.
\item Each of the embedded double$-$quote characters must be represented by a pair of double$-$quote characters.
\item Fields with embedded line breaks must be quoted.
\item In some CSV implementations, leading and trailing spaces and tabs are trimmed (ignored).
\item Spaces outside quotes in a field are not allowed.
\item In CSV implementations that do trim leading or trailing spaces, fields with such spaces as meaningful data must be
quoted.
\item Double quote processing need only apply if the field starts with a double quote.
\item The first record may be a header, which contains column names in each of the fields (there is no reliable way to
tell whether a file does this or not; however, it is uncommon to use characters other than letters, digits, and
underscores in such column names).
\eit
\eit

Here is an example of a CSV file:

\begin{block}
Year,Make,Model,Description,Price
1997,Ford,E350,"ac, abs, moon",3000.00
1999,Chevy,"Venture ""Extended Edition""","",4900.00
1999,Chevy,"Venture ""Extended Edition, Very Large""","",5000.00
1996,Jeep,Grand Cherokee,"MUST SELL! air, moon roof, loaded",4799.00
\end{block}

The above CSV of data may be represented in table format as follows:

\fig{img/csv}{0.65}

\subsection{JSON}

\fig{img/json}{0.19}

\bd[JavaScript Object Notation (JSON)]
\textbf{JavaScript Object Notation} (\textbf{JSON}) is an open standard file format and data interchange format that
uses human$-$readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other
serializable values).
\ed

JSON grew out of a need for a stateless, real$-$time server$-$to$-$browser communication protocol without using
browser plugins such as Flash or Java applets, the dominant methods used in the early 2000s. Douglas Crockford
originally specified and popularized the JSON format in the early 2000s. \v

JSON was based on a subset of the JavaScript scripting language and is commonly used with JavaScript, but it is a
language$-$independent data format. Code for parsing and generating JSON data is readily available in many
programming languages. JSON filenames use the extension \code{.json}. Any valid JSON file is a valid JavaScript file,
even though it makes no changes to a web page on its own. \v

The format of a JSON is very simple: it's a collection of key–value pairs separated by a colon, each one in each own
line. Indentation is not important but it is commonly used to help in human readability. JSON's basic data types are:
\bit
\item Number: a signed decimal number that may contain a fractional part, but cannot include non$-$numbers such as NaN\@.
\item String: a sequence of zero or more Unicode characters. Strings are delimited with double quotation marks and
support a backslash escaping syntax.
\item Boolean: either of the values true or false.
\item Array: an ordered list of zero or more elements, each of which may be of any type. Arrays use square bracket
notation with comma$-$separated elements.
\item Object: a collection of name–value pairs where the names (also called keys) are strings. Objects are notated with
curly brackets.
\item null: an empty value, using the word null.
\eit

The following example shows a possible JSON representation describing a person:

\begin{block}
{
  "firstName": "John",
  "lastName": "Smith",
  "isAlive": true,
  "age": 27,
  "address": {
    "streetAddress": "21 2nd Street",
    "city": "New York",
    "state": "NY",
    "postalCode": "10021-3100"
  },
  "phoneNumbers": [
    {
      "type": "home",
      "number": "212 555-1234"
    },
    {
      "type": "office",
      "number": "646 555-4567"
    }
  ],
  "children": [
      "Catherine",
      "Thomas",
      "Trevor"
  ],
  "spouse": null
}
\end{block}

\subsection{XML}

\fig{img/xml}{0.2}

\bd[Extensible Markup Language (XML)]
\textbf{Extensible Markup Language} (\textbf{XML}) is a markup language and file format for storing, transmitting, and
reconstructing arbitrary data, and defines a set of rules for encoding documents in a format that is both
human$-$readable and machine$-$readable.
\ed

The main purpose of XML is serialization, i.e.\ storing, transmitting, and reconstructing arbitrary data. For two
disparate systems to exchange information, they need to agree upon a file format. XML standardizes this process. XML
is analogous to a lingua franca for representing information. \v

The design goals of XML emphasize simplicity, generality, and usability across the Internet. It is a textual data
format with strong support via Unicode for different human languages. Although the design of XML focuses on
documents, the language is widely used for the representation of arbitrary data structures such as those used in web
services. As a markup language, XML labels, categorizes, and structurally organizes information. XML tags represent
the data structure and contain metadata. What's within the tags is data, encoded in the way the XML standard
specifies. \v

What follows is some components based on the XML Specification. This is not an exhaustive list of all the constructs
that appear in XML; it provides an introduction to the key constructs most often encountered in day$-$to$-$day use.
\bit
\item An XML document is a string of characters. Almost every legal Unicode character may appear in an XML document.
\item The processor analyzes the markup and passes structured information to an application. The specification places
requirements on what an XML processor must do and not do, but the application is outside its scope. The processor (as
the specification calls it) is often referred to colloquially as an XML parser.
\item The characters making up an XML document are divided into markup and content, which may be distinguished by the
application of simple syntactic rules. Generally, strings that constitute markup either begin with the character
\code{<} and end with a \code{>}. Strings of characters that are not markup are content.
\item A tag is a markup construct that begins with \code{<} and ends with \code{>}. There are three types of tags:
\bit
\item start$-$tag, such as \code{<section>}.
\item end$-$tag, such as \code{</section>}.
\item empty$-$element tag, such as \code{<line$-$break />}.
\eit
\item An element is a logical document component that either begins with a start$-$tag and ends with a matching
end$-$tag or consists only of an empty$-$element tag. The characters between the start$-$tag and end$-$tag, if any,
are the element's content, and may contain markup, including other elements, which are called child elements.
\item An attribute is a markup construct consisting of a name–value pair that exists within a start$-$tag or
empty$-$element tag. An XML attribute can only have a single value and each attribute can appear at most once on each
element. In the common situation where a list of multiple values is desired, this must be done by encoding the list
into a well$-$formed XML attribute with some format beyond that XML defines itself. Usually this is either a comma or
semi$-$colon delimited list or, if the individual values are known not to contain spaces, a space$-$delimited list
can be used.
\item XML documents may begin with an XML declaration that describes some information about themselves.
\item Comments may appear anywhere in a document outside other markup. Comments cannot appear before the XML
declaration. Comments begin with \code{$<!--$} and end with \code{$-->$}.
\eit

XML has regularly been criticized for verbosity, complexity and redundancy. Mapping the basic tree model of XML to
type systems of programming languages or databases can be difficult, especially when XML is used for exchanging
highly structured data between applications, which was not its primary design goal. \v

Other criticisms attempt to refute the claim that XML is a self$-$describing language (though the XML specification
itself makes no such claim). JSON and YAML are frequently proposed as simpler alternatives that focus on representing
highly structured data rather than documents, which may contain both highly structured and relatively unstructured
content. \v

The following example shows a possible XML representation describing a book:

\begin{block}
<bookstore>
  <book category="COOKING">
    <title lang="en">Everyday Italian</title>
    <author>Giada De Laurentiis</author>
    <year>2005</year>
    <price>30.00</price>
    <code>438043534</code>
  </book>
  <book category="CHILDREN">
    <title lang="en">Harry Potter</title>
    <author>J K. Rowling</author>
    <year>2005</year>
    <price>29.99</price>
    <code>40863986</code>
  </book>
  <book category="WEB">
    <title lang="en">Learning XML</title>
    <author>Erik T. Ray</author>
    <year>2003</year>
    <price>39.95</price>
    <code>509686346</code>
  </book>
</bookstore>
\end{block}

\subsection{YAML}

\fig{img/yaml}{0.04}

\bd[YAML Ain't Markup Language (YAML)]
\textbf{YAML Ain't Markup Language} (\textbf{YAML}) is a human$-$readable data$-$serialization language.
\ed

YAML was first proposed by Clark Evans in 2001. Originally YAML was said to mean ``Yet Another Markup Language'',
because it was released in an era that saw a proliferation of markup languages for presentation and connectivity. Its
initial name was intended as a tongue$-$in$-$cheek reference to the technology landscape, referencing its purpose as
a markup language with the yet another construct, but it was then repurposed as ``YAML Ain't Markup Language'', a
recursive acronym, to distinguish its purpose as data$-$oriented, rather than document markup. \v

YAML is commonly used for configuration files and in applications where data is being stored or transmitted. It
targets many of the same communications applications as XML but has a minimal syntax. It uses both Python$-$style
indentation to indicate nesting, and a more compact format that uses \code{$[\ldots]$} for lists and
\code{$\{\ldots\}$} for maps, thus JSON files are valid YAML. \v

Custom data types are allowed, but YAML natively encodes scalars (such as strings, integers, and floats), lists, and
associative arrays (also known as maps, dictionaries or hashes). These data types are based on the Perl programming
language, though all commonly used high$-$level programming languages share very similar concepts. YAML is intended
to be read and written in streams. \v

Support for reading and writing YAML is available for many programming languages. Some source$-$code editors such as
Vim, Emacs,and various integrated development environments have features that make editing YAML easier, such as
folding up nested structures or automatically highlighting syntax errors. \v

The official recommended filename extension for YAML files has been \code{.yaml} since 2006. \v

The following is a synopsis of the basic elements:
\bit
\item YAML accepts the entire Unicode character set, except for some control characters.
\item Whitespace indentation is used for denoting structure.
\item Comments begin with the number sign \code{$\#$}, can start anywhere on a line and continue until the end of the
line. Comments must be separated from other tokens by whitespace characters. If \code{$\#$} characters appear inside
of a string, then they are number sign \code{$\#$} literals.
\item List members are denoted by a leading hyphen \code{$-$} with one member per line.
\item A list can also be specified by enclosing text in square brackets \code{[$\ldots$]} with each entry separated by a
comma.
\item An associative array entry is represented using colon space in the form key: value with one entry per line. YAML
requires the colon be followed by a space.
\item A question mark can be used in front of a key, in the form \code{?key: value} to allow the key to contain leading
dashes, square brackets, etc, without quotes.
\item An associative array can also be specified by text enclosed in curly braces \code{$\{\ldots\}$}, with keys separated from
values by colon and the entries separated by commas (spaces are not required to retain compatibility with JSON).
\item Strings are ordinarily unquoted, but may be enclosed in double$-$quotes \code{"}, or single$-$quotes \code{'}.
\item Within double$-$quotes, special characters may be represented with C$-$style escape sequences starting with a
backslash \code{$\textbackslash$}.
\item Within single quotes the only supported escape sequence is a doubled single quote \code{"}.
\item Block scalars are delimited with indentation with optional modifiers to preserve \code{|} or fold \code{>}
newlines.
\item Multiple documents within a single stream are separated by three hyphens \code{\texttt{$-${}$-${}$-$}}.
\item Three periods \code{$\ldots$} optionally end a document within a stream.
\item Repeated nodes are initially denoted by an ampersand \code{$\&$} and thereafter referenced with an asterisk *.
\item Nodes may be labeled with a type or tag using a double exclamation mark \code{!!} followed by a string, which can
be expanded into a URI\@.
\item YAML documents in a stream may be preceded by directives composed of a percent sign \code{$\%$} followed by a name
and space$-$delimited parameters.
\eit

YAML has been criticized for its significant whitespace, confusing features, insecure defaults, and its complex and
ambiguous specification:
\bit
\item Configuration files can execute commands or load contents without the users realizing it.
\item Editing large YAML files is difficult, as indentation errors can go unnoticed.
\item Type autodetection is a source of errors. For example, unquoted \code{YES} and \code{NO} are converted to
booleans; software version numbers might be converted to floats.
\item Truncated files are often interpreted as valid YAML due to the absence of terminators.
\item The complexity of the standard led to inconsistent implementations and making the language non$-$portable.
\eit

The following example shows a possible YAML representation describing a receipt:

\begin{block}
receipt: Oz$-$Ware Purchase Invoice
date: 2012$-$08$-$06
customer:
    first_name: Dorothy
    middle_name: A\@.
    family_name: Gale
items:
    $-$ part_no: A4786
      descrip: Water Bucket (Filled)
      price: 1.47
      quantity: 4
    $-$ part_no: E1628
      descrip: High Heeled "Ruby" Slippers
      size: 8
      price: 133.7
      quantity: 1
bill$-$to:  &id001
    # symbol | preserves new lines
    street: |
            123 Tornado Alley
            Suite 16
    city: East Centerville
    state: KS
    floor: 5
    elevator: No
ship$-$to:  *id001
# symbol > folds new lines
specialDelivery:  >
    Follow the Yellow Brick
    Road to the Emerald City.
    Pay no attention to the
    man behind the curtain.
\end{block}

\section{Cryptography}

\bd[Cryptography]
\textbf{Cryptography} is the practice and study of techniques for secure communication in the presence of adversarial
behavior.
\ed

More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public
from reading private messages. Modern cryptography exists at the intersection of the disciplines of mathematics,
computer science, information security, electrical engineering, digital signal processing, physics, and others. Core
concepts related to information security (data confidentiality, data integrity, authentication, and non-repudiation)
are also central to cryptography. Practical applications of cryptography include electronic commerce, chip-based
payment cards, digital currencies, computer passwords, and military communications. \v

The two most commonly used techniques of modern cryptography are the so-called ``symmetric-key'' and ``public-key''
cryptography.

\bd[Symmetric-Key Cryptography]
\textbf{Symmetric-key cryptography} uses the same cryptographic keys for both the encryption of plaintext and the
decryption of ciphertext. \ed

In symmetric-key cryptography, the keys may be identical, or there may be a simple transformation to go between the
two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to
maintain a private information link. The requirement that both parties have access to the secret key is one of the
main drawbacks of symmetric-key encryption, in comparison to public-key encryption (also known as asymmetric-key
encryption).

\bd[Public-Key Cryptography / Assymetric Cryptography]
\textbf{Public-key cryptography}, or \textbf{asymmetric cryptography}, is the field of cryptographic systems that use
pairs of related keys. Each key pair consists of a public key and a corresponding private key. Key pairs are
generated with cryptographic algorithms based on mathematical problems termed one-way functions. Security of public-
key cryptography depends on keeping the private key secret; the public key can be openly distributed without
compromising security.
\ed

In a public-key encryption system, anyone with a public key can encrypt a message, yielding a ciphertext, but only
those who know the corresponding private key can decrypt the ciphertext to obtain the original message.

\be
For example, a journalist can publish the public key of an encryption key pair on a web site so that sources can send
secret messages to the news organization in ciphertext. Only the journalist who knows the corresponding private key
can decrypt the ciphertexts to obtain the sources' messages—an eavesdropper reading email on its way to the
journalist cannot decrypt the ciphertexts.
\ee

Public-key encryption does not conceal metadata like what computer a source used to send a message, when they sent it,
or how long it is. Public-key encryption on its own also does not tell the recipient anything about who sent a
message—it just conceals the content of a message in a ciphertext that can only be decrypted with the private key. \v

In a digital signature system, a sender can use a private key together with a message to create a signature. Anyone
with the corresponding public key can verify whether the signature matches the message, but a forger who does not
know the private key cannot find any message/signature pair that will pass verification with the public key.

\be
For example, a software publisher can create a signature key pair and include the public key in software installed on
computers. Later, the publisher can distribute an update to the software signed using the private key, and any
computer receiving an update can confirm it is genuine by verifying the signature using the public key. As long as
the software publisher keeps the private key secret, even if a forger can distribute malicious updates to computers,
they cannot convince the computers that any malicious updates are genuine.
\ee

Public key algorithms are fundamental security primitives in modern crypto-systems, including applications and
protocols which offer assurance of the confidentiality, authenticity and non-repudiability of electronic
communications and data storage.  Some public key algorithms provide key distribution and secrecy, some provide
digital signatures, and some provide both. Compared to symmetric encryption, asymmetric encryption is rather slower
than good symmetric encryption, too slow for many purposes. Today's crypto-systems (such as TLS, Secure Shell) use
both symmetric encryption and asymmetric encryption, often by using asymmetric encryption to securely exchange a
secret key which is then used for symmetric encryption.

\subsection{Secure Shell (SSH)}

\bd[Secure Shell (SSH)]
The \textbf{Secure Shell (SSH)} is a cryptographic network protocol designed on Unix-like operating systems in 1995
by Finnish computer scientist Tatu Ylönen, for operating network services securely over an unsecured network. SSH uses
public-key cryptography to authenticate the remote computer and allow it to authenticate the user, if necessary.
\ed

\bd[SSH Client]
An \textbf{SSH client} is a software program which uses the secure shell protocol to connect to a remote computer. This
article compares a selection of notable clients.
\ed

\bd[SSH Server]
An \textbf{SSH server} is a software program which uses the secure shell protocol to accept connections from remote
computers.
\ed

SSH applications are based on a client–server architecture, connecting an SSH client with an SSH server. SSH operates
as a layered protocol suite comprising three principal hierarchical components: the transport layer provides server
authentication, confidentiality, and integrity; the user authentication protocol validates the user to the server;
and the connection protocol multiplexes the encrypted tunnel into multiple logical communication channels. \v

SSH may be used in several methodologies. In the simplest manner, both ends of a communication channel use
automatically generated public-private key pairs to encrypt a network connection, and then use a password to
authenticate the user. \v

When the public-private key pair is generated by the user manually, the authentication is essentially performed when
the key pair is created, and a session may then be opened automatically without a password prompt. In this scenario,
the public key is placed on all computers that must allow access to the owner of the matching private key, which the
owner keeps private. While authentication is based on the private key, the key is never transferred through the
network during authentication. SSH only verifies that the same person offering the public key also owns the matching
private key. \v

In all versions of SSH it is important to verify unknown public keys, i.e.\ associate the public keys with identities,
before accepting them as valid. Accepting an attacker's public key without validation will authorize an
unauthorized attacker as a valid user. \v

On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is
allowed to log in remotely, in the file \code{~/.ssh}. This file is respected by SSH only if it is not writable by
anything apart from the owner and root. When the public key is present on the remote end and the matching private key
is present on the local end, typing in the password is no longer required. However, for additional security the private
key itself can be locked with a passphrase. The private key can also be looked for in standard places, and its full path
can be specified as a command line setting as we will see later. \v

SSH also supports password-based authentication that is encrypted by automatically generated keys. In this case, the
attacker could imitate the legitimate server side, ask for the password, and obtain it (man-in-the-middle attack).
However, this is possible only if the two sides have never authenticated before, as SSH remembers the key that the
server side previously used. The SSH client raises a warning before accepting the key of a new, previously unknown
server. Password authentication can be disabled from the server side. \v

SSH supports several public key algorithms for authentication keys. These include:
\bit
\item \textbf{rsa}: An old algorithm based on the difficulty of factoring large numbers. A key size of at least 2048
bits is recommended for RSA; 4096 bits is better. RSA is getting old and significant advances are being made in
factoring. Choosing a different algorithm may be advisable. It is quite possible the RSA algorithm will become
practically breakable in the foreseeable future. All SSH clients support this algorithm.
\item \textbf{dsa}: An old US government Digital Signature Algorithm. It is based on the difficulty of computing
discrete logarithms. A key size of 1024 would normally be used with it. DSA in its original form is no longer
recommended.
\item \textbf{ecdsa}: A new Digital Signature Algorithm standardized by the US government, using elliptic curves. This
is probably a good algorithm for current applications. Only three key sizes are supported: 256, 384, and 521 bits. We
would recommend always using it with 521 bits, since the keys are still small and probably more secure than the smaller
keys (even though they should be safe as well). Most SSH clients now support this algorithm.
\item \textbf{ed25519}: This is a new algorithm added in OpenSSH. Support for it in clients is not yet universal.
Thus, its use in general purpose applications may not yet be advisable.
\eit

The \code{ssh-keygen} utility produces the public and private keys, always in pairs.

\bd[ssh-keygen]
\textbf{ssh-keygen} is a standard utility component of SSH protocol suite used to s used to generate, manage, and
convert authentication keys.
\ed

Here's a list of commonly used flags to the \code{ssh-keygen} tool:
\bit
\item \code{-b}: \textbf{Bits}: Specifies the number of bits in the key to create. For RSA keys, the minimum size is
1024 bits and the default is 3072 bits. Generally, 3072 bits is considered sufficient. DSA keys must be exactly 1024
bits. For ECDSA keys, the flag determines the key length by selecting from one of three elliptic curve sizes: 256,
384 or 521 bits. Attempting to use bit lengths other than these three values for ECDSA keys will fail. ECDSA-SK,
Ed25519 and Ed25519-SK keys have a fixed length and the flag will be ignored.
\item \code{-t}: \textbf{Type}: This option specifies the type of algorithm to be used in order for the key to be
created. Commonly used values are: \code{rsa}, \code{dsa}, \code{ecdsa}, and \code{ed25519}.
\item \code{-f}: \textbf{File}: Specifies the filename of the key file.
\item \code{-C}: \textbf{Comment}: Changes the comment included in the public key for a keyfile.
\eit

\begin{bash}
# create public-private key pair
ssh$-$keygen $-$b <bytes> $-$t <algorithm> $-$f <filename> $-$C <comment>
\end{bash}

\section{Homebrew}

\fig{img/brew}{0.04}

\bd[Homebrew]
Homebrew is a non$-$profit, free and open$-$source software package management system, originally written by Max Howell,
that simplifies the installation of software on Apple's operating system, as well as Linux.
\ed

Homebrew has been
recommended for its ease of use as well as its integration into the command line interface. Homebrew has made extensive
use of GitHub to expand the support of several packages through user contributions.

\subsection{Installation}
Homebrew can be installed by following the instruction in its \href{https://brew.sh}{official webpage}:

\begin{bash}
# install Homebrew
/bin/bash $-$c $``\$$(curl $-$fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)$"$
\end{bash}

Once it is installed, it needs to be added to the \code{PATH} in order to work:

\begin{bash}
# add Homebrew to PATH
echo `export PATH=$``$/opt/homebrew/bin:$\$${HOME}/.bin:$\$$PATH$"$' >> $\$$HOME/.zshrc
\end{bash}

\begin{bash}
# make sure Homebrew is installed properly and ready to brew
brew doctor
\end{bash}

\subsection{Package Management}
Homebrew is just a package manager. One of the main things one can do is to search for packages:

\begin{bash}
# search for a specific package
brew search <package>
\end{bash}

\begin{bash}
# see info of a package
brew info <package>
\end{bash}

Once a package is found then it can be installed by:

\begin{bash}
# install package
brew install <package>
\end{bash}

On top of packages Homebrew can install also applications using the \code{--cask}:

\begin{bash}
# install application
brew install --cask <application>
\end{bash}

After the installation is complete one can delete all useless stuff created during installation by:

\begin{bash}
# clean old packages not used anymore
brew cleanup
\end{bash}

Keep in mind that many packages have dependencies on other packages, that will also be installed even if they are not
explicitly asked to be installed. One can see a complete list of all packages installed by:

\begin{bash}
# see all installed packages including dependencies
brew list
\end{bash}

\begin{bash}
# see all installed packages excluding dependencies
brew leaves
\end{bash}

In order to upgrade packages and applications installed via Homebrew, first one has to make sure the Homebrew itself
is up to date using:

\begin{bash}
# update Homebrew itself
brew update
\end{bash}

Then, optionally, one can see the outdated packages by:

\begin{bash}
# see outdated packages
brew outdated
\end{bash}

and outdated applications by:

\begin{bash}
# see outdated applications
brew outdated --cask --greedy
\end{bash}

The flag \code{--greedy} is needed in applications in order to include auto$-$updated applications. \v

Then upgrade all packages:

\begin{bash}
# upgrade all packages
brew upgrade
\end{bash}

and all applications by:

\begin{bash}
# upgrade all packages and applications
brew upgrade --cask --greedy
\end{bash}

One can also upgrade a specific package or application by using the same commands and specifying the package or
application in the end. \v

Note that one can upgrade both packages and applications at once using the following handy command:

\begin{bash}
# upgrade all applications
brew upgrade --greedy
\end{bash}

Finally, one can also uninstall a package or application using:

\begin{bash}
# uninstall package
brew uninstall <package>
\end{bash}

\begin{bash}
# uninstall application
brew uninstall --cask <package>
\end{bash}

Notice that \code{brew uninstall} uninstalls only the specified package/application and not its dependencies. In
order to uninstall the no longer needed dependencies one needs to run:

\begin{bash}
# uninstall all unused package dependencies
brew autoremove
\end{bash}

\section{Git}

\fig{img/git0}{0.12}

\bd[Version Control System (VCS)]
\textbf{Version Control System} (\textbf{VCS}) is a class of systems responsible for managing changes to computer
programs, documents, large web sites, or other collections of information.
\ed

The need for a logical way to organize and control revisions has existed for almost as long as writing has existed,
but revision control became much more important, and complicated, when the era of computing began. Today, the most
capable (as well as complex) revision control systems are those used in software development, where a team of people
may concurrently make changes to the same files. \v

Version control systems are most commonly run as stand$-$alone applications, but revision control is also embedded in
various types of software, such as word processors and spreadsheets, collaborative web docs, and content management
systems. \v

There are two broad categories of version control systems, centralized and distributed.

\bd[Centralized Version Control System (CVCS)]
In a \textbf{Centralized Version Control System} (\textbf{CVCS}), a server acts as the main repository which stores
every version of code.
\ed

\bd[Distributed Version Control System (CVCS)]
In a \textbf{Distributed Version Control System} (\textbf{DVCS}), is a form of version control in which the complete
codebase, including its full history, is mirrored on every developer's computer.
\ed

Compared to centralized version control, distributed version control enables automatic management branching and
merging, speeds up most operations (except pushing and pulling), improves the ability to work offline, and does not
rely on a single location for backups. \v

\bd[Git]
\textbf{Git} is a free and open source software for distributed version control, originally authored by Linus
Torvalds in 2005 for development of the Linux kernel. Its goals include speed, data integrity, and support for
distributed, non$-$linear workflows (thousands of parallel branches running on different systems).
\ed

\subsection{Installation \& Configuration}

Git can be simply installed via Homebrew:

\begin{bash}
# install Git via Homebrew
brew install git
\end{bash}

Once installed one can check the current version as usual by:

\begin{bash}
# show Git version
git --version
\end{bash}

Git carries some configuration values like name, email, etc. One can see these values by:

\begin{bash}
# show configuration list
git config --list
\end{bash}

One can set these configuration values either globally (for all repositories) or locally (for specific repository).
More often than not there is no reason to have different configurations for different repositories, for this reason
most of the time global is the way to go. \v

In order to set these values one can use:

\begin{bash}
# set global user name
git config --global user.name <user_name>
\end{bash}

\begin{bash}
# set global user email
git config --global user.mail <user_mail>
\end{bash}

One can also set a global \code{.gitignore} file (assuming there is a \code{.gitignore} file in the directory to use)
by using:

\begin{bash}
# set global ignored files
git config --global core.excludesfile ~/.gitignore
\end{bash}

More on \code{.gitignore} files later.

\subsection{Git Repository (Repo)}

\bd[Git Repository (Repo)]
A \textbf{Git repository}, or more simply a \textbf{repo}, tracks and saves the history of all changes made to the
files in a Git project.
\ed

\bd[Repository Folder]
Git saves everything related to a repo in a hidden directory called \code{.git} also known as the \textbf{repository
folder}.
\ed

Before anything, one has to turn a directory into a repo by:

\begin{bash}
# start tracking a directory with git
git init
\end{bash}

In any repo there are 3 very important areas: the working tree (or working area), the staging area (or index) and the
history area (or repository).

\bd[Working Tree / Working Area]
The \textbf{working tree}, or \textbf{working area} in Git is a directory (and its files and subdirectories) on a
file system that is associated with a Git repository. It's full of the new, edited and removed files.
\ed

Any changes to the working tree are noted by the staging area and show up as modified files.

\bd[Staging Area / Index]
The \textbf{staging area}, or \textbf{index}, is a place to record things before committing to history.
\ed

\bd[History / Repository]
The \textbf{history}, or \textbf{repository} provides information about the commit history associated with a file.
\ed

\fig{img/git1}{0.35}

In the above example there is a file called \code{S1} in the working tree. \v

In practice \code{git init} turns a directory into a repo by creating the staging area and the history inside the
repository folder \code{.git}. If one deletes \code{.git} the repo turns to a simple directory and no version control
is implemented. \v

At any point, the \code{git status} command shows the current state of the files in the repo:

\begin{bash}
# show the status of files in repository
git status
\end{bash}

A file can be in many different states. Let's start with the ``ignored'' files.

\bd[Ignored File]
\textbf{Ignored files} are files that are, and will be, completely ignored by Git.
\ed

One can ignore a file, by firstly creating a \code{.gitignore} file in the repo, and then adding the filenames to be
ignored in it.

\bd[.gitignore]
The \textbf{.gitignore} file is a special file where Git finds all the files and directories in repo that should be
ignored.
\ed

\begin{bash}
# create local gitignore file
touch .gitignore
\end{bash}

Let's move on with ``untracked'' files.

\bd[Untracked Files]
\textbf{Untracked files} are files that have been created within the repo's working tree but have not yet been added
to the repository's staging area.
\ed

An untracked file can be cleaned (i.e.\ removed) by:

\begin{bash}
# remove untracked file from working tree
git clean $-$f <file>
\end{bash}

\begin{bash}
# remove all untracked files from working tree
git clean $-$f .
\end{bash}

Finally, let's see ``Changes not staged for commit'' files.

\bd[Changes Not Staged For Commit Files]
\textbf{Changes not staged for commit files} are files the are already tracked from Git (they are not untracked
files), however they have changes in the working tree that are not in staging area.
\ed

Both untracked and changes not staged for commit files can be added to the staging area by:

\begin{bash}
# add a specific file from working tree to staging area
git add <file>
\end{bash}

\fig{img/git2}{0.35}

\begin{bash}
# add all files from current directory and beyond from working tree to staging area
git add .
\end{bash}

\begin{bash}
# add all files from all directories from working tree to staging area
git add $-$A
\end{bash}

\begin{bash}
# add only tracked files $($ignore untracked$)$ from working tree to staging area
git add $-$u
\end{bash}

After adding a file from working tree to staging area, one can keep working on working tree. By doing that, working
tree and staging area will no longer be in sync. One can see the differences by:

\begin{bash}
# show difference between working tree and staging area
git diff
\end{bash}

One can throw away these changes from working tree by copying the file from staging area by:

\begin{bash}
# copy file from staging area to working tree $($discards any non$-$staged changes in working tree$)$
git checkout <file>
\end{bash}

\fig{img/git3}{0.19}

\begin{bash}
# copy all from staging area to working tree $($discards any non$-$staged changes in working tree$)$
git checkout .
\end{bash}

After being added to staging area a file is said to be ``Changes to be committed''. Changes to be committed files,
are files the were added to the staging area from the working tree. In a sense they are in a transition state where
they are waiting to be committed to the history but they are not yet.

\begin{bash}
# show difference between staging area and latest commit in history
git diff --cached
\end{bash}

A changes to be committed file can further be committed to history by:

\begin{bash}
# commit all files from staging area to history with a message
git commit $-$m <message>
\end{bash}

\fig{img/git4}{0.35}

More often than not, one can make mistakes and realize them after he or she committed them. In this case the
\code{git commit --amend} command is a convenient way to modify the most recent commit. It lets one to simply edit
the latest commit message or combine new, uncommitted staged changes with the previous commit instead of creating an
entirely new commit.

\begin{bash}
# edit the latest commit message nothing must be staged
git commit --amend $-$m <updated_message>
\end{bash}

\begin{bash}
# edit the latest commit by adding more staged files without changing the commit message
git commit --amend --no$-$edit
\end{bash}

Notice that amended commits are actually entirely new commits and the previous commit will no longer be on the branch.
For this reason it's very important to not amend public commits that other developers might have used to create new
branches from, because that would be a very confusing situation and complicated to recover from). \v

Each commit carries a unique hash, the author and date of the commit, and the message included when it was committed.
One can see this information for all the history of commits (with the most recent one on top) by:

\begin{bash}
# show commits history
git log
\end{bash}

\fig{img/git5}{0.7}

After adding a file from working tree to staging area, one can unstage the changes to be committed files by copying
files from the latest commit from history to the staging area (in essence, one uses this command to undo a \code{git
add}):

\begin{bash}
# unstage file from staging area by copying it from history $($history $\&$ working tree stay same$)$
git reset <file>
\end{bash}

\begin{bash}
# unstage all from staging area by copying them from history $($history $\&$ working tree stay same$)$
git reset
\end{bash}

\fig{img/git6}{0.2}

Important to notice that in both cases we will NOT remove the commit from history and we will NOT discard anything in
working tree (we need the aforementioned \code{git checkout} for this). \v

In order to remove commits from the history, the way to do so is by moving the history to a previous commit (that way
all the commits after that will be ignored/erased). The way to do so is again by \code{git reset}.

\begin{bash}
# move history back to a specific commit
git reset --<type> <commit_hash>
\end{bash}

There are 3 different types to \code{git reset}. Depending on the type, different things will happen:
\bit
\item \code{--soft}: does not touch the staging area or the working tree files at all, but just resets the history to
commit. This leaves all the changed files as changes to be committed.
\item \code{--mixed}: which is the default action, resets the history and the staging area to commit, but leaves the
working tree untouched (i.e.\ the changed files are preserved but not marked for commit).
\item \code{--hard}: resets all history, staging area and working tree. Any changes to tracked files in the working
tree and staging area since ``commit'' are discarded. Any untracked files or directories are deleted. In other words
it creates a clean slate out of ``commit''.
\eit

Notice that for the command \code{git reset --mixed <latest$\_$hash>} we can discard both the \code{--mixed} part
since it's the default action and the ``latest hash'' since when we don't specify a commit the latest one will be
used. Thus, the command turns to \code{git reset} and according to the definition we just gave, it resets both the
history and the staging area to the latest commit, but leaves the working tree untouched. \v

However, since the history is already in the latest commit it stays also untouched. Hence, the only thing happening
is that staging gets a copy of the latest commit from history. In other words it unstages files from staging area by
copying them from history and leaves history and working tree untouched. It's exactly the same command as we used to
update staging area from history before. \v

Here is a graph that summarizes what it has been covered so far.

\fig{img/git7}{0.5}

\subsection{Branches}

\bd[Branch]
A \textbf{branch} is a pointer to a specific commit.
\ed

The branch pointer moves along with each new commit one makes, and only diverges in the graph if a commit is made on a
common ancestor commit.

\bd[Parent Branch]
The \textbf{parent branch} is the initial branch from which the branch was created.
\ed

Branches allow to work on different versions of the same files in parallel. Edits on one branch can be independent of
work on other branches. One can then decide to incorporate or merge the changes into other branches. Branches result
in a separation of versions of the same files. In this way, once can have branches for different purposes (e.g: a
production branch, a development branch, etc). \v

\bd[Master Branch]
The \textbf{master branch} is a default branch in Git, which it is instantiated when first commit made on the project.
\ed

At any point one can see all available branches by:

\begin{bash}
# show all local branches
git branch
\end{bash}

One can create a new branch by:

\begin{bash}
# create a new branch
git branch <branch>
\end{bash}

and can delete it by:

\begin{bash}
# delete existing branch
git branch $-$d <branch>
\end{bash}

\begin{bash}
# force delete existing branch
git branch $-$D <branch>
\end{bash}

Once a new branch is created, one can move to this branch by:

\begin{bash}
# switch to existing branch
git checkout <branch>
\end{bash}

A very useful shortcut command in order to create and switch to a new branch at the same time is:

\begin{bash}
# create and switch to new branch
git checkout $-$b <branch>
\end{bash}

Once to a new branch, one can start working, adding and committing. At some point she might decide to merge the
branch with master. The way to do so is by checking out to the master branch and:

\begin{bash}
# merge branch to master by including all previous commits of branch in chronological order
git merge <branch>
\end{bash}

\begin{bash}
# merge branch to master by squashing all previous commits to one single commit
git merge --squash <branch>
\end{bash}

\begin{bash}
# abort merge
git merge --abort
\end{bash}

\subsection{Remote Repository (Remote)}

\bd[Remote Repository (Remote)]
A \textbf{remote repository}, also called a \textbf{remote}, is a Git repository that's hosted on the Internet or
another network.
\ed

In other words, a remote is simply a repository in another location from where one is currently working. For example,
we may be working on a repo on our laptop. A version of that same work may be on GitHub. From the perspective of our
laptop, GitHub is a remote repository. When there are changes to the repo at a remote, we can download those changes
to get them locally. If we make changes to our local repo, we can upload our changes to the remote. \v

The starting point is downloading the remote locally:

\begin{bash}
# clone a remote from url
git clone <remote_url>
\end{bash}

The remote will appear in the local environment under the name origin. Origin is a default, shorthand name for the
remote repository that a project was originally cloned from. More precisely, it is used instead of that original
repository's URL $-$ and thereby makes referencing much easier. \v

One can see the remotes available locally by:

\begin{bash}
# show all remotes
git remote
\end{bash}

\begin{bash}
# show full locations of the remotes
git remote $-$v
\end{bash}

One can see all the available branches (both local and remote ones) by:

\begin{bash}
# show all branches local and remote
git branch $-$a
\end{bash}

The local branches appear as with the previous command \code{git branch}, however the remote ones appear as
$<$remote$>/<$branch$>$ to indicate that these branches are local copies of branches coming from the remote. Usually
it's not a good idea to checkout to these branches, unless one wants to check what's happening in the remote. \v

After cloning a repo, a connection has been established between the local repo and the remote one. However, Git does
not automatically update the local repo with all the changes that might have occurred by others in the remote repo.
One has to do this themselves by:

\begin{bash}
# get all updates from all branches from the remote to the local repo
git fetch <remote>
\end{bash}

\begin{bash}
# get all updates from just one branch from the remote to the local repo
git fetch <remote> <branch>
\end{bash}

There is also a command which is the best utility for cleaning outdated branches. It will connect to a shared remote
repository remote and fetch all remote branches. It will then delete all remote branches in local repo that are no
longer in use on the remote repository.

\begin{bash}
# get all updates from all branches from the remote and clean out outdated branches
git fetch <remote> --prune
\end{bash}

No matter which fetch one uses, these updates update the local versions of the remote branches, i.e.\ the
$<$remote$>/<$branch$>$ branches. This means that the updates have not been implemented yet to our local, working
branches. In order for this to happen, once the updates have been downloaded, they need to be merged to the local
repo. This can be done by first checkout to the local branch that needs to be merged and then:

\begin{bash}
# merge the remote branch to the local checked$-$out branch
git merge <remote>/<branch>
\end{bash}

One can combine fetch and merge within one single command pull:

\begin{bash}
# pull $=$ fetch $+$ merge
git pull <remote> <branch>
\end{bash}

The opposite direction is to push local changes to remote. This is a very important process that one has to done
correctly especially when there a lot of people working on the same project. Here is the correct way:
\bit
\item Step 1: Make sure local and remote are in sync with \code{git pull}.
\item Step 2: Create a new local branch from a parent branch (usually from master) and checkout to it with \code{git
checkout $-$b $<$branch$>$}.
\item Step 3: Push the local branch to the remote repo under the same name, and establish a connection between local and
remote branch.

\begin{bash}
# push local branch to remote and establish a connection between them
git push $-$u <remote> <branch>
\end{bash}

After one has used the \code{$-$u} flag, the local branch is directly associated with the remote one. Hence, there is
no need any more to specify neither the remote nor the branch when pulling and pushing.
\item Step 4: Work on local branch, make changes and commit them to local repo by \code{git add} and \code{git commit}.
\item Step 5: Push the changes to remote branch on remote.

\begin{bash}
# push updates from local branch to remote
git push
\end{bash}

\item Step 6: Repeat steps 4 and 5 for as long as needed.
\item Step 7: When the work is done, pull all changes that have happened in the parent branch in remote by \code{git
pull $<$remote$>$ $<$parent$\_$branch$>$}, and rebase the local branch to get these changes.

\begin{bash}
# rebase local checked$-$out branch to get updates
git rebase <parent$\_$branch>
\end{bash}

Another way to do that is also:

\begin{bash}
# pull all updates from remote branch of remote location and rebase checked$-$out branch
git pull --rebase <remote> <parent$\_$branch>
\end{bash}

In practice \code{git rebase} goes back in time, to the time of creation of local branch when the remote parent
branch was at commit $X$, and switches the commit $X$ to the newest commit $Y$ of remote parent branch, incorporating
all the updates that have happened in remote parent branch while we were working in our local branch. In other words
it's like creating a local branch to work with right now, with the current state of the parent branch in remote. \v

As it makes sense, this might create some problems, in case this newest commit $Y$ contradicts the work that we have
been doing so far (i.e.\ changes on the same lines of same files). In this case we say that we have a conflict, and we
need to manually resolve it by deciding which of the two options we should keep. More often than not, we need to sync
with the person that made the changes of the conflict in order to find a solution.
\item Step 8: Resolve any conflicts that might have occurred during rebasing.
\item Step 9: Push to remote for one last time with \code{git push}.
\item Step 10: Create a Pull Request (PR) from Github in order to merge the locally created branch to master branch in
remote.
\item Step 11: Once ready, close the PR by merging and if/when you are done with the remote branch delete it from the
remote.
\item Step 12: Update local repo with the PR happened in remote by \code{git pull --prune} (prune, in order to also
delete the remote branch locally).
\item Step 13: Checkout to master and delete the local branch by \code{git branch $-$d $<$branch$>$}.
\eit

\subsection{Useful Commands}

In this section I will go through some useful, unrelated to each other, commands that I often use.

\subsubsection{Git Cherry$-$Pick}

Sometimes one may accidentally commit to the wrong branch. In that case she wants to copy paste the commit from the
wrong branch to the correct one. This can be done by \code{git cherry$-$pick}.

\begin{bash}
# copy paste a specific commit from a branch to the checked$-$out branch using its hash
git cherry$-$pick <commit_hash>
\end{bash}

Important to notice that the original commit is not removed from its original branch. One needs to checkout to the
original, wrong branch and \code{git reset} the wrong commit.

\subsubsection{Git Stash}
The \code{git stash} command is very useful when one wants to record the current state of the working tree and
staging area because there are some changes that are not quite ready to be committed and he needs to throw them away
from working tree but keep them somewhere in memory for future use. This happens a lot when one needs to switch
branches without committing stuff (maybe he was working on the wrong branch), or to revert back temporarily to where
he started to check the initial state. \v

In order to do so one can use:

\begin{bash}
# stash only tracked $($staged and unstaged$)$ changes, including an optional message
git stash <optional_message>
\end{bash}

\begin{bash}
# stash all tracked and untracked $($staged and unstaged$)$ changes, including an optional message
git stash $-$u <optional_message>
\end{bash}

In order to see a list with all the stashes:

\begin{bash}
# show all stashes
git stash list
\end{bash}

In order to remove stashes:

\begin{bash}
# remove a single stash entry from the list of stash entries
git stash drop <stash>
\end{bash}

\begin{bash}
# remove all stash entries
git stash clear
\end{bash}

When one is done and wants the changes back to working tree and staging area they can use:

\begin{bash}
# put changes back $($on top of the current working tree$)$ from stash list and remove it from list
git stash pop <stash>
\end{bash}

\begin{bash}
# put changes back $($on top of the current working tree$)$ from stash list and leave it in list
git stash apply <stash>
\end{bash}

Notice that in case where no specific stash is selected then the most recent one the one on top of stash list will be
popped/applied.

\newpage

\section{Docker}

\fig{img/docker}{0.08}

\bd[Docker]
\textbf{Docker} is a set of platform as a service (PaaS)\footnotemark products that use OS$-$level virtualization to
deliver software in packages called containers.
\ed

\footnotetext[1]{Platform as a service (PaaS) is a category of cloud computing services that allows customers to
provision, instantiate, run, and manage a modular bundle comprising a computing platform and one or more
applications, without the complexity of building and maintaining the infrastructure typically associated with
developing and launching the application(s).}

\subsection{Introduction}

More often than not, when you have a team of developers working on some application, each developer would have to
install most of the services on their operating system directly. Depending on which operating system they're using,
the installation process will look different, and most of the time this process will contain multiple steps of
installation. This approach of setting up a new environment can actually be pretty tedious, depending on how complex
your application is. For example, if you have 10 services that your application is using, then you would have to do
that 10 times on each operating system environment. As it makes sense, the chances of something going wrong and error
happening is actually pretty high. \v

After developing the application, development team will produce artifacts together with a set of instructions of how
to actually install and configure those artifacts on the server. Then development team would give those artifacts
over to the operations team and the operations team will handle setting up the environment to deploy those
applications. The problem with this kind of approach is that, first of all, you need to configure everything and
install everything directly on the operating system, which we saw in the previous example that could actually lead to
conflicts with dependency version and multiple services running on the same host. Another problem that could arise
from this kind of process is when there is misunderstanding between the development team and operations because
everything is in a textual guide as instructions. All of these, could lead to some back and forth communication until
the application is successfully deployed on the server. \v

Docker is an open platform for developing, shipping, and running applications, which tries to solve all problems
described above. Docker enables you to separate your applications from your infrastructure so you can deliver
software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By
taking advantage of Docker's methodologies for shipping, testing, and deploying code quickly, you can significantly
reduce the delay between writing code and running it in production. \v

The key benefit of Docker is that it allows users to package an application with all of its dependencies into a
standardized unit for software development called a ``container''.

\bd[Docker Container]
A \textbf{Docker container}, or more simply a \textbf{container}, is a way to package applications with everything
they need inside the package, including the dependencies and all the necessary configuration. That package is
portable and can be easily shared and moved around within and between a development and an operation team.
\ed

The portability of containers plus everything packaged in one isolated environment gives it some of the advantages
that makes development and deployment process more efficient. More specifically, with containers, you do not have to
install any of the services directly on your operating system because the container is its own isolated operating
system layer with Linux based image. This makes the setting up your local development environment much easier and
much more efficient than the previous version. \v

On top of that you can have different versions of the same application running on your local environment without
having any conflict. Finally, with containers developers and operations are working in one team to package the whole
configuration dependencies inside the application. \v

Technically speaking, a container is made up of stacked images on top of each other.

\bd[Docker Image]
\textbf{Docker images}, or more simply \textbf{images}, are the basis of containers. An image is an ordered
collection of root filesystem changes and the corresponding execution parameters for use within a container runtime.
An image typically contains a union of layered filesystems stacked on top of each other. An image does not have state
and it never changes.
\ed

Images leave in an image repository. This is a special type of storage for images. Many companies have their own
private repositories where they store all their images, but there is also a public repository for images, called
``Docker Hub'', where you can browse and probably find any application images that you want. In Docker Hub there are
more than a hundred thousand images of different applications. For every application there is an official image. So
the only thing you need to do is run a Docker command that pulls an image that is stored somewhere in the repository
and then run it. This is, of course, a simplified version, but that makes exactly the problem that we saw on the
previous slide much more easier. No environmental configuration needed on the server. \v

So in simple words, a container is a runtime instance of an image, and consists of one or more images, an execution
environment and a standard set of instructions. (The concept is borrowed from actual shipping containers, which
define a standard to ship goods globally. Docker defines a standard to ship software). It is important to note that
most people use the terms image and container interchangeably, despite the fact that there is a fine difference
between the two. So remember, container is the running environment for an image. \v

Before we move on with going through the technical parts of Docker, let's first see a simplified example that
describes the workflow of Docker.

\be
So let's consider a simplified scenario where you're developing an application on your laptop. Right on your local
development environment, your application uses an SQL database, and instead of installing it on your laptop, you
download an image from the Docker Hub. You connect your application with the SQL database and you start developing.
When you are done with the first version of the application locally, you want to test it or you want to deploy it on
the development environment where a tester in your team is going to test it. You push your application in Git, or in
some other version control system, and that will trigger a continuous integration like Jenkins, or whatever you have
configured, which first it will build your application and then it will create an image out of it which then will get
pushed to a private image repository. Now is the next step, that could be configured on Jenkins or some other scripts
or tools, that image has to be deployed on a development server that pulls the image from the private repository, and
then pulls the SQL database that your application depends on from Docker Hub. And now you have two images, one being
your custom image and a the other one a publicly available SQL DB image which of course, they talk and communicate to
each other and run as an app.

\fig{img/docker6}{0.15}
\ee

\subsection{Installation}

Docker can be simply installed via Homebrew:

\begin{bash}
# install Docker via Homebrew
brew install --cask docker
\end{bash}

In order to make sure that Docker was installed properly, one can check the current Docker version:

\begin{bash}
# see Docker version
docker --version
\end{bash}

\subsection{Images}

As it makes sense the first step for using Docker is to have an image in hand. As we have already mentioned, one can
find thousand of images in the official Docker website ``Docker Hub''. Once you have decided on the image you are
interested in, you can pull the image locally on your computer by:

\begin{bash}
# pull image from Docker Hub
docker pull <image>:<version>
\end{bash}

More often than not, application have a lot of different versions (Hence, a lot of different images), so you need to
specify which one you want by including the version. In case the version is missing then Docker will pull the latest
version. As it is the norm for application to have many different version, it's a good practice to always include the
version even if you want the latest one. \v

One can see all available, locally pulled images by:

\begin{bash}
# show all available images
docker images
\end{bash}

By default \code{docker images} hides intermediate images. One can check them all by using:

\begin{bash}
# show all available images including intermediate ones
docker images $-$a
\end{bash}

Here follows a list with all the possible flags one can use with \code{docker images}:

\fig{img/docker4}{0.23}

In case one wants to get rid of an image they can use:

\begin{bash}
# remove image
docker rmi <image>
\end{bash}

The \code{docker rmi} cannot remove an image of a running container unless you use the force \code{$-$f} flag:

\begin{bash}
# remove image
docker rmi $-$f <image>
\end{bash}

Once can combine all the commands we just introduced to create a very useful command that removes all images from the
host:

\begin{bash}
# remove all images
docker rmi $-$f $\$$(docker images $-$a $-$q)
\end{bash}

\subsubsection{Building Images}

So far we have seen how to deal with images that are already done. However, what happens when we want to build an
image ourselves? For example, when a developer is done with her application, how can see turns it into an image that
can be pulled by others? In this section we will talk about building images. \v

Docker can build images automatically by reading the instructions from a special file called ``Dockerfile''.

\bd[Dockerfile]
A \textbf{Dockerfile} is a text document that contains all the commands a user could call on the command line to
assemble an image.
\ed

Here is the format of the Dockerfile:

\begin{block}
# Comment
INSTRUCTION arguments
\end{block}

Docker runs instructions in a Dockerfile in order. The instruction is not case$-$sensitive. However, convention is
for them to be UPPERCASE to distinguish them from arguments more easily. \v

Docker treats lines that begin with \code{$\#$} as a comment, unless the line is a valid parser directive. A
\code{$\#$} marker anywhere else in a line is treated as an argument. Comment lines are removed before the Dockerfile
instructions are executed. \v

Now let's take a detailed look in some of the most useful instructions one could include in a Dockerfile.
\bit
\item The \code{FROM} instruction initializes a new build stage and sets the base image (i.e.\ the parent image from
which you are building) for subsequent instructions. As such, a valid Dockerfile must start with a \code{FROM}
instruction. The image can be any valid image – it is especially easy to start by pulling an image from the Docker Hub.
\item The \code{WORKDIR} instruction sets the working directory for any \code{RUN}, \code{COPY}, and \code{CMD}
instructions that follow it in the Dockerfile (we will see these commands later). The \code{WORKDIR} instruction can
be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the
previous \code{WORKDIR} instruction. \v

If the \code{WORKDIR} doesn't exist, it will be created even if it's not used in any subsequent Dockerfile
instruction. If not specified, the default working directory is \code{/}. In practice, if you aren't building a
Dockerfile from scratch the \code{WORKDIR} may likely be set by the base image you're using. Therefore, to avoid
unintended operations in unknown directories, it is best practice to set your \code{WORKDIR} explicitly.
\item The \code{RUN} instruction will execute any commands in a new layer on top of the current image and commit the
results. The resulting committed image will be used for the next step in the Dockerfile. Important to notice that the
command will be executed inside the container and not in the host. \v

This concept of layering \code{RUN} instructions and generating commits conforms to the core concepts of Docker where
commits are cheap and containers can be created from any point in an image's history, much like source control.
\item The \code{ENV} instruction sets the environment variable <key> to the value <value>. This value will be in the
environment for all subsequent instructions in the build stage and can be replaced inline in many as well. The value
will be interpreted for other environment variables, so quote characters will be removed if they are not escaped.
Like command line parsing, quotes and backslashes can be used to include spaces within values. \v

The \code{ENV} instruction allows for multiple <key>=<value> variables to be set at one time. Be careful.
Environmental variables will be available to Docker during building time, but not to the image (and subsequently to
the container) during running time. In other words, the environmental variables defined in Dockerfile won't be
available to the containers.
\item The \code{COPY} instruction copies new files or directories from the <host$\_$path> in host and adds them to
the filesystem of the container at the path <image$\_$path>. Be careful. Copying from host, and pasting to image.
\item The \code{LABEL} instruction adds metadata to an image. A \code{LABEL} is a key$-$value pair. An image can have
more than one label. You can specify multiple labels on a single line. Labels included in base or parent images
(images in the \code{FROM} line) are inherited by your image. If a label already exists but with a different value,
the most$-$recently$-$applied value overrides any previously$-$set value.
\item The \code{EXPOSE} instruction informs Docker that the container listens on the specified network ports at
runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if the protocol is not
specified. \v

The \code{EXPOSE} instruction does not actually publish the port. It functions as a type of documentation between the
person who builds the image and the person who runs the container, about which ports are intended to be published. To
actually publish the port when running the container, use the \code{$-$p} flag on \code{docker run} to publish and
map one or more ports. Regardless of the \code{EXPOSE} settings, you can override them at runtime by using the
\code{$-$p} flag (more on that on containers section).
\item The \code{CMD} instruction provides the default, entrypoint command for the Dockerfile to execute. It is
identical to the \code{RUN} command, with two basic differences. The first one is that the \code{CMD} instructions is
actually telling to Dockerfile that the instructions are over, and the ``basic'' command to run is the one provided
in \code{CMD}. So for example, \code{CMD} could be to launch our application. Which naturally leads to the second one
which is that there can only be one \code{CMD} instruction in a Dockerfile. If you list more than one \code{CMD} then
only the last\code{CMD} will take effect.
\eit

So, this is an example of a Dockerfile:

\begin{block}
FROM <image>
WORKDIR <workdir>
RUN <command>
ENV <environmental_variable_1>=``<value_1>''
ENV <environmental_variable_2>=``<value_2>''
LABEL ``<label>''=``<value>''
COPY <host_path> <image_path>
EXPOSE 80
CMD [``<executable>'',``<param_1>'', $\ldots$,``<param_n>'']
\end{block}

After the Dockerfile has been completed, then we are are ready to build the image. Docker knows how to build it, due
to the instructions we have put in the Dockerfile. It very important to keep the name as Dockerfile, cause Docker by
default will search for it. The command to build an image is \code{build} and it is used as:

\begin{bash}
# build image using the instructions in Dockerfile
docker build $-$t <name>:<tag> <Dockerfile_path>
\end{bash}

As we have already said, it is a best practice to have the Dockerfile in the root of the application, in that case
the path for the Dockerfile is just the current directory \code{.}. \v

Very important, if any change takes place in the Dockerfile, the one has to re$-$build the image in order for the
changes to take effect. \v

Last but not least one can push a created image into a Docker repository with:

\begin{bash}
# push image to Docker repository
docker push <image>:<version>
\end{bash}

Of course, for this command to work one has to have configured a Docker repository somewhere, but this gets out of
topic for the purposes of this section.

\subsection{Containers}

Up to this point, we have only worked with images. There is no container involved and there is no image running in
one. In order to start running an image within a container one could run:

\begin{bash}
# run image
docker run <image>
\end{bash}

This command will initiate a container which will run the specified image. A very important note is that Docker will
try to find the specified image locally and if it fails to find it then it will search in Docker Hub, or any other
image repository we will specify. \v

One can also assign a assign a name to the container using the \code{--name} flag:

\begin{bash}
# run image and assign a name
docker run --name <name> <image>
\end{bash}

In case we don't assign a name, Docker will assign a random name by itself. \v

The \code{docker run} command will run the container in an interactive environment. In order to run a container in
background one can use the detach \code{$-$d} flag:

\begin{bash}
# run image in background
docker run $-$d <image>
\end{bash}

Recall that a container is just a virtual environment running on your host. And you can have multiple containers
running simultaneously on your host. Your host has certain ports available that you can open for certain applications.
By default the containers have one port exposed but, also by default, this port is not binded to any host port. \v

So in simple words, both run commands we showed, run the images within isolated containers, meaning that there is no
way for the container to communicate with other applications. So how it works is that you need to create a
so$-$called binding between a port from your host machine and a port from the container.

\begin{bash}
# run image and bind host and container port
docker run $-$p <host_port>:<container_port> <image>
\end{bash}

Once the port binding between the host and the container is already done, you can actually connect to the running
container using the port of the host. Notice that, of course, you can have two containers both listening at the same
container port as long as you're binding them to two different ports from your host machine. \v

One can include environmental variables for the container to use as such:

\begin{bash}
# run image with environmental variables
docker run $-$e <environmental_variable>=<value> <image>
\end{bash}

One can include many environmental variables by using multiple $-$e flags. \v

There is a huge list of available flags one can use with \code{docker run}, that can be find
\href{https://docs.docker.com/engine/reference/commandline/run/#options}{here}. \v

One can see all running containers by:

\begin{bash}
# show all running containers
docker ps
\end{bash}

By default \code{ps} hides stopped container. One can check them all by using:

\begin{bash}
# show all container including stopped ones
docker ps $-$a
\end{bash}

One can also see only the IDs of the container by using the quiet \code{$-$q} flag.

\begin{bash}
# show all running container IDs
docker ps $-$q
\end{bash}

Here follows a list with all the possible flags one can use with \code{docker ps}:

\fig{img/docker5}{0.20}

One can stop a running container by:

\begin{bash}
# stop a running container
docker stop <container>
\end{bash}

and start it again by:

\begin{bash}
# start a stopped container
docker start <container>
\end{bash}

Once can combine all the commands we just introduced to create a very useful command that stops all containers:

\begin{bash}
# stop all containers
docker stop $\$$(docker ps $-$a $-$q)
\end{bash}

In case one wants to get rid of a container they can use:

\begin{bash}
# remove container
docker rm <container>
\end{bash}

Note that \code{docker rm} cannot remove a running container unless you use the force \code{$-$f} flag:

\begin{bash}
# remove container even if it is running
docker rm $-$f <container>
\end{bash}

One can combine commands to create a handy command that removes all containers:

\begin{bash}
# remove all containers
docker rm $-$f $\$$(docker ps $-$a $-$q)
\end{bash}

When it comes to troubleshooting, if something goes wrong in the container, you want to see the logs of the container
by:

\begin{bash}
# batch$-$retrieve logs present at the time of execution
docker logs <container>
\end{bash}

The follow \code{$-$f} flag will continue streaming the new outputs from the container:

\begin{bash}
# batch$-$retrieve logs present at the time of execution and keep printing them as they come
docker logs $-$f <container>
\end{bash}

Here follows a list with all the possible flags one can use with \code{docker logs}:

\fig{img/docker2}{0.22}

Another very useful command in debugging is \code{docker exec} where we can actually run a new command in a running
container.

\begin{bash}
# run a new command in a running container
docker exec <container>
\end{bash}

More often than not one wants to actually connect to the container and use the terminal directly in the container. We
can achieve this with a combination of tty \code{$-$t} flag which allocates a pseudo$-$TTY and interactive
\code{$-$i} flag which keeps \code{STDIN} open.

\begin{bash}
# connect to the terminal of a running container
docker exec $-$i $-$t <container>
\end{bash}

Here follows all the possible flags one can use with \code{docker exec}:

\fig{img/docker3}{0.22}

\subsubsection{Multiple Containers}

Most of the time, an application needs more than one service at a time. For example our application might be a NodeJS
application which needs one Mongo database for storing user info as long as a Mongo Express UI in order to visualize
the Mongo database. In that case, one needs to pull multiple images (NodeJs, MongoDb, Mongo Express UI), but most
importantly to make these images to communicate and work together since most of the time services need to exchange
data. This is accomplished by creating a Docker network where all the containers live in. \v

\fig{img/docker7}{0.34}

In essence, a Docker network is primarily used to establish communication between containers and the outside world
via the host machine where the Docker daemon is running. Docker supports different types of networks, each fit for
certain use cases. In order to create a network you can run:

\begin{bash}
# create a Docker network
docker network create <network>
\end{bash}

In order to find all available networks:

\begin{bash}
# list all Docker networks
docker network ls
\end{bash}

Finally, to remove a no$-$longer needed network:

\begin{bash}
# remove a Docker network
docker network rm <network>
\end{bash}

Once a network is created, we are able to put container in it. If there are already running containers one wants to
put to a network, then the \code{docker network connect} command is the one to go:

\begin{bash}
# connect a container to a network
docker network connect <network> <container>
\end{bash}

You can connect a container by name or by ID. Once connected, the container can communicate with other containers in
the same network. \v

If on the other hand one wants to connect to the network a container that is not running yet, then one has to use the
\code{docker run} command including the network \code{$-$$-$net} flag

\begin{bash}
# connect a container to a network when it starts
docker run --net <network>
\end{bash}

While this process is quite straight forward, it is also a bit of an overkill. One has to create a network, and then
starting connecting container ony by one to the network. On top of that, she has to do that again every time she
needs to. For example, when the application is shipped for productionization, the production team would need to do
this process again. In order to save us from the trouble, Docker has an extra functionality called \code{docker
compose}, which helps us to launch multiple containers within a network in a reproducible way. \v

The \code{docker compose} is a tool that was developed to help define and share multi$-$container applications. With
\code{docker compose}, we can create a YAML file to define the services and with a single command, can spin
everything up or tear it all down. The big advantage of using \code{docker compose} is you can define your
application stack in a file, keep it at the root of your project repo (it's now version controlled), and easily
enable someone else to contribute to your project. Someone would only need to clone your repo and start the compose
app. In fact, you might see quite a few projects on GitHub doing exactly this now. \v

In order to begin, at the root of the app project, create a file named \code{docker$-$compose.yaml}.

\bd[docker$-$compose.yaml]
The \textbf{docker$-$compose.yaml} file is a YAML file defining services, networks, and volumes for a Docker application.
\ed

In the \code{docker$-$compose.yaml} file, we'll start off by defining the schema version. In most cases, it's best to
use the latest supported version. You can look at the Compose file reference for the current schema versions and the
compatibility matrix:

\begin{block}
version: <version>
\end{block}

Next, we'll define the list of services (or containers) we want to run as part of our application, and we'll start
migrating a service at a time into the compose file. We can pick any name for the service. The name will
automatically become a network alias, which will be useful when defining other services:

\begin{block}
version: <version>
services:
    <service_1>:
        image: <image_1>
        ports:
            $-$ <host_port_1>:<container_port_1>
        environment:
            $-$ <environmental_variable_1_1>=<value_1_1>
            $-$ <environmental_variable_1_2>=<value_1_2>
    <service_2>:
        image: <image_2>
        ports:
            $-$ <host_port_2>:<container_port_2>
        environment:
            $-$ <environmental_variable_2_1>=<value_2_1>
            $-$ <environmental_variable_2_2>=<value_2_2>
\end{block}

The above \code{docker$-$compose.yaml} file is just an example and in no way an exhaustive list. There are many more
parameters one can specify in the YAML and, of course, one can add as many services as he likes. \v

Once the \code{docker$-$compose.yaml} is ready, it can be shipped together with the application and then one can
launch all container at once by using:

\begin{bash}
# launch a Docker compose network
docker compose $-$f docker$-$compose.yaml up
\end{bash}

Notice that there is no need to create a network. \code{docker compose} will take care of that.

Once can also use the \code{$-$p]} flag to specify a name for the whole application. If not is given then Docker will
use the directory name as the default option.

\begin{bash}
# specify an alternate project name
docker compose $-$f docker$-$compose.yaml $-$p <name> up
\end{bash}

\subsection{Volumes}

When a container is deleted, relaunching the image will start a fresh container without any of the changes made in
the previously running container -- those changes are lost. As it makes sense this is quite inconvenient, since some
times we don't want to lose these changes. The most straight$-$forward example is in the case of a database where we
don't want to lose all the data when the container is deleted. \v

In order to be able to save (persist) data and also to share data between containers, Docker came up with the concept
of ``volumes''.

\bd[Volumes]
\textbf{Volumes} are directories (or files) that are outside of the container and exist as normal directories and files
on the host filesystem.
\ed

In simple terms, a directory folder on a host file system is mounted into a directory of folder in the virtual file
system of Docker, and then what happens is that when a container writes to its file system, it gets replicated or
automatically written on the host file system directory and vice versa. \v

There are 3 types of volumes in Docker. All of them use the \code{docker run} command, combined with the volume flag
\code{$-$v}. The first type of volume definition is called ``host volume'', and the main characteristic of this one
is that you decide where on the host file system that references made. In other words, which folder on the host file
system you mount into the container. The command to do so is:

\begin{bash}
# run a container combined with a host volume
docker run $-$v <host_volume_path>:<container_volume_path>
\end{bash}

The second type is where you create a volume just by referencing the container directory so you don't specify which
directory on the host should be mounted, which is being taken care by Docker itself. That directory is automatically
created by Docker and for each container there will be a folder generated that gets mounted automatically to the
container. This type of volumes are called ``anonymous volumes'' and can be created by:

\begin{bash}
# run a container combined with an anonymous volume
docker run $-$v <container_volume_path>
\end{bash}

The third volume type is actually an improvement of the anonymous volumes and it specifies the name of that folder on
the host file system, which acts as a simple reference to the directory. The selection of the name is up to you.
Compared to anonymous volumes, you can actually reference that volume just by name so you don't have to know exactly
the path. That type of volumes are called ``named volumes'' and can be created by:

\begin{bash}
# run a container combined with a named volume
docker run $-$v <name>:<container_volume_path>
\end{bash}

From these three types, the mostly used one and the one that you should be using in a production is actually the
named volumes because there are additional benefits to letting Docker actually manage those volume directories on the
host. \v

Of course, one can include volumes in the \code{docker$-$compose.yaml} file instead of creating them through
\code{run}. The way to do so is by including a list of all the volumes names on the same level as the services, and
then list them on the container level where you actually define which path that specific volume can be mounted. \v

The benefit of that is that you can actually mount a reference of the same folder on the host to more than one
containers, and that would be beneficial if those containers need to share the data. In this case, you would want the
same volume name or reference to two different containers and you can mount them into different path inside of the
container. \v

Here is an example of a \code{docker$-$compose.yaml} with volumes.

\begin{block}
version: <version>
services:
    <service_1>:
        image: <image_1>
        ports:
            $-$ <host_port_1>:<container_port_1>
        environment:
            $-$ <environmental_variable_1_1>=<value_1_1>
            $-$ <environmental_variable_1_2>=<value_1_2>
        volumes:
            $-$ <volume_1>:<container_volume_path_1>
    <service_2>:
        image: <image_2>
        ports:
            $-$ <host_port_2>:<container_port_2>
        environment:
            $-$ <environmental_variable_2_1>=<value_2_1>
            $-$ <environmental_variable_2_2>=<value_2_2>
        volumes:
            $-$ <volume_2>:<container_volume_path_2>
volumes:
    $-$ <volume_1>
    $-$ <volume_2>
\end{block}

As a final note, one can create and manage volumes outside the scope of a container by using the \code{docker volume}
commands. For example one can create a volume by:

\begin{bash}
# create a volume
docker volume create <volume>
\end{bash}

One can list all the created volumes by:

\begin{bash}
# list all volumes
docker volume ls
\end{bash}

Finally, to remove a volume:

\begin{bash}
# remove a volume
docker volume rm <volume>
\end{bash}

However, all these options create and manage volumes outside the scope of a container. In almost all the cases we
want to attach a container to a specific container, so we either use the \code{docker run} command or the
\code{docker$-$compose.yaml}.