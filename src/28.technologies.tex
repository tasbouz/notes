%! suppress = Quote
%! suppress = EscapeUnderscore
%! suppress = EscapeHashOutsideCommand
%! suppress = EscapeUnderscore
%! suppress = EscapeHashOutsideCommand
%! suppress = Quote
In this chapter, I will go through some of the technologies I find interesting and use on a daily basis. For each one
of them, I will provide a short history note and then I will explain the way to use them.

\section{Serialization}

\bd[Serialization]
\textbf{Serialization} is the process of translating a data structure or object state into a format that can be stored
(for example, in a file or memory data buffer) or transmitted (for example, over a computer network) and reconstructed
later (possibly in a different computer environment).
\ed

When the resulting series of bits is reread according to the serialization format, it can be used to create a
semantically identical clone of the original object.

\bd[Deserialization]
The opposite operation, extracting a data structure from a series of bytes, is \textbf{deserialization}.
\ed

For serialization to be useful, architecture independence must be maintained. For example, for maximal use of
distribution, a computer running on a different hardware architecture should be able to reliably reconstruct a
serialized data stream, regardless of endianness. This means that the simpler and faster procedure of directly
copying the memory layout of the data structure cannot work reliably for all architectures. \v

Serializing the data structure in an architecture$-$independent format means preventing the problems of byte
ordering, memory layout, or simply different ways of representing data structures in different programming languages.

\bd[Human$-$Readable]
A \textbf{human$-$readable} format is any encoding of data or information that can be naturally read by humans.
\ed

In this section we will be focusing on human$-$readable serialization. In other words, serialization formats that can
be read by humans. Although there are many different human$-$readable serialization formats, we will go through the 4
most used ones:
\bit
\item CSV
\item JSON
\item XML
\item YAML
\eit

\subsection{CSV}

\bd[Comma Separated Value (CSV)]
A \textbf{Comma Separated Value} (\textbf{CSV}) file is a delimited text file that uses a comma to separate values.
\ed

Each line of a CSV file is a data record. Each record consists of one or more fields, separated by commas. The use of
the comma as a field separator is the source of the name for this file format. A CSV file typically stores tabular
data (numbers and text) in plain text, in which case each line will have the same number of fields. \v

The CSV file format is not fully standardized. Separating fields with commas is the foundation, but commas in the
data or embedded line breaks have to be handled specially. Some implementations disallow such content while others
surround the field with quotation marks, which yet again creates the need for escaping if quotation marks are present
in the data. \v

The term CSV also denotes several closely$-$related delimiter$-$separated formats that use other field delimiters
such as semicolons. These include tab$-$separated values and space$-$separated values. A delimiter guaranteed not to
be part of the data greatly simplifies parsing. These alternative delimiter$-$separated files are often given a
\code{.csv} extension despite the use of a non$-$comma field separator. This loose terminology can cause problems in
data exchange. Many applications that accept CSV files have options to select the delimiter character and the
quotation character. \v

CSV is widely supported by consumer, business, and scientific applications. Among its most common uses is moving
tabular data between programs that natively operate on incompatible (often proprietary or undocumented) formats. \v

Typical rules other CSV specifications and implementations are as follows:
\bit
\item CSV is a delimited data format that has fields/columns separated by the comma character and records/rows
terminated by newlines.
\item A CSV file does not require a specific character encoding, byte order, or line terminator format.
\item A record ends at a line terminator. However, line terminators can be embedded as data within fields, so software
must recognize quoted line$-$separators in order to correctly assemble an entire record from perhaps multiple lines.
\item All records should have the same number of fields, in the same order.
\item Data within fields is interpreted as a sequence of characters, not as a sequence of bits or bytes.
\item Adjacent fields must be separated by a single comma. However, CSV formats vary greatly in this choice of separator
character. In particular, in locales where the comma is used as a decimal separator, a semicolon, tab, or other
character is used instead.
\item Any field may be quoted (that is, enclosed within double$-$quote characters), while some fields must be quoted, as
specified in the following rules:
\bit
\item Fields with embedded commas or double$-$quote characters must be quoted.
\item Each of the embedded double$-$quote characters must be represented by a pair of double$-$quote characters.
\item Fields with embedded line breaks must be quoted.
\item In some CSV implementations, leading and trailing spaces and tabs are trimmed (ignored).
\item Spaces outside quotes in a field are not allowed.
\item In CSV implementations that do trim leading or trailing spaces, fields with such spaces as meaningful data must be
quoted.
\item Double quote processing need only apply if the field starts with a double quote.
\item The first record may be a header, which contains column names in each of the fields (there is no reliable way to
tell whether a file does this or not; however, it is uncommon to use characters other than letters, digits, and
underscores in such column names).
\eit
\eit

Here is an example of a CSV file:

\begin{block}
Year,Make,Model,Description,Price
1997,Ford,E350,"ac, abs, moon",3000.00
1999,Chevy,"Venture ""Extended Edition""","",4900.00
1999,Chevy,"Venture ""Extended Edition, Very Large""","",5000.00
1996,Jeep,Grand Cherokee,"MUST SELL! air, moon roof, loaded",4799.00
\end{block}

The above CSV of data may be represented in table format as follows:

\fig{csv}{0.65}

\subsection{JSON}

\fig{json}{0.19}

\bd[JavaScript Object Notation (JSON)]
\textbf{JavaScript Object Notation} (\textbf{JSON}) is an open standard file format and data interchange format that
uses human$-$readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other
serializable values).
\ed

JSON grew out of a need for a stateless, real$-$time server$-$to$-$browser communication protocol without using
browser plugins such as Flash or Java applets, the dominant methods used in the early 2000s. Douglas Crockford
originally specified and popularized the JSON format in the early 2000s. \v

JSON was based on a subset of the JavaScript scripting language and is commonly used with JavaScript, but it is a
language$-$independent data format. Code for parsing and generating JSON data is readily available in many
programming languages. JSON filenames use the extension \code{.json}. Any valid JSON file is a valid JavaScript file,
even though it makes no changes to a web page on its own. \v

The format of a JSON is very simple: it's a collection of key–value pairs separated by a colon, each one in each own
line. Indentation is not important but it is commonly used to help in human readability. JSON's basic data types are:
\bit
\item Number: a signed decimal number that may contain a fractional part, but cannot include non$-$numbers such as NaN\@.
\item String: a sequence of zero or more Unicode characters. Strings are delimited with double quotation marks and
support a backslash escaping syntax.
\item Boolean: either of the values true or false.
\item Array: an ordered list of zero or more elements, each of which may be of any type. Arrays use square bracket
notation with comma$-$separated elements.
\item Object: a collection of name–value pairs where the names (also called keys) are strings. Objects are notated with
curly brackets.
\item null: an empty value, using the word null.
\eit

The following example shows a possible JSON representation describing a person:

\begin{block}
{
  "firstName": "John",
  "lastName": "Smith",
  "isAlive": true,
  "age": 27,
  "address": {
    "streetAddress": "21 2nd Street",
    "city": "New York",
    "state": "NY",
    "postalCode": "10021-3100"
  },
  "phoneNumbers": [
    {
      "type": "home",
      "number": "212 555-1234"
    },
    {
      "type": "office",
      "number": "646 555-4567"
    }
  ],
  "children": [
      "Catherine",
      "Thomas",
      "Trevor"
  ],
  "spouse": null
}
\end{block}

\subsection{XML}

\fig{xml}{0.2}

\bd[Extensible Markup Language (XML)]
\textbf{Extensible Markup Language} (\textbf{XML}) is a markup language and file format for storing, transmitting, and
reconstructing arbitrary data, and defines a set of rules for encoding documents in a format that is both
human$-$readable and machine$-$readable.
\ed

The main purpose of XML is serialization, i.e.\ storing, transmitting, and reconstructing arbitrary data. For two
disparate systems to exchange information, they need to agree upon a file format. XML standardizes this process. XML
is analogous to a lingua franca for representing information. \v

The design goals of XML emphasize simplicity, generality, and usability across the Internet. It is a textual data
format with strong support via Unicode for different human languages. Although the design of XML focuses on
documents, the language is widely used for the representation of arbitrary data structures such as those used in web
services. As a markup language, XML labels, categorizes, and structurally organizes information. XML tags represent
the data structure and contain metadata. What's within the tags is data, encoded in the way the XML standard
specifies. \v

What follows is some components based on the XML Specification. This is not an exhaustive list of all the constructs
that appear in XML; it provides an introduction to the key constructs most often encountered in day$-$to$-$day use.
\bit
\item An XML document is a string of characters. Almost every legal Unicode character may appear in an XML document.
\item The processor analyzes the markup and passes structured information to an application. The specification places
requirements on what an XML processor must do and not do, but the application is outside its scope. The processor (as
the specification calls it) is often referred to colloquially as an XML parser.
\item The characters making up an XML document are divided into markup and content, which may be distinguished by the
application of simple syntactic rules. Generally, strings that constitute markup either begin with the character
\code{<} and end with a \code{>}. Strings of characters that are not markup are content.
\item A tag is a markup construct that begins with \code{<} and ends with \code{>}. There are three types of tags:
\bit
\item start$-$tag, such as \code{<section>}.
\item end$-$tag, such as \code{</section>}.
\item empty$-$element tag, such as \code{<line$-$break />}.
\eit
\item An element is a logical document component that either begins with a start$-$tag and ends with a matching
end$-$tag or consists only of an empty$-$element tag. The characters between the start$-$tag and end$-$tag, if any,
are the element's content, and may contain markup, including other elements, which are called child elements.
\item An attribute is a markup construct consisting of a name–value pair that exists within a start$-$tag or
empty$-$element tag. An XML attribute can only have a single value and each attribute can appear at most once on each
element. In the common situation where a list of multiple values is desired, this must be done by encoding the list
into a well$-$formed XML attribute with some format beyond that XML defines itself. Usually this is either a comma or
semi$-$colon delimited list or, if the individual values are known not to contain spaces, a space$-$delimited list
can be used.
\item XML documents may begin with an XML declaration that describes some information about themselves.
\item Comments may appear anywhere in a document outside other markup. Comments cannot appear before the XML
declaration. Comments begin with \code{$<!--$} and end with \code{$-->$}.
\eit

XML has regularly been criticized for verbosity, complexity and redundancy. Mapping the basic tree model of XML to
type systems of programming languages or databases can be difficult, especially when XML is used for exchanging
highly structured data between applications, which was not its primary design goal. \v

Other criticisms attempt to refute the claim that XML is a self$-$describing language (though the XML specification
itself makes no such claim). JSON and YAML are frequently proposed as simpler alternatives that focus on representing
highly structured data rather than documents, which may contain both highly structured and relatively unstructured
content. \v

The following example shows a possible XML representation describing a book:

\begin{block}
<bookstore>
  <book category="COOKING">
    <title lang="en">Everyday Italian</title>
    <author>Giada De Laurentiis</author>
    <year>2005</year>
    <price>30.00</price>
    <code>438043534</code>
  </book>
  <book category="CHILDREN">
    <title lang="en">Harry Potter</title>
    <author>J K. Rowling</author>
    <year>2005</year>
    <price>29.99</price>
    <code>40863986</code>
  </book>
  <book category="WEB">
    <title lang="en">Learning XML</title>
    <author>Erik T. Ray</author>
    <year>2003</year>
    <price>39.95</price>
    <code>509686346</code>
  </book>
</bookstore>
\end{block}

\subsection{YAML}

\fig{yaml}{0.04}

\bd[YAML Ain't Markup Language (YAML)]
\textbf{YAML Ain't Markup Language} (\textbf{YAML}) is a human$-$readable data$-$serialization language.
\ed

YAML was first proposed by Clark Evans in 2001. Originally YAML was said to mean ``Yet Another Markup Language'',
because it was released in an era that saw a proliferation of markup languages for presentation and connectivity. Its
initial name was intended as a tongue$-$in$-$cheek reference to the technology landscape, referencing its purpose as
a markup language with the yet another construct, but it was then repurposed as ``YAML Ain't Markup Language'', a
recursive acronym, to distinguish its purpose as data$-$oriented, rather than document markup. \v

YAML is commonly used for configuration files and in applications where data is being stored or transmitted. It
targets many of the same communications applications as XML but has a minimal syntax. It uses both Python$-$style
indentation to indicate nesting, and a more compact format that uses \code{$[\ldots]$} for lists and
\code{$\{\ldots\}$} for maps, thus JSON files are valid YAML. \v

Custom data types are allowed, but YAML natively encodes scalars (such as strings, integers, and floats), lists, and
associative arrays (also known as maps, dictionaries or hashes). These data types are based on the Perl programming
language, though all commonly used high$-$level programming languages share very similar concepts. YAML is intended
to be read and written in streams. \v

Support for reading and writing YAML is available for many programming languages. Some source$-$code editors such as
Vim, Emacs,and various integrated development environments have features that make editing YAML easier, such as
folding up nested structures or automatically highlighting syntax errors. \v

The official recommended filename extension for YAML files has been \code{.yaml} since 2006. \v

The following is a synopsis of the basic elements:
\bit
\item YAML accepts the entire Unicode character set, except for some control characters.
\item Whitespace indentation is used for denoting structure.
\item Comments begin with the number sign \code{$\#$}, can start anywhere on a line and continue until the end of the
line. Comments must be separated from other tokens by whitespace characters. If \code{$\#$} characters appear inside
of a string, then they are number sign \code{$\#$} literals.
\item List members are denoted by a leading hyphen \code{$-$} with one member per line.
\item A list can also be specified by enclosing text in square brackets \code{[$\ldots$]} with each entry separated by a
comma.
\item An associative array entry is represented using colon space in the form key: value with one entry per line. YAML
requires the colon be followed by a space.
\item A question mark can be used in front of a key, in the form \code{?key: value} to allow the key to contain leading
dashes, square brackets, etc, without quotes.
\item An associative array can also be specified by text enclosed in curly braces \code{$\{\ldots\}$}, with keys separated from
values by colon and the entries separated by commas (spaces are not required to retain compatibility with JSON).
\item Strings are ordinarily unquoted, but may be enclosed in double$-$quotes \code{"}, or single$-$quotes \code{'}.
\item Within double$-$quotes, special characters may be represented with C$-$style escape sequences starting with a
backslash \code{$\textbackslash$}.
\item Within single quotes the only supported escape sequence is a doubled single quote \code{"}.
\item Block scalars are delimited with indentation with optional modifiers to preserve \code{|} or fold \code{>}
newlines.
\item Multiple documents within a single stream are separated by three hyphens \code{\texttt{$-${}$-${}$-$}}.
\item Three periods \code{$\ldots$} optionally end a document within a stream.
\item Repeated nodes are initially denoted by an ampersand \code{$\&$} and thereafter referenced with an asterisk *.
\item Nodes may be labeled with a type or tag using a double exclamation mark \code{!!} followed by a string, which can
be expanded into a URI\@.
\item YAML documents in a stream may be preceded by directives composed of a percent sign \code{$\%$} followed by a name
and space$-$delimited parameters.
\eit

YAML has been criticized for its significant whitespace, confusing features, insecure defaults, and its complex and
ambiguous specification:
\bit
\item Configuration files can execute commands or load contents without the users realizing it.
\item Editing large YAML files is difficult, as indentation errors can go unnoticed.
\item Type autodetection is a source of errors. For example, unquoted \code{YES} and \code{NO} are converted to
booleans; software version numbers might be converted to floats.
\item Truncated files are often interpreted as valid YAML due to the absence of terminators.
\item The complexity of the standard led to inconsistent implementations and making the language non$-$portable.
\eit

The following example shows a possible YAML representation describing a receipt:

\begin{block}
receipt: Oz$-$Ware Purchase Invoice
date: 2012$-$08$-$06
customer:
    first_name: Dorothy
    middle_name: A\@.
    family_name: Gale
items:
    $-$ part_no: A4786
      descrip: Water Bucket (Filled)
      price: 1.47
      quantity: 4
    $-$ part_no: E1628
      descrip: High Heeled "Ruby" Slippers
      size: 8
      price: 133.7
      quantity: 1
bill$-$to:  &id001
    # symbol | preserves new lines
    street: |
            123 Tornado Alley
            Suite 16
    city: East Centerville
    state: KS
    floor: 5
    elevator: No
ship$-$to:  *id001
# symbol > folds new lines
specialDelivery:  >
    Follow the Yellow Brick
    Road to the Emerald City.
    Pay no attention to the
    man behind the curtain.
\end{block}

\section{Cryptography}

\bd[Cryptography]
\textbf{Cryptography} is the practice and study of techniques for secure communication in the presence of adversarial
behavior.
\ed

More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public
from reading private messages. Modern cryptography exists at the intersection of the disciplines of mathematics,
computer science, information security, electrical engineering, digital signal processing, physics, and others. Core
concepts related to information security (data confidentiality, data integrity, authentication, and non-repudiation)
are also central to cryptography. Practical applications of cryptography include electronic commerce, chip-based
payment cards, digital currencies, computer passwords, and military communications. \v

The two most commonly used techniques of modern cryptography are the so-called ``symmetric-key'' and ``public-key''
cryptography.

\bd[Symmetric-Key Cryptography]
\textbf{Symmetric-key cryptography} uses the same cryptographic keys for both the encryption of plaintext and the
decryption of ciphertext. \ed

In symmetric-key cryptography, the keys may be identical, or there may be a simple transformation to go between the
two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to
maintain a private information link. The requirement that both parties have access to the secret key is one of the
main drawbacks of symmetric-key encryption, in comparison to public-key encryption (also known as asymmetric-key
encryption).

\bd[Public-Key Cryptography / Assymetric Cryptography]
\textbf{Public-key cryptography}, or \textbf{asymmetric cryptography}, is the field of cryptographic systems that use
pairs of related keys. Each key pair consists of a public key and a corresponding private key. Key pairs are
generated with cryptographic algorithms based on mathematical problems termed one-way functions. Security of public-
key cryptography depends on keeping the private key secret; the public key can be openly distributed without
compromising security.
\ed

In a public-key encryption system, anyone with a public key can encrypt a message, yielding a ciphertext, but only
those who know the corresponding private key can decrypt the ciphertext to obtain the original message.

\be
For example, a journalist can publish the public key of an encryption key pair on a web site so that sources can send
secret messages to the news organization in ciphertext. Only the journalist who knows the corresponding private key
can decrypt the ciphertexts to obtain the sources' messages—an eavesdropper reading email on its way to the
journalist cannot decrypt the ciphertexts.
\ee

Public-key encryption does not conceal metadata like what computer a source used to send a message, when they sent it,
or how long it is. Public-key encryption on its own also does not tell the recipient anything about who sent a
message—it just conceals the content of a message in a ciphertext that can only be decrypted with the private key. \v

In a digital signature system, a sender can use a private key together with a message to create a signature. Anyone
with the corresponding public key can verify whether the signature matches the message, but a forger who does not
know the private key cannot find any message/signature pair that will pass verification with the public key.

\be
For example, a software publisher can create a signature key pair and include the public key in software installed on
computers. Later, the publisher can distribute an update to the software signed using the private key, and any
computer receiving an update can confirm it is genuine by verifying the signature using the public key. As long as
the software publisher keeps the private key secret, even if a forger can distribute malicious updates to computers,
they cannot convince the computers that any malicious updates are genuine.
\ee

Public key algorithms are fundamental security primitives in modern crypto-systems, including applications and
protocols which offer assurance of the confidentiality, authenticity and non-repudiability of electronic
communications and data storage.  Some public key algorithms provide key distribution and secrecy, some provide
digital signatures, and some provide both. Compared to symmetric encryption, asymmetric encryption is rather slower
than good symmetric encryption, too slow for many purposes. Today's crypto-systems (such as TLS, Secure Shell) use
both symmetric encryption and asymmetric encryption, often by using asymmetric encryption to securely exchange a
secret key which is then used for symmetric encryption.

\subsection{Secure Shell (SSH)}

\bd[Secure Shell (SSH)]
The \textbf{Secure Shell (SSH)} is a cryptographic network protocol designed on Unix-like operating systems in 1995
by Finnish computer scientist Tatu Ylönen, for operating network services securely over an unsecured network. SSH uses
public-key cryptography to authenticate the remote computer and allow it to authenticate the user, if necessary.
\ed

\bd[SSH Client]
An \textbf{SSH client} is a software program which uses the secure shell protocol to connect to a remote computer. This
article compares a selection of notable clients.
\ed

\bd[SSH Server]
An \textbf{SSH server} is a software program which uses the secure shell protocol to accept connections from remote
computers.
\ed

SSH applications are based on a client–server architecture, connecting an SSH client with an SSH server. SSH operates
as a layered protocol suite comprising three principal hierarchical components: the transport layer provides server
authentication, confidentiality, and integrity; the user authentication protocol validates the user to the server;
and the connection protocol multiplexes the encrypted tunnel into multiple logical communication channels. \v

SSH may be used in several methodologies. In the simplest manner, both ends of a communication channel use
automatically generated public-private key pairs to encrypt a network connection, and then use a password to
authenticate the user. \v

When the public-private key pair is generated by the user manually, the authentication is essentially performed when
the key pair is created, and a session may then be opened automatically without a password prompt. In this scenario,
the public key is placed on all computers that must allow access to the owner of the matching private key, which the
owner keeps private. While authentication is based on the private key, the key is never transferred through the
network during authentication. SSH only verifies that the same person offering the public key also owns the matching
private key. \v

In all versions of SSH it is important to verify unknown public keys, i.e.\ associate the public keys with identities,
before accepting them as valid. Accepting an attacker's public key without validation will authorize an
unauthorized attacker as a valid user. \v

On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is
allowed to log in remotely, in the file \code{~/.ssh}. This file is respected by SSH only if it is not writable by
anything apart from the owner and root. When the public key is present on the remote end and the matching private key
is present on the local end, typing in the password is no longer required. However, for additional security the private
key itself can be locked with a passphrase. The private key can also be looked for in standard places, and its full path
can be specified as a command line setting as we will see later. \v

SSH also supports password-based authentication that is encrypted by automatically generated keys. In this case, the
attacker could imitate the legitimate server side, ask for the password, and obtain it (man-in-the-middle attack).
However, this is possible only if the two sides have never authenticated before, as SSH remembers the key that the
server side previously used. The SSH client raises a warning before accepting the key of a new, previously unknown
server. Password authentication can be disabled from the server side. \v

SSH supports several public key algorithms for authentication keys. These include:
\bit
\item \textbf{rsa}: An old algorithm based on the difficulty of factoring large numbers. A key size of at least 2048
bits is recommended for RSA; 4096 bits is better. RSA is getting old and significant advances are being made in
factoring. Choosing a different algorithm may be advisable. It is quite possible the RSA algorithm will become
practically breakable in the foreseeable future. All SSH clients support this algorithm.
\item \textbf{dsa}: An old US government Digital Signature Algorithm. It is based on the difficulty of computing
discrete logarithms. A key size of 1024 would normally be used with it. DSA in its original form is no longer
recommended.
\item \textbf{ecdsa}: A new Digital Signature Algorithm standardized by the US government, using elliptic curves. This
is probably a good algorithm for current applications. Only three key sizes are supported: 256, 384, and 521 bits. We
would recommend always using it with 521 bits, since the keys are still small and probably more secure than the smaller
keys (even though they should be safe as well). Most SSH clients now support this algorithm.
\item \textbf{ed25519}: This is a new algorithm added in OpenSSH. Support for it in clients is not yet universal.
Thus, its use in general purpose applications may not yet be advisable.
\eit

The \code{ssh-keygen} utility produces the public and private keys, always in pairs.

\bd[ssh-keygen]
\textbf{ssh-keygen} is a standard utility component of SSH protocol suite used to s used to generate, manage, and
convert authentication keys.
\ed

Here's a list of commonly used flags to the \code{ssh-keygen} tool:
\bit
\item \code{-b}: \textbf{Bits}: Specifies the number of bits in the key to create. For RSA keys, the minimum size is
1024 bits and the default is 3072 bits. Generally, 3072 bits is considered sufficient. DSA keys must be exactly 1024
bits. For ECDSA keys, the flag determines the key length by selecting from one of three elliptic curve sizes: 256,
384 or 521 bits. Attempting to use bit lengths other than these three values for ECDSA keys will fail. ECDSA-SK,
Ed25519 and Ed25519-SK keys have a fixed length and the flag will be ignored.
\item \code{-t}: \textbf{Type}: This option specifies the type of algorithm to be used in order for the key to be
created. Commonly used values are: \code{rsa}, \code{dsa}, \code{ecdsa}, and \code{ed25519}.
\item \code{-f}: \textbf{File}: Specifies the filename of the key file.
\item \code{-C}: \textbf{Comment}: Changes the comment included in the public key for a keyfile.
\eit

\begin{bash}
# create public-private key pair
ssh$-$keygen $-$b <bytes> $-$t <algorithm> $-$f <filename> $-$C <comment>
\end{bash}

\section{Homebrew}

\fig{brew}{0.04}

\bd[Homebrew]
Homebrew is a non$-$profit, free and open$-$source software package management system, originally written by Max Howell,
that simplifies the installation of software on Apple's operating system, as well as Linux.
\ed

Homebrew has been
recommended for its ease of use as well as its integration into the command line interface. Homebrew has made extensive
use of GitHub to expand the support of several packages through user contributions.

\subsection{Installation}
Homebrew can be installed by following the instruction in its \href{https://brew.sh}{official webpage}:

\begin{bash}
# install Homebrew
/bin/bash $-$c $``\$$(curl $-$fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)$"$
\end{bash}

Once it is installed, it needs to be added to the \code{PATH} in order to work:

\begin{bash}
# add Homebrew to PATH
echo `export PATH=$``$/opt/homebrew/bin:$\$${HOME}/.bin:$\$$PATH$"$' >> $\$$HOME/.zshrc
\end{bash}

\begin{bash}
# make sure Homebrew is installed properly and ready to brew
brew doctor
\end{bash}

\subsection{Package Management}
Homebrew is just a package manager. One of the main things one can do is to search for packages:

\begin{bash}
# search for a specific package
brew search <package>
\end{bash}

\begin{bash}
# see info of a package
brew info <package>
\end{bash}

Once a package is found then it can be installed by:

\begin{bash}
# install package
brew install <package>
\end{bash}

On top of packages Homebrew can install also applications using the \code{--cask}:

\begin{bash}
# install application
brew install --cask <application>
\end{bash}

After the installation is complete one can delete all useless stuff created during installation by:

\begin{bash}
# clean old packages not used anymore
brew cleanup
\end{bash}

Keep in mind that many packages have dependencies on other packages, that will also be installed even if they are not
explicitly asked to be installed. One can see a complete list of all packages installed by:

\begin{bash}
# see all installed packages including dependencies
brew list
\end{bash}

\begin{bash}
# see all installed packages excluding dependencies
brew leaves
\end{bash}

In order to upgrade packages and applications installed via Homebrew, first one has to make sure the Homebrew itself
is up to date using:

\begin{bash}
# update Homebrew itself
brew update
\end{bash}

Then, optionally, one can see the outdated packages by:

\begin{bash}
# see outdated packages
brew outdated
\end{bash}

and outdated applications by:

\begin{bash}
# see outdated applications
brew outdated --cask --greedy
\end{bash}

The flag \code{--greedy} is needed in applications in order to include auto$-$updated applications. \v

Then upgrade all packages:

\begin{bash}
# upgrade all packages
brew upgrade
\end{bash}

and all applications by:

\begin{bash}
# upgrade all packages and applications
brew upgrade --cask --greedy
\end{bash}

One can also upgrade a specific package or application by using the same commands and specifying the package or
application in the end. \v

Note that one can upgrade both packages and applications at once using the following handy command:

\begin{bash}
# upgrade all applications
brew upgrade --greedy
\end{bash}

Finally, one can also uninstall a package or application using:

\begin{bash}
# uninstall package
brew uninstall <package>
\end{bash}

\begin{bash}
# uninstall application
brew uninstall --cask <package>
\end{bash}

Notice that \code{brew uninstall} uninstalls only the specified package/application and not its dependencies. In
order to uninstall the no longer needed dependencies one needs to run:

\begin{bash}
# uninstall all unused package dependencies
brew autoremove
\end{bash}

\section{Git}

\fig{git0}{0.12}

\bd[Version Control System (VCS)]
\textbf{Version Control System} (\textbf{VCS}) is a class of systems responsible for managing changes to computer
programs, documents, large web sites, or other collections of information.
\ed

The need for a logical way to organize and control revisions has existed for almost as long as writing has existed,
but revision control became much more important, and complicated, when the era of computing began. Today, the most
capable (as well as complex) revision control systems are those used in software development, where a team of people
may concurrently make changes to the same files. \v

Version control systems are most commonly run as stand$-$alone applications, but revision control is also embedded in
various types of software, such as word processors and spreadsheets, collaborative web docs, and content management
systems. \v

There are two broad categories of version control systems, centralized and distributed.

\bd[Centralized Version Control System (CVCS)]
In a \textbf{Centralized Version Control System} (\textbf{CVCS}), a server acts as the main repository which stores
every version of code.
\ed

\bd[Distributed Version Control System (CVCS)]
In a \textbf{Distributed Version Control System} (\textbf{DVCS}), is a form of version control in which the complete
codebase, including its full history, is mirrored on every developer's computer.
\ed

Compared to centralized version control, distributed version control enables automatic management branching and
merging, speeds up most operations (except pushing and pulling), improves the ability to work offline, and does not
rely on a single location for backups. \v

\bd[Git]
\textbf{Git} is a free and open source software for distributed version control, originally authored by Linus
Torvalds in 2005 for development of the Linux kernel. Its goals include speed, data integrity, and support for
distributed, non$-$linear workflows (thousands of parallel branches running on different systems).
\ed

\subsection{Installation \& Configuration}

Git can be simply installed via Homebrew:

\begin{bash}
# install Git via Homebrew
brew install git
\end{bash}

Once installed one can check the current version as usual by:

\begin{bash}
# show Git version
git --version
\end{bash}

Git carries some configuration values like name, email, etc. One can see these values by:

\begin{bash}
# show configuration list
git config --list
\end{bash}

One can set these configuration values either globally (for all repositories) or locally (for specific repository).
More often than not there is no reason to have different configurations for different repositories, for this reason
most of the time global is the way to go. \v

In order to set these values one can use:

\begin{bash}
# set global user name
git config --global user.name <user_name>
\end{bash}

\begin{bash}
# set global user email
git config --global user.mail <user_mail>
\end{bash}

One can also set a global \code{.gitignore} file (assuming there is a \code{.gitignore} file in the directory to use)
by using:

\begin{bash}
# set global ignored files
git config --global core.excludesfile ~/.gitignore
\end{bash}

More on \code{.gitignore} files later.

\subsection{Git Repository (Repo)}

\bd[Git Repository (Repo)]
A \textbf{Git repository}, or more simply a \textbf{repo}, tracks and saves the history of all changes made to the
files in a Git project.
\ed

\bd[Repository Folder]
Git saves everything related to a repo in a hidden directory called \code{.git} also known as the \textbf{repository
folder}.
\ed

Before anything, one has to turn a directory into a repo by:

\begin{bash}
# start tracking a directory with git
git init
\end{bash}

In any repo there are 3 very important areas: the working tree (or working area), the staging area (or index) and the
history area (or repository).

\bd[Working Tree / Working Area]
The \textbf{working tree}, or \textbf{working area} in Git is a directory (and its files and subdirectories) on a
file system that is associated with a Git repository. It's full of the new, edited and removed files.
\ed

Any changes to the working tree are noted by the staging area and show up as modified files.

\bd[Staging Area / Index]
The \textbf{staging area}, or \textbf{index}, is a place to record things before committing to history.
\ed

\bd[History / Repository]
The \textbf{history}, or \textbf{repository} provides information about the commit history associated with a file.
\ed

\fig{git1}{0.35}

In the above example there is a file called \code{S1} in the working tree. \v

In practice \code{git init} turns a directory into a repo by creating the staging area and the history inside the
repository folder \code{.git}. If one deletes \code{.git} the repo turns to a simple directory and no version control
is implemented. \v

At any point, the \code{git status} command shows the current state of the files in the repo:

\begin{bash}
# show the status of files in repository
git status
\end{bash}

A file can be in many different states. Let's start with the ``ignored'' files.

\bd[Ignored File]
\textbf{Ignored files} are files that are, and will be, completely ignored by Git.
\ed

One can ignore a file, by firstly creating a \code{.gitignore} file in the repo, and then adding the filenames to be
ignored in it.

\bd[.gitignore]
The \textbf{.gitignore} file is a special file where Git finds all the files and directories in repo that should be
ignored.
\ed

\begin{bash}
# create local gitignore file
touch .gitignore
\end{bash}

Let's move on with ``untracked'' files.

\bd[Untracked Files]
\textbf{Untracked files} are files that have been created within the repo's working tree but have not yet been added
to the repository's staging area.
\ed

An untracked file can be cleaned (i.e.\ removed) by:

\begin{bash}
# remove untracked file from working tree
git clean $-$f <file>
\end{bash}

\begin{bash}
# remove all untracked files from working tree
git clean $-$f .
\end{bash}

Finally, let's see ``Changes not staged for commit'' files.

\bd[Changes Not Staged For Commit Files]
\textbf{Changes not staged for commit files} are files the are already tracked from Git (they are not untracked
files), however they have changes in the working tree that are not in staging area.
\ed

Both untracked and changes not staged for commit files can be added to the staging area by:

\begin{bash}
# add a specific file from working tree to staging area
git add <file>
\end{bash}

\fig{git2}{0.35}

\begin{bash}
# add all files from current directory and beyond from working tree to staging area
git add .
\end{bash}

\begin{bash}
# add all files from all directories from working tree to staging area
git add $-$A
\end{bash}

\begin{bash}
# add only tracked files $($ignore untracked$)$ from working tree to staging area
git add $-$u
\end{bash}

After adding a file from working tree to staging area, one can keep working on working tree. By doing that, working
tree and staging area will no longer be in sync. One can see the differences by:

\begin{bash}
# show difference between working tree and staging area
git diff
\end{bash}

One can throw away these changes from working tree by copying the file from staging area by:

\begin{bash}
# copy file from staging area to working tree $($discards any non$-$staged changes in working tree$)$
git checkout <file>
\end{bash}

\fig{git3}{0.19}

\begin{bash}
# copy all from staging area to working tree $($discards any non$-$staged changes in working tree$)$
git checkout .
\end{bash}

After being added to staging area a file is said to be ``Changes to be committed''. Changes to be committed files,
are files the were added to the staging area from the working tree. In a sense they are in a transition state where
they are waiting to be committed to the history but they are not yet.

\begin{bash}
# show difference between staging area and latest commit in history
git diff --cached
\end{bash}

A changes to be committed file can further be committed to history by:

\begin{bash}
# commit all files from staging area to history with a message
git commit $-$m <message>
\end{bash}

\fig{git4}{0.35}

More often than not, one can make mistakes and realize them after he or she committed them. In this case the
\code{git commit --amend} command is a convenient way to modify the most recent commit. It lets one to simply edit
the latest commit message or combine new, uncommitted staged changes with the previous commit instead of creating an
entirely new commit.

\begin{bash}
# edit the latest commit message nothing must be staged
git commit --amend $-$m <updated_message>
\end{bash}

\begin{bash}
# edit the latest commit by adding more staged files without changing the commit message
git commit --amend --no$-$edit
\end{bash}

Notice that amended commits are actually entirely new commits and the previous commit will no longer be on the branch.
For this reason it's very important to not amend public commits that other developers might have used to create new
branches from, because that would be a very confusing situation and complicated to recover from). \v

Each commit carries a unique hash, the author and date of the commit, and the message included when it was committed.
One can see this information for all the history of commits (with the most recent one on top) by:

\begin{bash}
# show commits history
git log
\end{bash}

\fig{git5}{0.7}

After adding a file from working tree to staging area, one can unstage the changes to be committed files by copying
files from the latest commit from history to the staging area (in essence, one uses this command to undo a \code{git
add}):

\begin{bash}
# unstage file from staging area by copying it from history $($history $\&$ working tree stay same$)$
git reset <file>
\end{bash}

\begin{bash}
# unstage all from staging area by copying them from history $($history $\&$ working tree stay same$)$
git reset
\end{bash}

\fig{git6}{0.2}

Important to notice that in both cases we will NOT remove the commit from history and we will NOT discard anything in
working tree (we need the aforementioned \code{git checkout} for this). \v

In order to remove commits from the history, the way to do so is by moving the history to a previous commit (that way
all the commits after that will be ignored/erased). The way to do so is again by \code{git reset}.

\begin{bash}
# move history back to a specific commit
git reset --<type> <commit_hash>
\end{bash}

There are 3 different types to \code{git reset}. Depending on the type, different things will happen:
\bit
\item \code{--soft}: does not touch the staging area or the working tree files at all, but just resets the history to
commit. This leaves all the changed files as changes to be committed.
\item \code{--mixed}: which is the default action, resets the history and the staging area to commit, but leaves the
working tree untouched (i.e.\ the changed files are preserved but not marked for commit).
\item \code{--hard}: resets all history, staging area and working tree. Any changes to tracked files in the working
tree and staging area since ``commit'' are discarded. Any untracked files or directories are deleted. In other words
it creates a clean slate out of ``commit''.
\eit

Notice that for the command \code{git reset --mixed <latest$\_$hash>} we can discard both the \code{--mixed} part
since it's the default action and the ``latest hash'' since when we don't specify a commit the latest one will be
used. Thus, the command turns to \code{git reset} and according to the definition we just gave, it resets both the
history and the staging area to the latest commit, but leaves the working tree untouched. \v

However, since the history is already in the latest commit it stays also untouched. Hence, the only thing happening
is that staging gets a copy of the latest commit from history. In other words it unstages files from staging area by
copying them from history and leaves history and working tree untouched. It's exactly the same command as we used to
update staging area from history before. \v

Here is a graph that summarizes what it has been covered so far.

\fig{git7}{0.5}

\subsection{Branches}

\bd[Branch]
A \textbf{branch} is a pointer to a specific commit.
\ed

The branch pointer moves along with each new commit one makes, and only diverges in the graph if a commit is made on a
common ancestor commit.

\bd[Parent Branch]
The \textbf{parent branch} is the initial branch from which the branch was created.
\ed

Branches allow to work on different versions of the same files in parallel. Edits on one branch can be independent of
work on other branches. One can then decide to incorporate or merge the changes into other branches. Branches result
in a separation of versions of the same files. In this way, once can have branches for different purposes (e.g: a
production branch, a development branch, etc). \v

\bd[Master Branch]
The \textbf{master branch} is a default branch in Git, which it is instantiated when first commit made on the project.
\ed

At any point one can see all available branches by:

\begin{bash}
# show all local branches
git branch
\end{bash}

One can create a new branch by:

\begin{bash}
# create a new branch
git branch <branch>
\end{bash}

and can delete it by:

\begin{bash}
# delete existing branch
git branch $-$d <branch>
\end{bash}

\begin{bash}
# force delete existing branch
git branch $-$D <branch>
\end{bash}

Once a new branch is created, one can move to this branch by:

\begin{bash}
# switch to existing branch
git checkout <branch>
\end{bash}

A very useful shortcut command in order to create and switch to a new branch at the same time is:

\begin{bash}
# create and switch to new branch
git checkout $-$b <branch>
\end{bash}

Once to a new branch, one can start working, adding and committing. At some point she might decide to merge the
branch with master. The way to do so is by checking out to the master branch and:

\begin{bash}
# merge branch to master by including all previous commits of branch in chronological order
git merge <branch>
\end{bash}

\begin{bash}
# merge branch to master by squashing all previous commits to one single commit
git merge --squash <branch>
\end{bash}

\begin{bash}
# abort merge
git merge --abort
\end{bash}

\subsection{Remote Repository (Remote)}

\bd[Remote Repository (Remote)]
A \textbf{remote repository}, also called a \textbf{remote}, is a Git repository that's hosted on the Internet or
another network.
\ed

In other words, a remote is simply a repository in another location from where one is currently working. For example,
we may be working on a repo on our laptop. A version of that same work may be on GitHub. From the perspective of our
laptop, GitHub is a remote repository. When there are changes to the repo at a remote, we can download those changes
to get them locally. If we make changes to our local repo, we can upload our changes to the remote. \v

The starting point is downloading the remote locally:

\begin{bash}
# clone a remote from url
git clone <remote_url>
\end{bash}

The remote will appear in the local environment under the name origin. Origin is a default, shorthand name for the
remote repository that a project was originally cloned from. More precisely, it is used instead of that original
repository's URL $-$ and thereby makes referencing much easier. \v

One can see the remotes available locally by:

\begin{bash}
# show all remotes
git remote
\end{bash}

\begin{bash}
# show full locations of the remotes
git remote $-$v
\end{bash}

One can see all the available branches (both local and remote ones) by:

\begin{bash}
# show all branches local and remote
git branch $-$a
\end{bash}

The local branches appear as with the previous command \code{git branch}, however the remote ones appear as
$<$remote$>/<$branch$>$ to indicate that these branches are local copies of branches coming from the remote. Usually
it's not a good idea to checkout to these branches, unless one wants to check what's happening in the remote. \v

After cloning a repo, a connection has been established between the local repo and the remote one. However, Git does
not automatically update the local repo with all the changes that might have occurred by others in the remote repo.
One has to do this themselves by:

\begin{bash}
# get all updates from all branches from the remote to the local repo
git fetch <remote>
\end{bash}

\begin{bash}
# get all updates from just one branch from the remote to the local repo
git fetch <remote> <branch>
\end{bash}

There is also a command which is the best utility for cleaning outdated branches. It will connect to a shared remote
repository remote and fetch all remote branches. It will then delete all remote branches in local repo that are no
longer in use on the remote repository.

\begin{bash}
# get all updates from all branches from the remote and clean out outdated branches
git fetch <remote> --prune
\end{bash}

No matter which fetch one uses, these updates update the local versions of the remote branches, i.e.\ the
$<$remote$>/<$branch$>$ branches. This means that the updates have not been implemented yet to our local, working
branches. In order for this to happen, once the updates have been downloaded, they need to be merged to the local
repo. This can be done by first checkout to the local branch that needs to be merged and then:

\begin{bash}
# merge the remote branch to the local checked$-$out branch
git merge <remote>/<branch>
\end{bash}

One can combine fetch and merge within one single command pull:

\begin{bash}
# pull $=$ fetch $+$ merge
git pull <remote> <branch>
\end{bash}

The opposite direction is to push local changes to remote. This is a very important process that one has to done
correctly especially when there a lot of people working on the same project. Here is the correct way:
\bit
\item Step 1: Make sure local and remote are in sync with \code{git pull}.
\item Step 2: Create a new local branch from a parent branch (usually from master) and checkout to it with \code{git
checkout $-$b $<$branch$>$}.
\item Step 3: Push the local branch to the remote repo under the same name, and establish a connection between local and
remote branch.

\begin{bash}
# push local branch to remote and establish a connection between them
git push $-$u <remote> <branch>
\end{bash}

After one has used the \code{$-$u} flag, the local branch is directly associated with the remote one. Hence, there is
no need any more to specify neither the remote nor the branch when pulling and pushing.
\item Step 4: Work on local branch, make changes and commit them to local repo by \code{git add} and \code{git commit}.
\item Step 5: Push the changes to remote branch on remote.

\begin{bash}
# push updates from local branch to remote
git push
\end{bash}

\item Step 6: Repeat steps 4 and 5 for as long as needed.
\item Step 7: When the work is done, pull all changes that have happened in the parent branch in remote by \code{git
pull $<$remote$>$ $<$parent$\_$branch$>$}, and rebase the local branch to get these changes.

\begin{bash}
# rebase local checked$-$out branch to get updates
git rebase <parent$\_$branch>
\end{bash}

Another way to do that is also:

\begin{bash}
# pull all updates from remote branch of remote location and rebase checked$-$out branch
git pull --rebase <remote> <parent$\_$branch>
\end{bash}

In practice \code{git rebase} goes back in time, to the time of creation of local branch when the remote parent
branch was at commit $X$, and switches the commit $X$ to the newest commit $Y$ of remote parent branch, incorporating
all the updates that have happened in remote parent branch while we were working in our local branch. In other words
it's like creating a local branch to work with right now, with the current state of the parent branch in remote. \v

As it makes sense, this might create some problems, in case this newest commit $Y$ contradicts the work that we have
been doing so far (i.e.\ changes on the same lines of same files). In this case we say that we have a conflict, and we
need to manually resolve it by deciding which of the two options we should keep. More often than not, we need to sync
with the person that made the changes of the conflict in order to find a solution.
\item Step 8: Resolve any conflicts that might have occurred during rebasing.
\item Step 9: Push to remote for one last time with \code{git push}.
\item Step 10: Create a Pull Request (PR) from Github in order to merge the locally created branch to master branch in
remote.
\item Step 11: Once ready, close the PR by merging and if/when you are done with the remote branch delete it from the
remote.
\item Step 12: Update local repo with the PR happened in remote by \code{git pull --prune} (prune, in order to also
delete the remote branch locally).
\item Step 13: Checkout to master and delete the local branch by \code{git branch $-$d $<$branch$>$}.
\eit

\subsection{Useful Commands}

In this section I will go through some useful, unrelated to each other, commands that I often use.

\subsubsection{Git Cherry$-$Pick}

Sometimes one may accidentally commit to the wrong branch. In that case she wants to copy paste the commit from the
wrong branch to the correct one. This can be done by \code{git cherry$-$pick}.

\begin{bash}
# copy paste a specific commit from a branch to the checked$-$out branch using its hash
git cherry$-$pick <commit_hash>
\end{bash}

Important to notice that the original commit is not removed from its original branch. One needs to checkout to the
original, wrong branch and \code{git reset} the wrong commit.

\subsubsection{Git Stash}
The \code{git stash} command is very useful when one wants to record the current state of the working tree and
staging area because there are some changes that are not quite ready to be committed and he needs to throw them away
from working tree but keep them somewhere in memory for future use. This happens a lot when one needs to switch
branches without committing stuff (maybe he was working on the wrong branch), or to revert back temporarily to where
he started to check the initial state. \v

In order to do so one can use:

\begin{bash}
# stash only tracked $($staged and unstaged$)$ changes, including an optional message
git stash <optional_message>
\end{bash}

\begin{bash}
# stash all tracked and untracked $($staged and unstaged$)$ changes, including an optional message
git stash $-$u <optional_message>
\end{bash}

In order to see a list with all the stashes:

\begin{bash}
# show all stashes
git stash list
\end{bash}

In order to remove stashes:

\begin{bash}
# remove a single stash entry from the list of stash entries
git stash drop <stash>
\end{bash}

\begin{bash}
# remove all stash entries
git stash clear
\end{bash}

When one is done and wants the changes back to working tree and staging area they can use:

\begin{bash}
# put changes back $($on top of the current working tree$)$ from stash list and remove it from list
git stash pop <stash>
\end{bash}

\begin{bash}
# put changes back $($on top of the current working tree$)$ from stash list and leave it in list
git stash apply <stash>
\end{bash}

Notice that in case where no specific stash is selected then the most recent one the one on top of stash list will be
popped/applied.

\newpage

\section{Docker}\label{sec:docker}

\fig{docker}{0.08}

\bd[Docker]
\textbf{Docker} is a set of platform as a service (PaaS)\footnotemark products that use OS$-$level virtualization to
deliver software in packages called containers.
\ed

\footnotetext[1]{Platform as a service (PaaS) is a category of cloud computing services that allows customers to
provision, instantiate, run, and manage a modular bundle comprising a computing platform and one or more
applications, without the complexity of building and maintaining the infrastructure typically associated with
developing and launching the application(s).}

\subsection{Introduction}

More often than not, when you have a team of developers working on some application, each developer would have to
install most of the services on their operating system directly. Depending on which operating system they're using,
the installation process will look different, and most of the time this process will contain multiple steps of
installation. This approach of setting up a new environment can actually be pretty tedious, depending on how complex
your application is. For example, if you have 10 services that your application is using, then you would have to do
that 10 times on each operating system environment. As it makes sense, the chances of something going wrong and error
happening is actually pretty high. \v

After developing the application, development team will produce artifacts together with a set of instructions of how
to actually install and configure those artifacts on the server. Then development team would give those artifacts
over to the operations team and the operations team will handle setting up the environment to deploy those
applications. The problem with this kind of approach is that, first of all, you need to configure everything and
install everything directly on the operating system, which we saw in the previous example that could actually lead to
conflicts with dependency version and multiple services running on the same host. Another problem that could arise
from this kind of process is when there is misunderstanding between the development team and operations because
everything is in a textual guide as instructions. All of these, could lead to some back and forth communication until
the application is successfully deployed on the server. \v

Docker is an open platform for developing, shipping, and running applications, which tries to solve all problems
described above. Docker enables you to separate your applications from your infrastructure so you can deliver
software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By
taking advantage of Docker's methodologies for shipping, testing, and deploying code quickly, you can significantly
reduce the delay between writing code and running it in production. \v

The key benefit of Docker is that it allows users to package an application with all of its dependencies into a
standardized unit for software development called a ``container''.

\bd[Docker Container]
A \textbf{Docker container}, or more simply a \textbf{container}, is a way to package applications with everything
they need inside the package, including the dependencies and all the necessary configuration. That package is
portable and can be easily shared and moved around within and between a development and an operation team.
\ed

The portability of containers plus everything packaged in one isolated environment gives it some of the advantages
that makes development and deployment process more efficient. More specifically, with containers, you do not have to
install any of the services directly on your operating system because the container is its own isolated operating
system layer with Linux based image. This makes the setting up your local development environment much easier and
much more efficient than the previous version. \v

On top of that you can have different versions of the same application running on your local environment without
having any conflict. Finally, with containers developers and operations are working in one team to package the whole
configuration dependencies inside the application. \v

Technically speaking, a container is made up of stacked images on top of each other.

\bd[Docker Image]
\textbf{Docker images}, or more simply \textbf{images}, are the basis of containers. An image is an ordered
collection of root filesystem changes and the corresponding execution parameters for use within a container runtime.
An image typically contains a union of layered filesystems stacked on top of each other. An image does not have state
and it never changes.
\ed

Images leave in an image repository. This is a special type of storage for images. Many companies have their own
private repositories where they store all their images, but there is also a public repository for images, called
``Docker Hub'', where you can browse and probably find any application images that you want. In Docker Hub there are
more than a hundred thousand images of different applications. For every application there is an official image. So
the only thing you need to do is run a Docker command that pulls an image that is stored somewhere in the repository
and then run it. This is, of course, a simplified version, but that makes exactly the problem that we saw on the
previous slide much more easier. No environmental configuration needed on the server. \v

So in simple words, a container is a runtime instance of an image, and consists of one or more images, an execution
environment and a standard set of instructions. (The concept is borrowed from actual shipping containers, which
define a standard to ship goods globally. Docker defines a standard to ship software). It is important to note that
most people use the terms image and container interchangeably, despite the fact that there is a fine difference
between the two. So remember, container is the running environment for an image. \v

Before we move on with going through the technical parts of Docker, let's first see a simplified example that
describes the workflow of Docker.

\be
So let's consider a simplified scenario where you're developing an application on your laptop. Right on your local
development environment, your application uses an SQL database, and instead of installing it on your laptop, you
download an image from the Docker Hub. You connect your application with the SQL database and you start developing.
When you are done with the first version of the application locally, you want to test it or you want to deploy it on
the development environment where a tester in your team is going to test it. You push your application in Git, or in
some other version control system, and that will trigger a continuous integration like Jenkins, or whatever you have
configured, which first it will build your application and then it will create an image out of it which then will get
pushed to a private image repository. Now is the next step, that could be configured on Jenkins or some other scripts
or tools, that image has to be deployed on a development server that pulls the image from the private repository, and
then pulls the SQL database that your application depends on from Docker Hub. And now you have two images, one being
your custom image and a the other one a publicly available SQL DB image which of course, they talk and communicate to
each other and run as an app.

\fig{docker6}{0.15}
\ee

\subsection{Installation}

Docker can be simply installed via Homebrew:

\begin{bash}
# install Docker via Homebrew
brew install --cask docker
\end{bash}

In order to make sure that Docker was installed properly, one can check the current Docker version:

\begin{bash}
# see Docker version
docker --version
\end{bash}

\subsection{Images}

As it makes sense the first step for using Docker is to have an image in hand. As we have already mentioned, one can
find thousand of images in the official Docker website ``Docker Hub''. Once you have decided on the image you are
interested in, you can pull the image locally on your computer by:

\begin{bash}
# pull image from Docker Hub
docker pull <image>:<version>
\end{bash}

More often than not, application have a lot of different versions (Hence, a lot of different images), so you need to
specify which one you want by including the version. In case the version is missing then Docker will pull the latest
version. As it is the norm for application to have many different version, it's a good practice to always include the
version even if you want the latest one. \v

One can see all available, locally pulled images by:

\begin{bash}
# show all available images
docker images
\end{bash}

By default \code{docker images} hides intermediate images. One can check them all by using:

\begin{bash}
# show all available images including intermediate ones
docker images $-$a
\end{bash}

Here follows a list with all the possible flags one can use with \code{docker images}:

\fig{docker4}{0.23}

In case one wants to get rid of an image they can use:

\begin{bash}
# remove image
docker rmi <image>
\end{bash}

The \code{docker rmi} cannot remove an image of a running container unless you use the force \code{$-$f} flag:

\begin{bash}
# remove image
docker rmi $-$f <image>
\end{bash}

Once can combine all the commands we just introduced to create a very useful command that removes all images from the
host:

\begin{bash}
# remove all images
docker rmi $-$f $\$$(docker images $-$a $-$q)
\end{bash}

\subsubsection{Building Images}

So far we have seen how to deal with images that are already done. However, what happens when we want to build an
image ourselves? For example, when a developer is done with her application, how can see turns it into an image that
can be pulled by others? In this section we will talk about building images. \v

Docker can build images automatically by reading the instructions from a special file called ``Dockerfile''.

\bd[Dockerfile]
A \textbf{Dockerfile} is a text document that contains all the commands a user could call on the command line to
assemble an image.
\ed

Here is the format of the Dockerfile:

\begin{block}
# Comment
INSTRUCTION arguments
\end{block}

Docker runs instructions in a Dockerfile in order. The instruction is not case$-$sensitive. However, convention is
for them to be UPPERCASE to distinguish them from arguments more easily. \v

Docker treats lines that begin with \code{$\#$} as a comment, unless the line is a valid parser directive. A
\code{$\#$} marker anywhere else in a line is treated as an argument. Comment lines are removed before the Dockerfile
instructions are executed. \v

Now let's take a detailed look in some of the most useful instructions one could include in a Dockerfile.
\bit
\item The \code{FROM} instruction initializes a new build stage and sets the base image (i.e.\ the parent image from
which you are building) for subsequent instructions. As such, a valid Dockerfile must start with a \code{FROM}
instruction. The image can be any valid image – it is especially easy to start by pulling an image from the Docker Hub.
\item The \code{WORKDIR} instruction sets the working directory for any \code{RUN}, \code{COPY}, and \code{CMD}
instructions that follow it in the Dockerfile (we will see these commands later). The \code{WORKDIR} instruction can
be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the
previous \code{WORKDIR} instruction. \v

If the \code{WORKDIR} doesn't exist, it will be created even if it's not used in any subsequent Dockerfile
instruction. If not specified, the default working directory is \code{/}. In practice, if you aren't building a
Dockerfile from scratch the \code{WORKDIR} may likely be set by the base image you're using. Therefore, to avoid
unintended operations in unknown directories, it is best practice to set your \code{WORKDIR} explicitly.
\item The \code{RUN} instruction will execute any commands in a new layer on top of the current image and commit the
results. The resulting committed image will be used for the next step in the Dockerfile. Important to notice that the
command will be executed inside the container and not in the host. \v

This concept of layering \code{RUN} instructions and generating commits conforms to the core concepts of Docker where
commits are cheap and containers can be created from any point in an image's history, much like source control.
\item The \code{ENV} instruction sets the environment variable <key> to the value <value>. This value will be in the
environment for all subsequent instructions in the build stage and can be replaced inline in many as well. The value
will be interpreted for other environment variables, so quote characters will be removed if they are not escaped.
Like command line parsing, quotes and backslashes can be used to include spaces within values. \v

The \code{ENV} instruction allows for multiple <key>=<value> variables to be set at one time. Be careful.
Environmental variables will be available to Docker during building time, but not to the image (and subsequently to
the container) during running time. In other words, the environmental variables defined in Dockerfile won't be
available to the containers.
\item The \code{COPY} instruction copies new files or directories from the <host$\_$path> in host and adds them to
the filesystem of the container at the path <image$\_$path>. Be careful. Copying from host, and pasting to image.
\item The \code{LABEL} instruction adds metadata to an image. A \code{LABEL} is a key$-$value pair. An image can have
more than one label. You can specify multiple labels on a single line. Labels included in base or parent images
(images in the \code{FROM} line) are inherited by your image. If a label already exists but with a different value,
the most$-$recently$-$applied value overrides any previously$-$set value.
\item The \code{EXPOSE} instruction informs Docker that the container listens on the specified network ports at
runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if the protocol is not
specified. \v

The \code{EXPOSE} instruction does not actually publish the port. It functions as a type of documentation between the
person who builds the image and the person who runs the container, about which ports are intended to be published. To
actually publish the port when running the container, use the \code{$-$p} flag on \code{docker run} to publish and
map one or more ports. Regardless of the \code{EXPOSE} settings, you can override them at runtime by using the
\code{$-$p} flag (more on that on containers section).
\item The \code{CMD} instruction provides the default, entrypoint command for the Dockerfile to execute. It is
identical to the \code{RUN} command, with two basic differences. The first one is that the \code{CMD} instructions is
actually telling to Dockerfile that the instructions are over, and the ``basic'' command to run is the one provided
in \code{CMD}. So for example, \code{CMD} could be to launch our application. Which naturally leads to the second one
which is that there can only be one \code{CMD} instruction in a Dockerfile. If you list more than one \code{CMD} then
only the last\code{CMD} will take effect.
\eit

So, this is an example of a Dockerfile:

\begin{block}
FROM <image>
WORKDIR <workdir>
RUN <command>
ENV <environmental_variable_1>=``<value_1>''
ENV <environmental_variable_2>=``<value_2>''
LABEL ``<label>''=``<value>''
COPY <host_path> <image_path>
EXPOSE 80
CMD [``<executable>'',``<param_1>'', $\ldots$,``<param_n>'']
\end{block}

After the Dockerfile has been completed, then we are are ready to build the image. Docker knows how to build it, due
to the instructions we have put in the Dockerfile. It very important to keep the name as Dockerfile, cause Docker by
default will search for it. The command to build an image is \code{build} and it is used as:

\begin{bash}
# build image using the instructions in Dockerfile
docker build $-$t <name>:<tag> <Dockerfile_path>
\end{bash}

As we have already said, it is a best practice to have the Dockerfile in the root of the application, in that case
the path for the Dockerfile is just the current directory \code{.}. \v

Very important, if any change takes place in the Dockerfile, the one has to re$-$build the image in order for the
changes to take effect. \v

Last but not least one can push a created image into a Docker repository with:

\begin{bash}
# push image to Docker repository
docker push <image>:<version>
\end{bash}

Of course, for this command to work one has to have configured a Docker repository somewhere, but this gets out of
topic for the purposes of this section.

\subsection{Containers}

Up to this point, we have only worked with images. There is no container involved and there is no image running in
one. In order to start running an image within a container one could run:

\begin{bash}
# run image
docker run <image>
\end{bash}

This command will initiate a container which will run the specified image. A very important note is that Docker will
try to find the specified image locally and if it fails to find it then it will search in Docker Hub, or any other
image repository we will specify. \v

One can also assign a assign a name to the container using the \code{--name} flag:

\begin{bash}
# run image and assign a name
docker run --name <name> <image>
\end{bash}

In case we don't assign a name, Docker will assign a random name by itself. \v

The \code{docker run} command will run the container in an interactive environment. In order to run a container in
background one can use the detach \code{$-$d} flag:

\begin{bash}
# run image in background
docker run $-$d <image>
\end{bash}

Recall that a container is just a virtual environment running on your host. And you can have multiple containers
running simultaneously on your host. Your host has certain ports available that you can open for certain applications.
By default the containers have one port exposed but, also by default, this port is not binded to any host port. \v

So in simple words, both run commands we showed, run the images within isolated containers, meaning that there is no
way for the container to communicate with other applications. So how it works is that you need to create a
so$-$called binding between a port from your host machine and a port from the container.

\begin{bash}
# run image and bind host and container port
docker run $-$p <host_port>:<container_port> <image>
\end{bash}

Once the port binding between the host and the container is already done, you can actually connect to the running
container using the port of the host. Notice that, of course, you can have two containers both listening at the same
container port as long as you're binding them to two different ports from your host machine. \v

One can include environmental variables for the container to use as such:

\begin{bash}
# run image with environmental variables
docker run $-$e <environmental_variable>=<value> <image>
\end{bash}

One can include many environmental variables by using multiple $-$e flags. \v

There is a huge list of available flags one can use with \code{docker run}, that can be find
\href{https://docs.docker.com/engine/reference/commandline/run/#options}{here}. \v

One can see all running containers by:

\begin{bash}
# show all running containers
docker ps
\end{bash}

By default \code{ps} hides stopped container. One can check them all by using:

\begin{bash}
# show all container including stopped ones
docker ps $-$a
\end{bash}

One can also see only the IDs of the container by using the quiet \code{$-$q} flag.

\begin{bash}
# show all running container IDs
docker ps $-$q
\end{bash}

Here follows a list with all the possible flags one can use with \code{docker ps}:

\fig{docker5}{0.20}

One can stop a running container by:

\begin{bash}
# stop a running container
docker stop <container>
\end{bash}

and start it again by:

\begin{bash}
# start a stopped container
docker start <container>
\end{bash}

Once can combine all the commands we just introduced to create a very useful command that stops all containers:

\begin{bash}
# stop all containers
docker stop $\$$(docker ps $-$a $-$q)
\end{bash}

In case one wants to get rid of a container they can use:

\begin{bash}
# remove container
docker rm <container>
\end{bash}

Note that \code{docker rm} cannot remove a running container unless you use the force \code{$-$f} flag:

\begin{bash}
# remove container even if it is running
docker rm $-$f <container>
\end{bash}

One can combine commands to create a handy command that removes all containers:

\begin{bash}
# remove all containers
docker rm $-$f $\$$(docker ps $-$a $-$q)
\end{bash}

When it comes to troubleshooting, if something goes wrong in the container, you want to see the logs of the container
by:

\begin{bash}
# batch$-$retrieve logs present at the time of execution
docker logs <container>
\end{bash}

The follow \code{$-$f} flag will continue streaming the new outputs from the container:

\begin{bash}
# batch$-$retrieve logs present at the time of execution and keep printing them as they come
docker logs $-$f <container>
\end{bash}

Here follows a list with all the possible flags one can use with \code{docker logs}:

\fig{docker2}{0.22}

Another very useful command in debugging is \code{docker exec} where we can actually run a new command in a running
container.

\begin{bash}
# run a new command in a running container
docker exec <container>
\end{bash}

More often than not one wants to actually connect to the container and use the terminal directly in the container. We
can achieve this with a combination of tty \code{$-$t} flag which allocates a pseudo$-$TTY and interactive
\code{$-$i} flag which keeps \code{STDIN} open.

\begin{bash}
# connect to the terminal of a running container
docker exec $-$i $-$t <container>
\end{bash}

Here follows all the possible flags one can use with \code{docker exec}:

\fig{docker3}{0.22}

\subsubsection{Multiple Containers}

Most of the time, an application needs more than one service at a time. For example our application might be a NodeJS
application which needs one Mongo database for storing user info as long as a Mongo Express UI in order to visualize
the Mongo database. In that case, one needs to pull multiple images (NodeJs, MongoDb, Mongo Express UI), but most
importantly to make these images to communicate and work together since most of the time services need to exchange
data. This is accomplished by creating a Docker network where all the containers live in. \v

\fig{docker7}{0.34}

In essence, a Docker network is primarily used to establish communication between containers and the outside world
via the host machine where the Docker daemon is running. Docker supports different types of networks, each fit for
certain use cases. In order to create a network you can run:

\begin{bash}
# create a Docker network
docker network create <network>
\end{bash}

In order to find all available networks:

\begin{bash}
# list all Docker networks
docker network ls
\end{bash}

Finally, to remove a no$-$longer needed network:

\begin{bash}
# remove a Docker network
docker network rm <network>
\end{bash}

Once a network is created, we are able to put container in it. If there are already running containers one wants to
put to a network, then the \code{docker network connect} command is the one to go:

\begin{bash}
# connect a container to a network
docker network connect <network> <container>
\end{bash}

You can connect a container by name or by ID. Once connected, the container can communicate with other containers in
the same network. \v

If on the other hand one wants to connect to the network a container that is not running yet, then one has to use the
\code{docker run} command including the network \code{$-$$-$net} flag

\begin{bash}
# connect a container to a network when it starts
docker run --net <network>
\end{bash}

While this process is quite straight forward, it is also a bit of an overkill. One has to create a network, and then
starting connecting container ony by one to the network. On top of that, she has to do that again every time she
needs to. For example, when the application is shipped for productionization, the production team would need to do
this process again. In order to save us from the trouble, Docker has an extra functionality called \code{docker
compose}, which helps us to launch multiple containers within a network in a reproducible way. \v

The \code{docker compose} is a tool that was developed to help define and share multi$-$container applications. With
\code{docker compose}, we can create a YAML file to define the services and with a single command, can spin
everything up or tear it all down. The big advantage of using \code{docker compose} is you can define your
application stack in a file, keep it at the root of your project repo (it's now version controlled), and easily
enable someone else to contribute to your project. Someone would only need to clone your repo and start the compose
app. In fact, you might see quite a few projects on GitHub doing exactly this now. \v

In order to begin, at the root of the app project, create a file named \code{docker$-$compose.yaml}.

\bd[docker$-$compose.yaml]
The \textbf{docker$-$compose.yaml} file is a YAML file defining services, networks, and volumes for a Docker application.
\ed

In the \code{docker$-$compose.yaml} file, we'll start off by defining the schema version. In most cases, it's best to
use the latest supported version. You can look at the Compose file reference for the current schema versions and the
compatibility matrix:

\begin{block}
version: <version>
\end{block}

Next, we'll define the list of services (or containers) we want to run as part of our application, and we'll start
migrating a service at a time into the compose file. We can pick any name for the service. The name will
automatically become a network alias, which will be useful when defining other services:

\begin{block}
version: <version>
services:
    <service_1>:
        image: <image_1>
        ports:
            $-$ <host_port_1>:<container_port_1>
        environment:
            $-$ <environmental_variable_1_1>=<value_1_1>
            $-$ <environmental_variable_1_2>=<value_1_2>
    <service_2>:
        image: <image_2>
        ports:
            $-$ <host_port_2>:<container_port_2>
        environment:
            $-$ <environmental_variable_2_1>=<value_2_1>
            $-$ <environmental_variable_2_2>=<value_2_2>
\end{block}

The above \code{docker$-$compose.yaml} file is just an example and in no way an exhaustive list. There are many more
parameters one can specify in the YAML and, of course, one can add as many services as he likes. \v

Once the \code{docker$-$compose.yaml} is ready, it can be shipped together with the application and then one can
launch all container at once by using:

\begin{bash}
# launch a Docker compose network
docker compose $-$f docker$-$compose.yaml up
\end{bash}

Notice that there is no need to create a network. \code{docker compose} will take care of that.

Once can also use the \code{$-$p]} flag to specify a name for the whole application. If not is given then Docker will
use the directory name as the default option.

\begin{bash}
# specify an alternate project name
docker compose $-$f docker$-$compose.yaml $-$p <name> up
\end{bash}

\subsection{Volumes}

When a container is deleted, relaunching the image will start a fresh container without any of the changes made in
the previously running container -- those changes are lost. As it makes sense this is quite inconvenient, since some
times we don't want to lose these changes. The most straight$-$forward example is in the case of a database where we
don't want to lose all the data when the container is deleted. \v

In order to be able to save (persist) data and also to share data between containers, Docker came up with the concept
of ``volumes''.

\bd[Volumes]
\textbf{Volumes} are directories (or files) that are outside of the container and exist as normal directories and files
on the host filesystem.
\ed

In simple terms, a directory folder on a host file system is mounted into a directory of folder in the virtual file
system of Docker, and then what happens is that when a container writes to its file system, it gets replicated or
automatically written on the host file system directory and vice versa. \v

There are 3 types of volumes in Docker. All of them use the \code{docker run} command, combined with the volume flag
\code{$-$v}. The first type of volume definition is called ``host volume'', and the main characteristic of this one
is that you decide where on the host file system that references made. In other words, which folder on the host file
system you mount into the container. The command to do so is:

\begin{bash}
# run a container combined with a host volume
docker run $-$v <host_volume_path>:<container_volume_path>
\end{bash}

The second type is where you create a volume just by referencing the container directory so you don't specify which
directory on the host should be mounted, which is being taken care by Docker itself. That directory is automatically
created by Docker and for each container there will be a folder generated that gets mounted automatically to the
container. This type of volumes are called ``anonymous volumes'' and can be created by:

\begin{bash}
# run a container combined with an anonymous volume
docker run $-$v <container_volume_path>
\end{bash}

The third volume type is actually an improvement of the anonymous volumes and it specifies the name of that folder on
the host file system, which acts as a simple reference to the directory. The selection of the name is up to you.
Compared to anonymous volumes, you can actually reference that volume just by name so you don't have to know exactly
the path. That type of volumes are called ``named volumes'' and can be created by:

\begin{bash}
# run a container combined with a named volume
docker run $-$v <name>:<container_volume_path>
\end{bash}

From these three types, the mostly used one and the one that you should be using in a production is actually the
named volumes because there are additional benefits to letting Docker actually manage those volume directories on the
host. \v

Of course, one can include volumes in the \code{docker$-$compose.yaml} file instead of creating them through
\code{run}. The way to do so is by including a list of all the volumes names on the same level as the services, and
then list them on the container level where you actually define which path that specific volume can be mounted. \v

The benefit of that is that you can actually mount a reference of the same folder on the host to more than one
containers, and that would be beneficial if those containers need to share the data. In this case, you would want the
same volume name or reference to two different containers and you can mount them into different path inside of the
container. \v

Here is an example of a \code{docker$-$compose.yaml} with volumes.

\begin{block}
version: <version>
services:
    <service_1>:
        image: <image_1>
        ports:
            $-$ <host_port_1>:<container_port_1>
        environment:
            $-$ <environmental_variable_1_1>=<value_1_1>
            $-$ <environmental_variable_1_2>=<value_1_2>
        volumes:
            $-$ <volume_1>:<container_volume_path_1>
    <service_2>:
        image: <image_2>
        ports:
            $-$ <host_port_2>:<container_port_2>
        environment:
            $-$ <environmental_variable_2_1>=<value_2_1>
            $-$ <environmental_variable_2_2>=<value_2_2>
        volumes:
            $-$ <volume_2>:<container_volume_path_2>
volumes:
    $-$ <volume_1>
    $-$ <volume_2>
\end{block}

As a final note, one can create and manage volumes outside the scope of a container by using the \code{docker volume}
commands. For example one can create a volume by:

\begin{bash}
# create a volume
docker volume create <volume>
\end{bash}

One can list all the created volumes by:

\begin{bash}
# list all volumes
docker volume ls
\end{bash}

Finally, to remove a volume:

\begin{bash}
# remove a volume
docker volume rm <volume>
\end{bash}

However, all these options create and manage volumes outside the scope of a container. In almost all the cases we
want to attach a container to a specific container, so we either use the \code{docker run} command or the
\code{docker$-$compose.yaml}.

\section{Kubernetes}\label{sec:kubernetes}

\subsection{Introduction}

There are two main architectural styles for building applications: ``monolithic'' and ``microservice''.

\bd[Monolithic Architecture]
\textbf{Monolithic architecture} is a software architecture where an application is built as a single unit.
\ed

In a monolithic architecture all the different parts of the application are tightly coupled and run as a single service,
making all the different functionalities to be grouped together. The main advantage of a this architecture is that
it is simple to develop, test, and deploy. However, it is difficult to scale and adapt to changing requirements. For
this reason, a new, more modern architectural style has emerged.

\bd[Microservice Architecture]
\textbf{Microservice architecture} is a software architecture  that structures an application as a collection of
services that are loosely coupled and independently deployable.
\ed

Microservice architecture is characterized by the ability to develop and deploy services independently, improving m
odularity,scalability, and adaptability. However, it introduces additional complexity, particularly in managing
distributed systems and inter-service communication, making the initial implementation more challenging compared to a
monolithic architecture. \v

Despite the challenges, microservices have become increasingly popular in recent years, as they offer a number of
advantages over monolithic architectures, with the leading one being the ability to scale and adapt to changing
requirements. \v

The rise of microservices architecture, increased the usage of container technologies, because the containers
actually offer the perfect host for small independent applications. This rise of containers in the microservice
technology, resulted in applications that are now comprised of hundreds or sometimes maybe even thousands of
containers. As it makes sense, managing those loads of containers across multiple environments using scripts and
self-made tools can be really complex and sometimes even impossible. This caused the need for having container
orchestration technologies.

\bd[Container Orchestration]
\textbf{Container orchestration} is the process of managing the lifecycle of containers.
\ed

Among other things, container orchestration technologies were designed to provide:
\bit
\item \textbf{High Availability:} The application has no downtime and it's always accessible by the user.
\item \textbf{Scalability:} The application has high performance, loads fast and the users have very high response rates.
\item \textbf{Disaster Recovery:} The infrastructure has mechanisms to restore the application to its latest state after
an unexpected disaster.
\eit

\bd[Kubernetes]
\textbf{Kubernetes} is an open-source container orchestration system for automating container application deployment,
scaling, and management.
\ed

Kubernetes was released by Google in 2015 and it is now maintained by the Cloud Native Computing Foundation which is a
Google partnership with the Linux Foundation. The design and development of Kubernetes was influenced by Google's Borg
cluster manager. Unlike Borg, which was written in C++, Kubernetes source code is in the Go language. \v

Kubernetes aims to provide a platform for automating deployment, scaling, and operations of container workloads.
Google was already offering managed Kubernetes services, while Red Hat was supporting Kubernetes as part of OpenShift
since the inception of the Kubernetes project in 2014. In 2017, the principal competitors rallied around Kubernetes
and announced adding native support for it. Kubernetes works with a variety of container runtimes, including Docker
which we introduced in the previous section. \v

On March 6, 2018, Kubernetes Project reached ninth place in the list of GitHub projects by the number of commits, and
second place in authors and issues, after the Linux kernel.

\subsection{Components}

Kubernetes consists of a large collection of components, each one with its own usage. It is crucial to understand
these individual components since they are the basis of the Kubernetes architecture. For this reason in this section
we will go through each one of them and explain their role. \v

Let's start with ``containers''. We have already defined the concept of a Docker container in the Docker chapter as a
way to package applications with everything they need inside the package, including the dependencies and all the
necessary configuration. The definition of a container in Kubernetes is exactly the same, however one needs to keep
in mind that now we are not focusing exclusively on Docker containers although more often than not Docker is the
preferred container technology.

\bd[Container]
A \textbf{container} is a ready-to-run software package, containing everything needed to run an application: the
code and any runtime it requires, application and system libraries, and default values for any essential settings.
\ed

\fig{k81}{0.45}

The container is the lowest level of a microservice, which holds the running application, libraries, and their
dependencies. Kubernetes containers are not restricted to a specific operating system, unlike virtual machines.
Instead, they are able to share operating systems and run anywhere. The most common container technology is Docker. \v

Although containers, is the lowest level of microservices, it is not the lowest level of Kubernetes. The basic
component, or the smallest unit of Kubernetes, is a ``pod''.

\bd[Pod]
The basic scheduling unit in Kubernetes is a \textbf{pod} which is a grouping of containerized components. A container
resides inside a pod.
\ed

\fig{k82}{0.45}

Pod is an abstraction over a container, creating a layer on top of the container in order to abstract away the
underlying container technology so that you can replace them, if you want to, and also to don't have to directly work
with whatever container technology you use and only interact with the Kubernetes layer. \v

Usually one runs only one application per pod. This is one of best practices, and it is a good rule to follow since
makes things easier and aligns the philosophy of a container with this of a pod, and it also aligns with the general
philosophy of microservices. However, more often than not, one needs more than one applications in parallel (for
example an app and a database), hence, more than one pods in parallel. For this reason, Kubernetes has a component
called a ``node'' (or ``worker node'').

\bd[Node / Worker Node]
A \textbf{node} (or \textbf{worker node}) is a physical or virtual server where pods are deployed in.
\ed

\fig{k83}{0.45}

Kubernetes offers an out of the box virtual network which means that each pod within a node gets its own IP address
which can use to communicate with other pods in the node. This is of course an internal IP address, so an application
pod can communicate with a database pod but not anyone else.

\fig{k84}{0.45}

Another important concept in Kubernetes is that pod components are ephemeral, which means that they can die very
easily. When that happens, a new one gets created in its place and it is assigned a new IP address. This obviously is
inconvenient since if you communicate with other pods by using IP addresses, you have to adjust them every time a pod
restarts. In order to solve this problem, Kubernetes uses another component called ``service''.

\bd[Service]
A \textbf{service} is a set of pods that work together, such as one tier of a multi-tier application. The set of
pods that constitute a service are defined by a label selector.
\ed

Service assigns a stable IP address and DNS name to each pod, and load balances traffic to network connections of
that IP address among the pods, matching the selector (even as failures cause the pods to move from machine to
machine). In other words, services and pods' lifecycles are not connected, so even if a pod dies the service will
stay so you don't have to change that endpoint.

\fig{k85}{0.45}

As we already said, services (and IPs) are only for internal communication and not accessible outside a node.
However, one would want the application to be accessible from outside (through a browser for example) and for this
reason one would have to create an external service that opens the communication from external sources. Obviously
though, you wouldn't want your database to be open to the public requests. For this reason, there is another
component of Kubernetes called ``ingress''.

\bd[Ingress]
\textbf{Ingress} is an API object that provides routing rules to manage external users' access to the services in a
node, typically via HTTPS/HTTP\@.
\ed

\fig{k86}{0.5}

With ingress, you can easily set up rules for routing traffic without creating a bunch of load balancers or exposing
each service on the node. In this way, instead of service, the request goes first to ingress, and ingress does the
forwarding then to the service. \v

A common application challenge is deciding where to store and manage configuration information, some of which may
contain sensitive data. Configuration data can be anything as fine-grained as individual properties or
coarse-grained information like entire configuration files or JSON / XML documents. Kubernetes provides two closely
related mechanisms to deal with this need: ``configmaps'' and ``secrets'', both of which allow for configuration
changes to be made without requiring an application build.

\bd[ConfigMap]
A \textbf{ConfigMap} is an API object used to store non-confidential data in key-value pairs. Pods can consume
ConfigMaps as environment variables, command-line arguments, or even as configuration files.
\ed

\bd[Secret]
\textbf{Secret} is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such
information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't
need to include confidential data in your application code.
\ed

\fig{k88}{0.5}

Configmap is used to provide access to configuration through the filesystem visible to the container, while secret is
use to provide access to credentials needed to access remote resources securely, by providing those credentials on
the filesystem visible only to authorized containers. The biggest difference between a configmap and a secret is that
the content of the data in a secret is base64 encoded. \v

Recent versions of Kubernetes have introduced support for encryption
to be used as well. Secrets are often used to store data like certificates, passwords, pull secrets (credentials to work
with image registries), and ssh keys. \v

A configmap and/or a secret is only sent to a node if a pod on that node requires it. Kubernetes will keep it in
memory on that node. Once the pod that depends on the secret or configmap is deleted, the in-memory copy of all
bound configmaps and secrets are deleted as well. \v

As we already said, the data is accessible to the pod through one of two ways: either as environment variables (which
will be created by Kubernetes when the pod is started) or available on the container filesystem that is visible only
from within the pod. \v

Filesystems in the Kubernetes container provide ephemeral storage, by default. This means that a restart of the pod
will wipe out any data on such containers, and therefore, this form of storage is quite limiting in anything but
trivial applications. For this reason we use ``volumes''.

\bd[Volume]
A \textbf{volume} is a directory that contains data accessible to containers in a given pod in the orchestration and
scheduling platform.
\ed

\fig{k89}{0.5}

Volumes provide a plug-in mechanism to connect ephemeral containers with persistent data stores elsewhere. A pod
can define a volume, such as a local disk directory or a network disk, and expose it to the containers in the pod. A
volume provides persistent storage that exists for the lifetime of the pod itself. This storage can also be used as
shared disk space for containers within the pod. \v

As we already discussed in Docker chapter, volumes are mounted at specific mount points within the container, which
are defined by the pod configuration, and cannot mount onto other volumes or link to other volumes. The same volume
can be mounted at different points in the filesystem tree by different containers. \v

A node has multiple application pods with containers running on it and the way Kubernetes does that, is by using
three processes that must be installed on every node, that are used to schedule and manage those parts. \v

The first process that needs to run on every node is the container runtime.

\bd[Container Runtime]
A \textbf{container runtime}, also known as container engine, is a software component that can run containers on a host
operating system.
\ed

More often than not, Docker is the container runtime since in almost all cases developers use the Docker technology
for containerization. However, it could be any other container technology as well.

\fig{k90}{0.42}

The process that actually schedules container runtime in pods and monitors the containers underneath is kubelet which
is a process of Kubernetes itself, unlike container runtime.

\bd[Kubelet]
\textbf{Kubelet} is responsible for the running state of each node, ensuring that all containers on the node are
healthy.
\ed

Kubelet takes care of starting, stopping, and maintaining application containers organized into pods as directed by
the control plane. Kubelet monitors the state of a pod, and if not in the desired state, the pod re-deploys to the
same node. Kubelet is also responsible for assigning resources like CPU, RAM, and storage from the node to the pods
(containers).

\fig{k91}{0.42}

The third process that is installed in a node is the so called ``kube-proxy''.

\bd[Kube-Proxy]
\textbf{Kube-Proxy} is an implementation of a network proxy and a load balancer. It maintains network rules on the
node which allow network communication to the pods from network sessions.
\ed

\fig{k92}{0.42}

Kube-proxy supports the service abstraction along with other networking operation and it is responsible for routing
traffic to the appropriate pod (container) based on IP and port number of the incoming request. \v

This pretty much sums up the basic components of a node. What happens though, if the application pod dies, or
crashes, or one has to restart the pod because she built a new container image? As it makes sense, one would have a
downtime where a user cannot reach the application which is obviously a very bad thing if it happens in production. \v

This is exactly the advantage of distributed systems and containers. Instead of relying on just one application pod
and one database pod, we are replicating everything on multiple servers so we would have another node where a replica
of our application would run. This collection of replica nodes is called a ``cluster''.

\bd[Cluster]
A \textbf{cluster} is a set of nodes that run containerized applications. Clusters allow containers to run across
multiple machines and environments: virtual, physical, cloud-based, and on-premises.
\ed

Previously we said that the service is like an persistent static IP address with a DNS name so that you don't have to
constantly adjust the endpoint when a pod dies. However, service is also a load balancer which means that the service
will actually catch the request and forward it to whichever part is list busy. So services have both of these
functionalities and for this reason they are not really parts of specific nodes but they are shared between nodes
acting as the links among the nodes in a cluster.

\fig{k93}{0.4}

As it makes sense, Kubernetes needs to make sure that a stable set of replica Pods are running at any given time.
This is achieved with another component called ``ReplicaSet''.

\bd[ReplicaSet]
A \textbf{ReplicaSet} maintains a stable set of replica pods running at any given time. As such, it is often used to
guarantee the availability of a specified number of identical Pods.
\ed

A ReplicaSet is defined with fields, including a selector that specifies how to identify pods it can acquire, a
number of replicas indicating how many pods it should be maintaining, and a pod template specifying the data of new
pods it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating and
deleting pods as needed to reach the desired number. When a ReplicaSet needs to create new pods, it uses its pod
template. \v

While a ReplicaSet declares the number of instances of a pod that is needed, another component called
``ReplicationController'' manages the system so that the number of healthy pods that are running matches the number
of pods declared in the ReplicaSet.

\bd[ReplicationController]
A \textbf{ReplicationController} ensures that a specified number of pod replicas are running at any time.
\ed

In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is always up and available.
If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a ReplicationController
are automatically replaced if they fail or are terminated.

\fig{k94}{0.38}

In order to create multiple nodes in the cluster, you wouldn't need to define each replica separately from the
scratch, but instead you would need to define a blueprint for a pod and specify how many replicas you would like to
run. This blueprint is defined through another component called ``deployment''.

\bd[Deployment]
\textbf{Deployments} are a higher level management mechanism for ReplicaSets, providing declarative updates for pods and
replicaSets.
\ed

While the ReplicationController manages the scale of the ReplicaSet, deployments will manage what happens to the
ReplicaSet - whether an update has to be rolled out, or rolled back, etc. When deployments are scaled up or down,
this results in the declaration of the ReplicaSet changing - and this change in declared state is managed by the
ReplicationController.

\fig{k95}{0.38}

As pods are a layer of abstraction on top of containers, deployment is another abstraction on top of pods. In
practice you would not be working with pods, nodes, ReplicaSets and ReplicationControllers, but you would be creating
deployments. In deployments you specify how many replicas you want, and you can also scale up or scale down the
number of replicas that you need, which is more convenient than interacting with pods directly. With deployment in
place, if one of the replicas of your application pod would die, the service will forward the requests to another one
so your application would still be accessible for the user. \v

As it makes sense, we can't replicate database pods using a deployment, because database has a state which is its
data, meaning that if we have replicas of the database they would all need to access the same shared data storage. So
one would need some kind of mechanism that manages which pods are currently writing to that storage or which pods are
reading from that storage in order to avoid data inconsistencies. This mechanism in addition to replicating feature
is offered by another Kubernetes component called ``StatefulSet''.

\bd[StatefulSet]
\textbf{StatefulSet} manages the deployment and scaling of a set of pods, and provides guarantees about the ordering and
uniqueness of these pods.
\ed

Like a deployment, a StatefulSet manages pods that are based on an identical container spec. Unlike a deployment, a
StatefulSet maintains a sticky identity for each of their pods. These pods are created from the same spec, but are
not interchangeable: each has a persistent identifier that it maintains across any rescheduling. If you want to use
storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although
individual pods in a StatefulSet are susceptible to failure, the persistent pod identifiers make it easier to match
existing volumes to the new pods that replace any that have failed.

\fig{k96}{0.3}

In reality, deploying database applications using stateful sets in Kubernetes clusters can be somewhat tedious so
it's definitely more difficult than working with deployments where you don't have all these challenges. That's why
it's also a common practice to host database applications outside of the Kubernetes clusters and just have the
deployments or stateless applications that replicate and scale with no problem inside of the Kubernetes cluster and
communicate with the external database. \v

Now let's get into the details of a cluster. Clusters are comprised of one master node usually called ``master'' or
``control plane'' and a number of worker nodes (the ones we have seen so far) which can either be physical computers
or virtual machines, depending on the cluster.

\bd[Master Node / Control Plane]
The \textbf{master node} (or \textbf{control plane}) is a node which controls and manages a set of worker nodes and
resembles a cluster in Kubernetes.
\ed

In other words, the master node controls the state of the cluster; which applications are running and their
corresponding container images. The master node is the origin for all task assignments. It coordinates processes such
as scheduling and scaling applications maintaining a cluster's state and implementing updates.

\fig{k97}{0.3}

While worker nodes have the three processes we already described (container runtime, kubelet, kube-proxy), master
nodes have four completely different processes running inside them. As with worker nodes, these four processes run on
every master node that control the cluster. \v

The first processes is called ``API server''.

\bd[API Server]
The \textbf{API server} is a key component and serves the Kubernetes API using JSON over HTTP, which provides both the
internal and external interface to Kubernetes.
\ed

The API server processes and validates REST requests and updates the state of the API objects, thereby allowing
clients to configure workloads and containers across worker nodes.

\fig{k98}{0.3}

In simple words, when you as a user wants to deploy a new application in a Kubernetes cluster, you interact with the
API server using some client (it could be a UI like Kubernetes dashboard, or it could be a command line tool like
kubelet, or a Kubernetes API). In essence, API server is like a cluster Gateway which gets the initial request of any
updates into the cluster or even the queries from the cluster and it also acts as a gatekeeper for authentication to
make sure that only authenticated and authorized requests get through. That means that whatever you want to do, you
have to talk to the API server on the master node and the API server then will validate your request, and if
everything is fine then it will forward your request to other processes. This is very good for security, because you
just have one entry point into the cluster \v

Whenever you make a request to the API server, after it validates your request, it will hand it to another process in
the master node called ``scheduler'' which will perform the request through a new pod on one of the worker nodes.

\bd[Scheduler]
The \textbf{scheduler} is the pluggable component that selects which node an unscheduled pod runs on, based on resource
availability.
\ed

The scheduler tracks resource use on each node to ensure that workload is not scheduled in excess of available
resources. For this purpose, the scheduler must know the resource requirements, resource availability, and other
user-provided constraints and policy directives such as quality-of-service, affinity/anti-affinity
requirements, data locality, and so on. In essence, the scheduler's role is to match resource supply to workload demand.

\fig{k99}{0.3}

Scheduler instead of just randomly assigning to any node, it has this whole intelligent way of deciding on which
specific worker node the next pod will be scheduled. First it will look at your request and see how much resources
(CPU, RAM, etc) the pod that you want to schedule will need. Then it's going to go through all the worker nodes and
see the available resources on each one of them. If it says that one of them is the least busy or has the most
resources available it will schedule the new pod on that note. \v

An important point here is that the scheduler just decides on which node a new pod will be scheduled. The process
that actually starts that pod within the node is kubelet, which gets the request from the scheduler and executes it
on that specific node. \v

The next process is called ``controller manager'' and it is another crucial component that detects the pods that have
died and reschedules them as soon as possible.

\bd[Controller Manager]
A \textbf{Controller Manager} is a reconciliation loop that drives actual cluster state toward the desired cluster
state, communicating with the API server to create, update, and delete the resources it manages (pods, service
endpoints, etc).
\ed

The controller manager is a process that manages a set of core Kubernetes controllers.

\fig{k100}{0.3}

What controller manager does, is detecting the state changes (like crashing of pods) and trying to recover the
cluster state as soon as possible. In order to do so, it makes a request to the scheduler to reschedule those dead
pods. At the same time scheduler decides, based on the resource calculation, which worker nodes should restart those
pods again and makes requests to the corresponding kubelet on those worker nodes to actually restart the pods. \v

Finally, the last master process is called ``etcd'' and it is a key-value store of a cluster state.

\bd[etcd]
\textbf{etcd} is a persistent, lightweight, distributed, key-value data store developed by CoreOS that reliably
stores the configuration data of the cluster, representing the overall state of the cluster at any given point of
time.
\ed

\fig{k101}{0.3}

You can think of etcd as the cluster's brain, which means that every change in the cluster gets saved or updated into
this key-value data store of edcd. The reason why etcd is so important for a cluster, is because all the other
processes (scheduler, controller manager, etc) work because of etcd's data. For example, scheduler knows what
resources are available on worker node, and controller manager knows that a cluster state changed in some way, due to
the information that is stored in etcd. \v

It is important to note that the actual application data (e.g.\ database application running inside a cluster) will not
be stored in etcd, since the latter is just a database for cluster's state information used for master processes. \v

So now you probably already understood why that four master processes are absolutely crucial for the cluster's
operation. In practice a Kubernetes' cluster is usually made up of multiple masters nodes where each of them runs its
processes. Of course, the API server is also load balanced and the etcd store forms a distributed storage across all
the master nodes. \v

In a very small cluster you would probably have two or three master nodes and a couple of worker notes. Also to note
here that the hardware resources of master and worker nodes actually differ. Master processes are more important but
they actually have less load of work, so they need less resources than worker nodes, which do the actual job of
running those pods. As your application complexity, and its demand of resources increases, you may actually need to
add more master and worker nodes to your cluster, and thus forming a more powerful and robust cluster to meet your
application resource requirements.

\fig{k102}{0.35}

\section{Airflow}\label{sec:airflow}

\fig{airflow0}{0.022}

\subsection{Graphs}

\bd[Graph]
A \textbf{graph} is a structure consisting of a set of objects where some pairs of the objects are in some sense
``related''.
\ed

\bd[Vertex / Node / Point]
A \textbf{vertex}, or \textbf{node}, or \textbf{point}, is an object in a graph, represented by a circle.
\ed

\bd[Edge / Link / Arc / Line]
An \textbf{edge}, or \textbf{link}, or \textbf{arc}, or \textbf{line}, is a connection between two vertices in a graph,
represented by a line.
\ed

Two of the most important characteristics of graphs are: ``orientation'' and ``cycles''. Starting with the first,
depending on how the edges link the vertices, graphs can be divided into two categories: ``undirected'' and ``directed''.

\bd[Undirected Graph]
An \textbf{undirected graph} is a graph in which edges have no orientations.
\ed

\fig{airflow1}{0.18}

\bd[Directed Graph]
A \textbf{directed graph} is a graph in which edges have orientations.
\ed

\fig{airflow2}{0.18}

Moving on, let us define the concept of a ``cycle''.

\bd[Cycle]
A \textbf{cycle} in a graph is a path of edges and vertices wherein a vertex is reachable from itself.
\ed

Depending on whether a graph has cycles or not, graphs can be divided ``acyclic'' or ``cyclic''.

\bd[Acyclic Graph]
An \textbf{acyclic graph} is a graph that has no cycles.
\ed

\fig{airflow1}{0.18}

\bd[Cyclic Graph]
A \textbf{cyclic graph} is a graph that has cycles.
\ed

\fig{airflow3}{0.18}

Based on the last 4 definitions, a graph can be:
\bit
\item \textbf{Undirected Acyclic Graph:} A graph in which edges have no orientations and has no cycles.
\item \textbf{Undirected Cyclic Graph:} A graph in which edges have no orientations and has cycles.
\item \textbf{Directed Acyclic Graph:} A graph in which edges have orientations and has no cycles.
\item \textbf{Directed Cyclic Graph:} A graph in which edges have orientations and has cycles.
\eit

Out of these 4 categories, the most important one for our topic is the ``Directed Acyclic Graph'' or more simply
``DAG''. That is, a graph that consists of nodes and edges, with each edge directed from one node to another, such
that following those directions will never form a closed loop i.e.\ a cycle.

\subsection{Workflow}

\bd[Task]
A \textbf{task} is a unit of work in a workflow.
\ed

\bd[Workflow]
A \textbf{workflow} is an ordered series of dependent tasks, executed to achieve a desired result.
\ed

The dependency among tasks in a workflow arises from the fact that the output of one task is the input of the next
one, and so on. One way to make dependencies between tasks more explicit is to draw the workflow as a DAG. In this
graph-based representation, tasks are represented as nodes in the graph, while dependencies between tasks are
represented by edges between the nodes (tasks). The direction of the edge indicates the direction of the dependency,
with an edge pointing from task A to task B, indicating that task A needs to be completed before task B can start. \v

The acyclic property is extremely important, as it prevents the graph from running into circular dependencies between
tasks (where task 1 depends on task 2 and vice versa). These circular dependencies become problematic when trying
to execute the graph, as one runs into a situation where task 2 can only execute once task 3 has been completed, while
task 3 can only execute once task 2 has been completed. This logical inconsistency leads to a deadlock type of
situation, in which neither task 2 nor 3 can run, preventing someone from executing the graph.

\fig{airflow4}{0.55}

A nice property of this DAG representation is that it provides a relatively straightforward algorithm that one can use
to run the workflow. Conceptually, this algorithm consists of the following steps:
\begin{mdframed}
\begin{enumerate}
\item For each uncompleted task in the graph:
\bit
\item For each edge pointing toward the task, check if the ``upstream'' task on the other end of the edge has been
completed.
\item If all upstream tasks have been completed, add the task under consideration to a queue of tasks to be executed.
\eit
\item Execute the tasks in the execution queue, marking them completed once they finish performing their work.
\item Jump back to step 1 and repeat until all tasks in the graph have been completed.
\end{enumerate}
\end{mdframed}

\subsubsection{Workflow Management Platforms}

Although the graph representation of a workflow provides an intuitive overview of the tasks and their dependencies,
one may be wondering why not just use a simple script to run this linear chain of tasks. Indeed, in the recent past,
the way of running and scheduling graphs, was through running scripts with cron jobs (\ref{subsec:cron}). Although this
is a quite straight forward solution, it comes with some very basic problems:
\bit
\item \textbf{Failure Handling}: What to do in the case of a failure? What if a single task has failed?
\item \textbf{Monitoring}: How to monitor the status of the workflow run?
\item \textbf{Dependencies}: How to run in parallel tasks which are independent of each other?
\item \textbf{Scalability}: How to schedule between different cron machines?
\item \textbf{Deployment}: How to deploy new changes constantly?
\item \textbf{Process Historic Data}: How to backfill/rerun historical data?
\item \textbf{Modularity}: How to separate workflow sinto small incremental tasks rather than having one monolithic
script that does all the work?
\eit

The challenge of running graphs of dependent tasks is hardly a new problem in computing. To overomce these challenges,
over the years, many so-called ``workflow management platforms'' have been developed to tackle this problem, such as
``Apache Airflow'' by Airbnb, ``Luigi'' by Spotify, ``Metaflow'' by Netflix, ``Prefect'' by Prefect Technologies, and
many others. Picking the right workflow management solution requires some careful consideration of the key features of
the different solutions and how they fit the underlying needs and requirements. In these notes we will explore ``Apache
Airflow'', or more simply ``Airflow''.

\subsection{Airflow}

\bd[Airflow]
\textbf{Airflow} is an open-source workflow management platform for data engineering workflows.
\ed

Airflow started at Airbnb in October 2014 as a solution to manage the company's increasingly complex workflows.
Creating Airflow allowed Airbnb to programmatically author and schedule their workflows and monitor them via the
built-in Airflow user interface. From the beginning, the project was made open source, becoming an Apache Incubator
project in March 2016 and a top-level Apache Software Foundation project in January 2019. \v

Airflow is best thought of as a spider in a web: it sits in the middle of data processes and coordinates work happening
across the different (distributed) systems. As such, Airflow is not a data processing tool in itself but orchestrates
the different components responsible for processing data in data pipelines. \v

Similar to other workflow managers, Airflow allows to define workflows as DAGs of tasks. These DAGs are very similar
to the example sketched in the previous sections, with tasks being defined as nodes in the graph and dependencies as
directed edges between the tasks. Airflow works best with workflows that are mostly static and slowly changing. \v

Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results
of the task will be the same, and will not create duplicated data in a destination system), and should not pass
large quantities of data from one task to the next. For high-volume, data-intensive tasks, a best practice is to
delegate to external services specializing in that type of work. \v

Airflow handles workflows through ``Airflow Pipelines''.

\bd[Airflow Pipeline]
An \textbf{Airflow Pipeline} is a configuration as code (Python) consisted of several ordered tasks that define a
workflow.
\ed

Airflow is built on top of these 4 principles:
\bit
\item \textbf{Dynamic}: Airflow allows for writing code that instantiates pipelines dynamically.
\item \textbf{Extensible}: Airflow allows defining custom objects so that it fits the needs that suits the problem.
\item \textbf{Elegant}: Airflow pipelines are lean and explicit. Parameterizing scripts is built into the core of
Airflow using the powerful Jinja templating engine.
\item \textbf{Scalable}: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number
of workers.
\eit

Although Airflow has many rich features, several of Airflow's design choices may make it less suitable for certain cases:
\bit
\item \textbf{Handling Streaming Pipelines}: Airflow is primarily designed to run recurring or batch-oriented tasks,
rather than streaming workloads. However, Airflow is often used to process real-time data, pulling data off streams in
batches.
\item \textbf{Implementing Highly Dynamic Pipelines}: Airflow favors pipelines that do not change in structure every
time they run. Although it can implement this kind of dynamic behavior, the web interface will only show tasks that are
still defined in the most recent version of the DAG\@.
\item \textbf{No Python Experience}: Implementing DAGs in Python can be hard with no Python experience.
\item \textbf{Complexity}: Python code in DAGs can quickly become complex for larger use cases. As such, implementing
and maintaining Airflow DAGs require proper engineering rigor to keep things maintainable in the long run.
\item \textbf{Data Lineages}: Airflow is primarily a workflow management platform and does not (currently) include more
extensive features such as maintaining data lineages, data versioning, and so on.
\eit

\subsubsection{Airflow Architecture}

Although some of the concepts of this part will be introduced in the upcoming sections, it is useful to have a
high-level overview of the architecture of Airflow before diving into the details. The reader is advised to revisit
this section after reading the upcoming sections. \v

Airflow is organized into 3 components:
\bit
\item \textbf{Airflow Scheduler}: The ``brain of Airflow'', responsible for parsing DAG files, extracting the
corresponding tasks and their dependencies, checking their schedule interval, and starting scheduling the DAGs' tasks
for execution by passing them to the Airflow workers. At a high level, the scheduler runs through the following steps,
similar to the algorithm introduced previously:
\begin{mdframed}
\begin{enumerate}
\item The scheduler checks whether the schedule interval for the DAG has passed since the last time it was read. If so,
the tasks in the DAG are scheduled for execution.
\item For each scheduled task, the scheduler checks whether the dependencies (upstream tasks) of the task have been
completed. If so, the task is added to the execution queue.
\item The scheduler waits for several moments before starting jumping back to step 1.
\end{enumerate}
\end{mdframed}
\item \textbf{Airflow Workers}: The ``muscles of Airflow'', responsible for actually doing the work. Once tasks have
been queued for execution by the scheduler, they are picked up by workers that execute them in parallel and track their
results. These results are communicated to Airflow's metastore so users can view both the progress and logs of tasks
using the UI provided by the Airflow webserver.
\item \textbf{Airflow Webserver}: An extensive web interface that visualizes the DAG file parsed by the scheduler, and
provides the main interface for users to monitor DAG runs and their results. It also provides a clear overview of the
DAG's tasks and dependencies similar to the schematic overviews we've been drawing.
\eit

\subsubsection{Pip Install}

One can install Airflow through PyPi with a simple \code{pip install}:
\begin{bash}
# install Airflow through pip
pip install apache$-$airflow
\end{bash}

After installing Airflow, start it by initializing the metastore (a database in which all Airflow state is stored),
creating a user, and starting the scheduler and webserver:
\begin{bash}
# initialize the Airflow metastore
airflow db init
\end{bash}

\begin{bash}
# create a user
airflow users create $-$$-$username airflow $-$$-$password airflow
    $-$$-$firstname Airflow $-$$-$lastname Admin
    $-$$-$role Admin $-$$-$email airflowadmin@example.org
\end{bash}

\begin{bash}
# start the Airflow scheduler
airflow scheduler
\end{bash}

\begin{bash}
# start the Airflow webserver
airflow webserver
\end{bash}

Note the scheduler and webserver are both continuous processes that keep your terminal open. After you're set up, go
to localhost at port \code{8080} in your browser and log in with username "airflow" and password "airflow" to view
the Airflow webserver UI\@.

\subsubsection{Docker Compose Install}

Airflow has an active community, making constant upgrades and releases. Individual contributors create many
subpackages making Airflow able to communicate with other services. Setting aside that it takes a lot of time to
set up and config Airflow environment, managing and maintaining all the changes is also really difficult. Last but
not least, we also need a good way to share development and production Ariflow environments for all developers. \v

For all these reasons, a better approach to install Airflow is through Docker. \v

To deploy Airflow with Docker Compose, you should fetch \code{docker-compose.yaml}:
\begin{bash}
# fetch docker-compose.yaml
curl $-$LfO 'https://airflow.apache.org/docs/apache$-$airflow/2.10.2/docker$-$compose.yaml'
\end{bash}

You may get a warning that \code{AIRFLOW\_UID} is not set, but you can ignore it. You can also create a \code{.env} file
in the same folder as \code{docker-compose.yaml} with this content to get rid of the warning:
\begin{block}
AIRFLOW_UID=50000
\end{block}

Then, you need to run database migrations and create the first user account. To do this, run:
\begin{bash}
# initialize the Airflow database
docker compose up airflow$-$init
\end{bash}

After initialization is complete, you can start all services with:
\begin{bash}
# start all services
docker compose up $-$d
\end{bash}

After you're set up, go to localhost at port \code{8080} in your browser and log in with username "airflow" and password
"airflow" to view the Airflow webserver UI\@. \v

On top of that, you can connect to the docker container that runs Airflow by running:
\begin{bash}
# connect to the Airflow container
docker exec $-$it docker$-$airflow$-$airflow$-$webserver$-$1 /bin/bash
\end{bash}

Once you are done you can stop all services and clean up enviroment with:
\begin{bash}
# stop all services and clean up environment
docker compose down $-$$-$volumes $-$$-$rmi all
\end{bash}

\subsection{Airflow Tasks}

\bd[Airfow Task]
An \textbf{Airflow task} is the basic unit of execution in Airflow.
\ed

Although Airflow does much of the heavy lifting when it comes to running Airflow tasks (tasks), one needs to ensure
that tasks fulfill two key properties for proper result:
\bit
\item \textbf{Atomicity}: Tasks should be defined so that they either succeed and produce some proper result or fail in
a manner that does not affect the state of the system.
\item \textbf{Idempotency}: Calling the same task multiple times with the same inputs has no additional effect. This
means that rerunning a task without changing the inputs should not change the overall output.
\eit

The key part of using tasks is defining how they relate to each other, i.e.\ their dependencies, whicn in Airflow this
is achieved through the concept of ``downstream'' and ``upstream'' tasks.

\subsubsection{Task Dependencies}

\bd[Downstream / Upstream]
A \textbf{downstream} task is a task that depends on the completion of one or more \textbf{upstream} tasks to start.
\ed

\bd[Downstream / Upstream Bit Shift Operator]
The \textbf{downstream bit shift operator} \code{$\gg$} is used to define a downstream dependency between two tasks,
while conversly the \textbf{upstream bit shift operator} \code{$\ll$} is used to define an upstream dependency between
two tasks.
\ed

\be
\begin{block}
# taks$\_$2 is a downstream task of task$\_$1 that cannot start until upstream task$\_$1 finishes
task$\_$1 >> task$\_$2

# similar to the above, but using the upstream bit shift operator
task$\_$2 << task$\_$1
\end{block}
\ee

In addition to linear chains of tasks, Airflow can be used to create more complex dependency structures, with the
so-called ``fan-out'' and ``fan-in'' structure.

\bd[Fan-Out / Fan-In]
\textbf{Fan-out} is the case where a single task has multiple downstream dependencies, while conversly \textbf{fan-in}
is the case where a single task has multiple upstream dependencies.
\ed

Airlflow allows to define fan-out and fan-in dependencies by combining dependencies to a list.

\be
\begin{block}
# task$\_$1 and task$\_$2 are upstream tasks fanning into the downstream task$\_$3
[task$\_$1, task$\_$2] >> task$\_$3

# task$\_$1 is an upstream task fanning out to downstream task$\_$2 and task$\_$3
task$\_$1 >> [task$\_$2, task$\_$3]
\end{block}
\ee

\be
\fig{airflow8}{0.28}

One can reproduce the above picture as follows:
\begin{block}
task$\_$1 >> [task$\_$2, task$\_$4]
task$\_$2 >> task$\_$3
task$\_$4 >> task$\_$5
[task$\_$3, task$\_$5] >> task$\_$6
task$\_$6 >> task$\_$7 >> task$\_$8
\end{block}
\ee

\subsubsection{Task States}

Tasks can be in different ``states'' during their lifecycle.

\bd[Task State]
A \textbf{task state} is the current status of a task during its lifecycle.
\ed

The following are the different states a task can be in, together with their corresponding colors in the Airflow
webserver UI:
\bit
\item \textbf{none}: (white) The task has not yet been queued for execution (its dependencies are not yet met).
\item \textbf{\textcolor{scheduled}{scheduled}}: The scheduler has determined the task's dependencies are met and it
should run.
\item \textbf{\textcolor{queued}{queued}}: The task has been assigned to an executor and is awaiting a worker.
\item \textbf{\textcolor{running}{running}}: The task is running on a worker.
\item \textbf{\textcolor{success}{success}}: The task finished running without errors.
\item \textbf{\textcolor{failed}{failed}}: The task had an error during execution and failed to run.
\item \textbf{\textcolor{skipped}{skipped}}: The task was skipped due to branching (more on this later).
\item \textbf{\textcolor{upstream_failed}{upstream\_failed}}: An upstream task failed but trigger rule (more on this
later) says it is needed.
\item \textbf{\textcolor{up_for_retry}{up\_for\_retry}}: The task failed, but has retry attempts left and will be
rescheduled.
\item \textbf{\textcolor{up_for_reschedule}{up\_for\_reschedule}}: The task is a Sensor (more on this later) that is in
reschedule mode.
\item \textbf{\textcolor{shutdown}{shutdown}}: The task has been shutdown by the executor.
\item \textbf{removed}: (white) The task has vanished from the DAG since the run started.
\eit

The following image summarizes all the different, possible states of a task.

\fig{airflow10}{0.32}

Ideally, a task should flow from \textbf{none}, to \textbf{\textcolor{scheduled}{scheduled}}, to
\textbf{\textcolor{queued}{queued}}, to \textbf{\textcolor{running}{running}}, and finally to
\textbf{\textcolor{success}{success}}.

\fig{airflow11}{0.32}

\subsection{Airflow Operators}

\bd[Airfow Operator]
An \textbf{Airfow operator} is a predefined task template that one can string together quickly to build most parts of a
DAG\@.
\ed

Airflow tasks (tasks) and Airfow operators (operators) are somewhat considered interchangeable, and the two
often substitute each other in discussions. However, it is useful to think of them as separate concepts. Tasks
manage the execution of an operator; they can be thought of as a small wrapper around an operator that ensures the
operator executes correctly.

\fig{airflow6}{0.33}

In essense, one instantiates operators in DAG files (that we will see right after) and turns them into tasks,
similar to an object (operator) and an instance (task). Hence, the user can focus on the work to be done by using
operators, while Airflow ensures correct execution of the work via tasks. For this reason, from now on we will stick
with the term ``operator''. \v

There are three basic kinds of operators in Airflow:
\bit
\item \textbf{Operators}: The basic operator which triggers a certain action.
\item \textbf{Sensors}: A subclass of operators built for waiting for an external event to happen.
\item \textbf{Transfers}: A subclass of operators built for moving data from one location to another.
\eit

For now, we will focus on the first kind of operators, which is the most basic one. Later in this chapter we will see
the other two kinds. \v

The main class object that defines all operators is the so-called \code{BaseOperator} class. All operators inherit
from this class, which provides the basic functionality that all operators share. The following are all subclasses of
the \code{BaseOperator} inheriting its functionality:
\bit
\item \code{BaseOperator}: Base class for all operators, also used for creating custom operators.
\item \code{DummyOperator}: Does nothing. It can be used to define a task that is a no-op.
\item \code{BashOperator}: Executes a Bash command.
\item \code{PythonOperator}: Executes a Python function.
\item \code{BranchPythonOperator}: Executes a Python function that determines the path of execution.
\item \code{EmailOperator}: Sends an email.
\item \code{SimpleHTTPOperator}: Calls an HTTP endpoint.
\item \code{MySQLOperator}: Executes a SQL command in MySQL\@.
\eit

\subsubsection{Taskflow API}

Airflow also provides a decorator-based API for defining operators through Python functions.

\bd[Taskflow API]
The \textbf{Taskflow API} is a decorator-based API for defining operators through Python functions.
\ed

Taskflow API aims to simplify the definition of operators by making it easier to convert Python functions to
operators, hence making it much easier to author clean DAGs without extra boilerplate. \v

Taskflow API provides the following decorators:
\bit
\item \code{@dag}: A decorator that converts a Python function to a DAG\@.
\item \code{@task}: A decorator that converts a Python function to an operator.
\eit

One can use both apporaches (operators and TaskFlow API) within the same DAG, allowing to tailor the DAG definition to
specific needs within the data pipeline.

\subsubsection{Branching}

Beyond the main functionality of building workflows, operators offer a few additional features that can be useful in
more complex workflows. One of these features is ``branching''.

\bd[Branching]
\textbf{Branching} is a feature that allows to define a conditional statement in a DAG that determines the path of
execution based on the result of a task.
\ed

In essense, branching is the case where one needs a workflow to go down a certain path based on an arbitrary
condition which is typically related to something that happened in an upstream task. \v

One way to do this is by using the \code{BranchPythonOperator} (a subclass of the \code{BaseOperator}) which is much
like the \code{PythonOperator} except that it expects a \code{python\_callable} that returns a \code{task\_id} (or list
of \code{task\_ids}) referencing a task directly downstream from the \code{BranchPythonOperator} task. The \code{task\_id}
returned is followed, and all the other paths are skipped.

\be
In the following example, task\_2 is a \code{BranchPythonOperator} deciding whether to go down the task\_3 or task\_5
path.

\fig{airflow9}{0.25}
\ee

\subsection{Airflow DAGs}

Operators have a single piece of responsibility: they exist to perform one single unit of work, and multiple operators
together form an ``Airflow DAG''.

\bd[Airflow DAG]
An \textbf{Airflow DAG} is a container that is used to organize tasks and set their execution context.
\ed

Airflow DAGs (DAGs) contain both the operators and the dependencies among them, defining in that way the the order
in which they should be executed. The role of a DAG is to orchestrate the execution of this collection of operators,
including the starting and stopping of operators, starting consecutive tasks once an operator is done, ensuring
dependencies between operators are met, and so on. \v

Important to notice that one declares their tasks first, and then they declare their dependencies second. All these
happen in the so-called ``Airflow DAG file''.

\bd[Airflow DAG File]
An \textbf{Airflow DAG file} is a Python script located in the \code{dags} directory that defines the structure of the
corresponding DAG\@.
\ed

Each Airflow DAG file (DAG file) typically describes the set of tasks for a given DAG and the dependencies between
the tasks (i.e., upstream and downstream tasks), which are then parsed by Airflow to identify the DAG structure.
Other than this, DAG files typically contain some additional metadata about the DAG telling Airflow how and when it
should be executed, and so on. \v

One advantage of defining Airflow DAGs in Python code is that this programmatic approach provides a lot of
flexibility for building DAGs. This flexibility gives a great deal of customization in building pipelines, allowing
one to fit Airflow to their needs for building arbitrarily complex pipelines. In addition to this flexibility,
another advantage of Airflow's Python foundation is that tasks can execute any operation that one can implement in
Python. Over time, this has led to the development of many Airflow extensions that enable to execute tasks across a
wide variety of systems, allowing to build complex data pipelines bringing together data processes across many
different systems. \v

A typical DAG file consists of 3 steps:
\ben
\item \textbf{Define DAG}: This is done by creating an instance of the \code{DAG} class (either as operator or as the
\code{@dag} decorator), which takes a number of arguments to define the DAG's metadata. The most important arguments
are:
\bit
\item \code{dag\_id}: The id of the dag, acting as a unique identifier.
\item \code{description}: The description of the DAG\@.
\item \code{schedule}: A cron expression defining rules according to which DAG runs are scheduled. \v

An important limitation of cron expressions is that they are unable to represent certain frequency-based schedules,
e.g.\ ``every three days''. To support this type of frequency-based schedule, Airflow also allows to define intervals
in terms of a relative time interval. To use such a frequency-based schedule, one can pass a timedelta instance (from
the \code{datetime} module) as a schedule interval, e.g.\ \code{schedule=dt.timedelta(days=3)}.

\item \code{start\_date}: The date and time when the DAG should be run. \v

Imporant to notice that the first
execution will happen at the first \code{schedule} \textbf{after} \code{start\_date}, i.e.\ \code{start\_date} +
\code{schedule} interval. Subsequent runs will continue executing at \code{schedule} following this first interval.

\fig{airflow7}{0.25}

\item \code{end\_date}: The date beyond which DAG won't run. Defaults to \code{None} for open-ended.
\item \code{default\_args}: A dictionary of default parameters to be used as constructor parameters when initialising
operators. Note that operators arguments precede those defined here.
\item \code{params}: A dictionary of DAG level parameters that are made accessible in templates, namespaced under params.
These params can be overridden at the task level.
\item \code{catchup}: Whether to backfill missed runs between the current date and \code{start\_date} (if any).
\item \code{tags}: A list of tags that can be used to categorize DAGs.
\item \code{dag\_display\_name}: The display name of the DAG in the Airflow webserver UI\@.
\eit
\item \textbf{Define Tasks}: This is done by creating instances of the desired operators (either as operator or using
the \code{@task} decorator) and setting their dependencies. This is done by instantiating the operator class, passing
the operator's arguments as constructor parameters. \v

Although each operator has its own set of arguments, there are a few common arguments inherited from the
\code{BaseOperator} class that are available for all operators. The most important ones are:
\bit
\item \code{task\_id}: The id of the task, acting as a unique identifier.
\item \code{owner}: The owner of the task.
\item \code{dag}: The DAG to which the task belongs.
\item \code{depends\_on\_past}: Whether the task should run when the previous task has succeeded.
\item \code{retries}: The number of retries that should be performed before failing the task.
\item \code{retry\_delay}: The delay between retries.
\item \code{trigger\_rule}: Rules to determine whether task is ready to execute, as a function of its dependencies. The
most common trigger rules are:
\bit
\item \code{all\_success}: All upstream tasks are in a success state. This is the default trigger rule.
\item \code{all\_failed}: All upstream tasks are in a failed state.
\item \code{all\_done}: All upstream tasks are done regardless of their state.
\item \code{all\_skipped}: All upstream tasks are in a skipped state.
\item \code{one\_success}: At least one upstream task is in a success state.
\item \code{one\_failed}: At least one upstream task is in a failed state.
\item \code{one\_done}: At least one upstream task is done regardless of their state.
\item \code{none\_failed}: No upstream tasks are in a failed state.
\item \code{none\_skipped}: No upstream tasks are in a skipped state.
\item \code{always}: The task is always ready to run.
\eit
\item \code{email}: The email address to send notifications to.
\item \code{email\_on\_retry}: Whether to send an email notification when the task is retried.
\item \code{email\_on\_failure}: Whether to send an email notification when the task fails.
\eit

One needs to keep in mind that some of these arguments are meant to be shared across all tasks in the DAG, and are
therefore defined at the DAG level in the \code{default\_args} dictionary. This way, one can avoid repeating the same
arguments for each task in the DAG\@. As already mentioned, arguments defined on task level will override those
\code{default\_args} at DAG level.

\item \textbf{Define Dependencies}: This is done by using downstream and upstream tasks, fan-out and fan-in patterns,
and branching, as described in the previous sections.
\een

\fig{airflow5}{0.25}

\be
The following is an example of a simple DAG file using the operator approach.

\begin{block}
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator, PythonOperator

# Define default arguments for the DAG shared by all tasks
default_args = {
    'owner': 'Airflow',
    'retries': 5,
    'retry_delay': timedelta(minutes=1)
}

# 1. Instantiate DAG

dag = DAG(
    dag_id='tutorial',
    description='Simple tutorial DAG',
    schedule="0 0 * * *",
    start_date=datetime(2021, 1, 1),
    end_date=None,
    default_args=default_args,
    catchup=False,
)

# 2. Define tasks

task_1 = BashOperator(

    # The name of the task
    task_id='echo_hello_world',

    # The command to be executed
    bash_command='echo hello world',

    # The DAG this task belongs to
    dag=dag,
)

task_2 = BashOperator(
    task_id='sleep_five_seconds',
    bash_command='sleep 5',
    dag=dag
)

def _print_hello_world(message: str):
    print(message)

task_3 = PythonOperator(

    # The name of the task
    task_id='print_hello_world',

    # The function to be called
    python_callable=_print_hello_world,

    # A dictionary of keyword arguments that will get unpacked in the function
    op_kwargs = {
        'message': 'Hello World!'
    },

    # The DAG this task belongs to
    dag=dag
)

# 3. Set up dependencies

task_1 >> [task_2, task_3]
\end{block}
\ee

\be
The following is the same example as before using the Taskflow API\@.

\begin{block}
from datetime import datetime, timedelta
from airflow.decorators import dag, task

# Define default arguments for the DAG shared by all tasks
default_args = {
    'owner': 'Airflow',
    'retries': 5,
    'retry_delay': timedelta(minutes=1)
}

# 1. Instantiate DAG

@dag(
    dag_id='tutorial',
    description='Simple tutorial DAG',
    schedule="0 0 * * *",
    start_date=datetime(2021, 1, 1),
    end_date=None,
    default_args=default_args,
    catchup=False
)
def dag():

    # 2. Define tasks

    @task.bask
    def echo_hello_world():
        f"""echo hello world"""

    @task.bask
    def sleep_five_seconds():
        f"""sleep 5"""

    @task
    def print_hello_world(message: str):
        print(message)

    # 3. Set up dependencies

    echo_hello_world()
    sleep_five_seconds()
    print_hello_world("Hello World!")
\end{block}
\ee

\subsection{Airflow Sensors}

In the previous subsection, we explored how to schedule DAGs in Airflow by setting the \code{schedule} argument in
the DAG file. While this is the most common way to start a workflow at a certain, predefined time, Airflow provides
a few other ways to trigger DAGs based on external events. This is done through the sensor operators (sensors) that
have already been defined as the second most basic kind of Airflow operators. \v

Sensors are placed in the beginning of a DAG\footnote{When a sensor is part of a DAG, the \code{start\_date} of the DAG
is set to the time when the sensor should start poking. This way, sensors at the start of the DAG continuously poll for
the condition and continue to the next task once the condition has been met.}, and they continuously ``poll'' for
certain conditions to be \code{True}.

\bd[Poking]
\textbf{Poking} is the process of running a sensor to check if a certain condition is met.
\ed

If the condition is met, the sensor returns \code{True} and the DAG continues to the next task. If the condition is
not met, the sensor returns \code{False} and keeps poking every \code{poke\_interval} seconds until the condition
is met or a \code{timeout} amount of seconds has passed. By default \code{timeout} is set to seven days. \v

The main class object that defines all sensors is the so-called \code{BaseSensorOperator} class. All sensors inherit
from this class, which provides the basic functionality that all sensors share. The following are all subclasses of
the \code{BaseSensorOperator} inheriting its functionality:
\bit
\item \code{BaseSensorOperator}: Base class for all sensors, also used for creating custom sensors.
\item \code{FileSensor}: Waits for a file or folder to appear in a certain location.
\item \code{PythonSensor}: Similar to \code{PythonOperator}, but the \code{python\_callable} is a Python function
limited to returning \code{True} or \code{False} to indicate the status of the condition.
\item \code{TimeSensor}: Waits until a certain time of day.
\item \code{ExternalTaskSensor}: Waits for a task in another DAG to complete.
\item \code{HttpSensor}: Waits for an HTTP endpoint to return a status code within a certain range.
\eit

\be
The following is an example of a DAG file using the \code{FileSensor}.
\begin{block}
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor

# Define default arguments for the DAG shared by all tasks
default_args = {
    'owner': 'Airflow',
    'retries': 5,
    'retry_delay': timedelta(minutes=1)
}

# 1. Instantiate DAG
dag = DAG(
    dag_id='tutorial',
    description='Simple tutorial DAG',
    schedule="0 0 * * *",
    start_date=datetime(2021, 1, 1),
    end_date=None,
    default_args=default_args,
    catchup=False
)

# 2. Define tasks
sensor = FileSensor(
    task_id='wait_for_file',
    filepath='/tmp/hello_world.txt',
    poke_interval=30,
    timeout=600,
    dag=dag
)

task_1 = BashOperator(
    task_id='echo_hello_world',
    bash_command='echo hello world',
    dag=dag
)

# 3. Set up dependencies
sensor >> task_1
\end{block}
\ee

\subsection{Airflow Variables}

\bd[Airflow Variables]
\textbf{Airflow variables} are key-value pairs stored in Airflow's metadata database.
\ed

Airflow variables (variables) are a way to store and retrieve arbitrary, static content or settings from the metadata
database. Variables are typically used to store configuration variables, configuration files, lists of tables, and lists
of IDs to dynamically generate tasks from. \v

The most straight-forward way to set variables, is through the Airflow webserver UI, under ``Admin > Variables''. There
one can see a list of all variables, as well as create new onew by clicking on the ``+'' button and following the
instructions. \v

However, setting variables through the UI is not the recommended way to do so, since it is neither scalable nor clean.
Instead, one should create a \code{variables.json} file inside the \code{config} directory and put all variables inside
there\footnote{One has to keep in mind that since variables are stored in metadata database, any call to variables would
mean a connection to the database. Hence, instead of storing a large number of variables in a DAG, which may end up
saturating the number of allowed connections to your database, it is recommended to store all DAG configuration inside a
single JSON file.}. \v

Then, one can upload the file into Airflow by running:
\begin{bash}
# upload variables to Airflow
airflow variables import <variables.json>
\end{bash}

On top of user-defined variables, Airflow defines a few default variables, accessible in all DAGs.\footnote{For a list
of all default variables check \href{https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html}{here}.}
One has to keep in mind that default variables may be accessed on the level that are defined.

\be
For example, one of the most important default variables is the \code{logical\_date} which represents the date and
time for which the DAG is being executed. This default variable can be accessed from within a \code{DAG}. Similarly,
task level variables can be accessed from within an Operator and so on.
\ee

Speaking about accessing variables, there are two ways to retrieve variables from Airflow in a DAG file: either by using
the \code{Variable} class from the \code{airflow.models} module, or by using ``Jinja templating''.

\be
Assume a \code{variables.json} file with the following content:
\begin{block}
{
    "example_key_1": "example_value_1",
    "example_key_2": "example_value_2"
}
\end{block}

The following is the way to retrieve variables from Airflow using the \code{Variable} class and the Jinja templating:
\begin{block}
from airflow.models import Variable

# Retrieve variables from airflow using the Variable class
dag_config = Variable.get("variables", deserialize_json-True)
example_variable_1 = dag_config["example_key_1"]
example_variable_2 = dag_config["example_key_2"]

# Retrieve variables from airflow using Jinja templating
example_variable_1 = "{{ var.json.variables.example_key_1 }}"
example_variable_2 = "{{ var.json.variables.example_key_2 }}"
\end{block}
\ee

An important caveat to keep in mind is that the \code{@task} decorated tasks don't support rendering jinja templates
passed as arguments. All user-defined variables can be accessed with the \code{Variable} class, and all defaul variables
by using the \code{TaskInstance} and \code{DagRun} objects that are automatically available in the DAG file.

\be
The following is an example of how to access the default variables set by Airflow using the \code{dag} object:
\begin{block}
from airflow.models.taskinstance import TaskInstance
from airflow.models.dagrun import DagRun

@task
def print_ti_info(task_instance: TaskInstance, dag_run: DagRun):
    print(f"Run ID: {task_instance.run_id}")
    print(f"Duration: {task_instance.duration}")
    print(f"DAG Run queued at: {dag_run.queued_at}")
\end{block}
\ee